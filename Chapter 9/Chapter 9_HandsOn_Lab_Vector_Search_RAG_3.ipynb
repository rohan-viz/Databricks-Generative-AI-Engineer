{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fba9911-99b8-402a-9d96-a18f890e77c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Building a Scalable Vector Search and Retrieval System\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are a **generative AI engineer** responsible for designing a high‑performance retrieval‑augmented generation (RAG) system that supports thousands of daily queries from analysts across your organization. Users rely on the system to search technical documents, internal wikis, and operational reports. Recently, leadership approved a redesign of the system to improve retrieval accuracy, accelerate response time, and support higher traffic volumes.\n",
    "\n",
    "Your task is to build and optimize the retrieval layer using **Databricks Mosaic AI and Vector Search**. The lab requires you to:\n",
    "\n",
    "1. Create an embedding pipeline using a Databricks‑hosted embedding model.\n",
    "2. Build a Vector Search index and populate it with documents.\n",
    "3. Configure a retriever to power semantic search.\n",
    "4. Serve the LLM and embedding model using Mosaic AI Model Serving.\n",
    "5. Implement batching and adjust context length to improve throughput.\n",
    "6. Profile RAG performance and identify bottlenecks.\n",
    "7. Optimize system behavior using the tuning strategies introduced in Chapter 9, including context‑length tuning, embedding dimensionality adjustments, batching optimization, and index‑level performance improvements.\n",
    "\n",
    "This hands‑on scenario mirrors real enterprise workloads, where retrieval performance and scalability directly influence the usefulness of generative AI systems. By completing this lab, you will apply the full set of concepts from Chapter 9—including embedding selection, index creation, batch inference, and profiling—to build a robust RAG pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ✅ Design and implement a scalable Vector Search index.\n",
    "- ✅ Choose and apply appropriate embedding models.\n",
    "- ✅ Serve embedding and LLM endpoints using Mosaic AI.\n",
    "- ✅ Configure and tune batching, context length, and concurrency.\n",
    "- ✅ Diagnose performance issues using profiling and metrics.\n",
    "- ✅ Apply optimization strategies to improve accuracy and throughput.\n",
    "- ✅ Connect system bottlenecks to root‑cause adjustments using the patterns introduced in Chapter 9, such as identifying slow vector search, diagnosing model execution delays, detecting inefficient batching, and resolving orchestration or data‑retrieval slowdowns.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this lab, ensure you have:\n",
    "\n",
    "1. **Databricks Workspace** with Unity Catalog enabled\n",
    "2. **Cluster** with Databricks Runtime 14.3 LTS ML or higher\n",
    "3. **Vector Search** endpoint created in your workspace\n",
    "4. **Model Serving** permissions enabled\n",
    "5. Access to **Foundation Model APIs** (for embedding and LLM models)\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Duration\n",
    "\n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                        RAG System Architecture                          │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────────────┐  │\n",
    "│  │   Documents  │───▶│  Embedding   │───▶│   Vector Search Index    │  │\n",
    "│  │   (Delta)    │    │   Pipeline   │    │   (Mosaic AI)            │  │\n",
    "│  └──────────────┘    └──────────────┘    └──────────────────────────┘  │\n",
    "│                                                    │                    │\n",
    "│                                                    ▼                    │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────────────┐  │\n",
    "│  │  User Query  │───▶│  Retriever   │───▶│   LLM Model Serving      │  │\n",
    "│  │              │    │              │    │   (Response Generation)  │  │\n",
    "│  └──────────────┘    └──────────────┘    └──────────────────────────┘  │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eac5318-698b-47bd-b88c-bdb216aed261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup and Configuration\n",
    "\n",
    "In this section, we will:\n",
    "- Install required libraries\n",
    "- Configure workspace settings\n",
    "- Set up catalog and schema for our data\n",
    "\n",
    "This establishes the foundation for our Vector Search and RAG implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cdbaf0-cbdb-44f7-9c9b-4137f4e300d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Required Packages\n",
    "\n",
    "The following cell installs the Python packages required for this lab:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|---------|\n",
    "| `databricks-vectorsearch` | Client library for creating and querying Vector Search indices |\n",
    "| `databricks-sdk` | Unified SDK for interacting with Databricks workspace APIs |\n",
    "| `langchain` | Framework for building LLM-powered applications |\n",
    "| `langchain-community` | Community integrations including Databricks connectors |\n",
    "| `langchain-text-splitters` | Text chunking utilities for document processing |\n",
    "| `langchain-core` | Core abstractions for prompts, output parsers, and runnables |\n",
    "| `tiktoken` | OpenAI's tokenizer for counting tokens in text |\n",
    "\n",
    "**Why these packages?**\n",
    "- Vector Search requires the `databricks-vectorsearch` client to create indices and perform similarity searches\n",
    "- LangChain provides a unified interface for embedding models, LLMs, and retrieval pipelines\n",
    "- The `dbutils.library.restartPython()` ensures the newly installed packages are available in the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbe5b04-a870-46f9-8397-94f9a3678372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages for Vector Search and RAG pipeline\n",
    "# Note: Using compatible versions for Databricks Runtime 14.3+ LTS ML\n",
    "%pip install databricks-vectorsearch databricks-sdk langchain>=0.2.0 langchain-community>=0.2.0 langchain-text-splitters>=0.2.0 langchain-core>=0.2.0 tiktoken --quiet\n",
    "\n",
    "# Restart Python to ensure new packages are loaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7648e5-27a0-4242-b555-712348162710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "The following cell imports all necessary libraries organized by their function:\n",
    "\n",
    "**Standard Libraries:**\n",
    "- `os`, `time`, `json`, `datetime` - System utilities for file operations, timing, and data handling\n",
    "- `typing` - Type hints for better code documentation\n",
    "\n",
    "**PySpark & Delta Lake:**\n",
    "- `SparkSession` - Entry point for Spark functionality\n",
    "- `pyspark.sql.functions` - DataFrame transformation functions (col, lit, concat, etc.)\n",
    "- `pyspark.sql.types` - Schema definitions for structured data\n",
    "\n",
    "**Databricks SDK & Vector Search:**\n",
    "- `WorkspaceClient` - Interact with Databricks workspace (list endpoints, manage resources)\n",
    "- `VectorSearchClient` - Create, manage, and query Vector Search indices\n",
    "\n",
    "**MLflow:**\n",
    "- `mlflow` - Experiment tracking and model registry\n",
    "- `MlflowClient` - Programmatic access to MLflow tracking server\n",
    "\n",
    "**LangChain Components:**\n",
    "- `RecursiveCharacterTextSplitter` - Splits documents into chunks while respecting natural boundaries\n",
    "- `DatabricksEmbeddings` - Wrapper for Databricks-hosted embedding models\n",
    "- `ChatDatabricks` - Wrapper for Databricks-hosted LLM endpoints\n",
    "- `DatabricksVectorSearch` - LangChain integration with Databricks Vector Search\n",
    "\n",
    "**Data Analysis:**\n",
    "- `pandas`, `numpy` - Data manipulation and numerical operations for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410cb4cb-7057-4bbf-a1fb-8ffca1c04632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "\n",
    "# Suppress deprecation warnings from LangChain and other libraries\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*deprecated.*\")\n",
    "\n",
    "# Suppress Databricks SDK authentication notices in notebook output\n",
    "# These notices recommend Service Principal auth for production but are informational only\n",
    "\n",
    "# Method 1: Environment variable to disable notices globally\n",
    "os.environ[\"DATABRICKS_SDK_UPSTREAM\"] = \"true\"\n",
    "os.environ[\"DATABRICKS_SDK_NO_NOTICE\"] = \"true\"\n",
    "\n",
    "# Method 2: Suppress logging-based notices before any SDK imports\n",
    "import logging\n",
    "logging.getLogger(\"databricks.sdk\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"databricks.vector_search\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "# Method 3: Patch the print function to filter notices\n",
    "_original_print = print\n",
    "def _filtered_print(*args, **kwargs):\n",
    "    \"\"\"Filter out [NOTICE] messages from print output.\"\"\"\n",
    "    if args:\n",
    "        text = str(args[0])\n",
    "        if \"[NOTICE]\" in text or \"Using a notebook authentication token\" in text:\n",
    "            return  # Suppress the notice\n",
    "    return _original_print(*args, **kwargs)\n",
    "\n",
    "# Apply the filter (comment out if you want to see notices)\n",
    "import builtins\n",
    "builtins.print = _filtered_print\n",
    "\n",
    "# PySpark and Delta Lake for distributed data processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat, monotonically_increasing_id, udf, current_timestamp\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, StructType, StructField, IntegerType\n",
    "\n",
    "# Databricks SDK for workspace management and Vector Search\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# MLflow for experiment tracking and model serving\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# LangChain components for RAG pipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "\n",
    "# Data analysis libraries for performance profiling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16cfaa7-e2b0-42d6-821d-791e191e5b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure Workspace Settings\n",
    "\n",
    "This cell defines all the configuration parameters for the lab. These settings control:\n",
    "\n",
    "**Unity Catalog Settings:**\n",
    "- `CATALOG_NAME` - The Unity Catalog catalog where tables will be created\n",
    "- `SCHEMA_NAME` - The schema (database) within the catalog for organizing our tables\n",
    "\n",
    "**Table Paths:**\n",
    "- `SOURCE_TABLE_PATH` - Delta table storing the original source documents\n",
    "- `CHUNKS_TABLE_PATH` - Delta table storing chunked documents with embeddings\n",
    "\n",
    "**Vector Search Settings:**\n",
    "- `VECTOR_SEARCH_ENDPOINT_NAME` - The compute endpoint that hosts Vector Search indices\n",
    "- `VECTOR_INDEX_PATH` - Full path to the Vector Search index\n",
    "\n",
    "**Model Settings:**\n",
    "- `EMBEDDING_MODEL_NAME` - The Databricks-hosted embedding model (BGE-large produces 1024-dim vectors)\n",
    "- `LLM_MODEL_NAME` - The Databricks-hosted LLM for response generation\n",
    "\n",
    "**Tuning Parameters (Chapter 9 Focus):**\n",
    "- `CHUNK_SIZE` - Maximum characters per chunk (affects context granularity)\n",
    "- `CHUNK_OVERLAP` - Characters shared between adjacent chunks (maintains context continuity)\n",
    "- `EMBEDDING_DIMENSION` - Vector size (must match the embedding model output)\n",
    "- `TOP_K_RESULTS` - Number of similar documents to retrieve\n",
    "- `BATCH_SIZE` - Documents processed together (affects throughput vs. memory)\n",
    "\n",
    "**⚠️ Important:** Update `CATALOG_NAME` and `VECTOR_SEARCH_ENDPOINT_NAME` to match your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127739fb-172d-4c45-9592-83d9150f14e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# ============================================================\n",
    "\n",
    "# Unity Catalog settings\n",
    "CATALOG_NAME = \"main\"  # Your Unity Catalog name\n",
    "SCHEMA_NAME = \"rag_lab\"  # Schema for this lab\n",
    "\n",
    "# Table names\n",
    "SOURCE_TABLE_NAME = \"technical_documents\"\n",
    "CHUNKS_TABLE_NAME = \"document_chunks\"\n",
    "\n",
    "# Vector Search settings\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"rag_lab_endpoint\"  # Your VS endpoint name\n",
    "VECTOR_INDEX_NAME = \"document_chunks_index\"\n",
    "\n",
    "# Model Serving settings\n",
    "EMBEDDING_MODEL_NAME = \"databricks-bge-large-en\"  # Databricks hosted embedding model\n",
    "# Available LLM endpoints in this workspace (discovered via endpoint listing):\n",
    "# - databricks-meta-llama-3-3-70b-instruct (recommended)\n",
    "# - databricks-meta-llama-3-1-405b-instruct\n",
    "# - databricks-claude-sonnet-4\n",
    "# - databricks-gemma-3-12b\n",
    "LLM_MODEL_NAME = \"databricks-meta-llama-3-3-70b-instruct\"  # Llama 3.3 70B\n",
    "\n",
    "# Performance tuning parameters\n",
    "CHUNK_SIZE = 1000  # Characters per chunk\n",
    "CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "EMBEDDING_DIMENSION = 1024  # BGE-large embedding dimension\n",
    "TOP_K_RESULTS = 5  # Number of results to retrieve\n",
    "BATCH_SIZE = 32  # Batch size for embedding generation\n",
    "\n",
    "# Full table paths\n",
    "SOURCE_TABLE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{SOURCE_TABLE_NAME}\"\n",
    "CHUNKS_TABLE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{CHUNKS_TABLE_NAME}\"\n",
    "VECTOR_INDEX_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
    "\n",
    "print(f\"\uD83D\uDCC1 Catalog: {CATALOG_NAME}\")\n",
    "print(f\"\uD83D\uDCC1 Schema: {SCHEMA_NAME}\")\n",
    "print(f\"\uD83D\uDCC4 Source Table: {SOURCE_TABLE_PATH}\")\n",
    "print(f\"\uD83D\uDCC4 Chunks Table: {CHUNKS_TABLE_PATH}\")\n",
    "print(f\"\uD83D\uDD0D Vector Index: {VECTOR_INDEX_PATH}\")\n",
    "print(f\"\uD83E\uDD16 Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"\uD83E\uDD16 LLM Model: {LLM_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca46b89-c256-4d1b-b313-9d107f81ea45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize Clients and Create Schema\n",
    "\n",
    "This cell performs three critical setup tasks:\n",
    "\n",
    "**1. Initialize SparkSession:**\n",
    "- `SparkSession` is the entry point for all Spark functionality\n",
    "- In Databricks, a session is pre-configured, so we use `getOrCreate()` to access it\n",
    "- This enables distributed data processing for our document pipeline\n",
    "\n",
    "**2. Initialize Databricks Clients:**\n",
    "- `WorkspaceClient` - Provides access to workspace-level APIs (endpoints, jobs, clusters)\n",
    "- `VectorSearchClient` - Specialized client for creating and managing Vector Search indices\n",
    "- `MlflowClient` - Programmatic access to MLflow for experiment tracking\n",
    "\n",
    "**3. Create Unity Catalog Schema:**\n",
    "- Creates the catalog if it doesn't exist (requires appropriate permissions)\n",
    "- Creates the schema within the catalog for organizing our tables\n",
    "- Sets the current catalog and schema context for subsequent SQL operations\n",
    "\n",
    "**Why Unity Catalog?**\n",
    "Unity Catalog provides centralized governance, fine-grained access control, and data lineage tracking - essential for production RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861c8c95-bf87-475b-bc3b-7ff2573f2307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE CLIENTS AND CREATE SCHEMA\n",
    "# ============================================================\n",
    "\n",
    "# Get or create SparkSession (pre-configured in Databricks environment)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initialize Databricks workspace client for API access\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "# Initialize Vector Search client for index management\n",
    "# disable_notice=True suppresses authentication method warnings in notebook output\n",
    "vector_search_client = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "# Initialize MLflow client for experiment tracking\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "# Create Unity Catalog and Schema if they don't exist\n",
    "# Note: You need CREATE CATALOG permission for the first command\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "# Set the current catalog and schema context\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "print(f\"✅ Schema '{CATALOG_NAME}.{SCHEMA_NAME}' is ready!\")\n",
    "print(f\"✅ All clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de35d1d-40b5-41df-b966-a3e2bfa9985c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Discover Available Model Serving Endpoints\n",
    "\n",
    "Before using LLM endpoints, we need to verify which Foundation Models are available in your workspace. This is important because:\n",
    "\n",
    "**Why Discovery is Necessary:**\n",
    "- Different Databricks regions have different model availability\n",
    "- Azure Databricks has different models than AWS Databricks\n",
    "- Some models may be disabled by workspace administrators\n",
    "- Custom endpoints (fine-tuned models) may also be available\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Uses the `WorkspaceClient` to list all serving endpoints\n",
    "2. Categorizes them into Foundation Models (Databricks-hosted) and Custom Models\n",
    "3. Shows the ready status of each endpoint\n",
    "\n",
    "**After Running:**\n",
    "- Identify an LLM endpoint for response generation (e.g., `databricks-meta-llama-3-3-70b-instruct`)\n",
    "- Verify the embedding model is available (e.g., `databricks-bge-large-en`)\n",
    "- Update `LLM_MODEL_NAME` in the configuration cell if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372eedd1-3f97-4187-9cba-c4a1ce2012d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DISCOVER AVAILABLE MODEL SERVING ENDPOINTS\n",
    "# ============================================================\n",
    "\n",
    "# Re-import WorkspaceClient (in case Python was restarted)\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Create workspace client instance\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"\uD83D\uDCCB Available Model Serving Endpoints:\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Separate foundation models from custom endpoints\n",
    "foundation_models = []\n",
    "custom_models = []\n",
    "\n",
    "# Iterate through all serving endpoints in the workspace\n",
    "for endpoint in w.serving_endpoints.list():\n",
    "    endpoint_name = endpoint.name\n",
    "    # Check if endpoint is ready to serve requests\n",
    "    state = endpoint.state.ready if endpoint.state else \"UNKNOWN\"\n",
    "\n",
    "    # Categorize based on naming convention\n",
    "    if endpoint_name.startswith(\"databricks-\"):\n",
    "        foundation_models.append((endpoint_name, state))\n",
    "    else:\n",
    "        custom_models.append((endpoint_name, state))\n",
    "\n",
    "# Display Foundation Model endpoints\n",
    "print(\"\uD83C\uDFE2 Foundation Model Endpoints (Databricks-hosted):\")\n",
    "if foundation_models:\n",
    "    for name, state in sorted(foundation_models):\n",
    "        status_icon = \"✅\" if state else \"⏳\"\n",
    "        print(f\"   {status_icon} {name}\")\n",
    "else:\n",
    "    print(\"   ⚠️  No Foundation Model endpoints found\")\n",
    "\n",
    "# Display Custom Model endpoints\n",
    "print(\"\\n\uD83D\uDD27 Custom Model Endpoints:\")\n",
    "if custom_models:\n",
    "    for name, state in sorted(custom_models):\n",
    "        status_icon = \"✅\" if state else \"⏳\"\n",
    "        print(f\"   {status_icon} {name}\")\n",
    "else:\n",
    "    print(\"   (none)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\uD83D\uDCA1 Update LLM_MODEL_NAME in the configuration cell above\")\n",
    "print(\"   to use one of the available Foundation Model endpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6005d285-7027-413d-8a07-00402becc462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Sample Data Generation (Prerequisite)\n",
    "\n",
    "Before building our RAG system, we need realistic sample data that simulates enterprise content. This section creates a diverse document corpus that will be used throughout the lab.\n",
    "\n",
    "### Why Sample Data Matters\n",
    "\n",
    "The quality and diversity of your document corpus directly impacts RAG system performance:\n",
    "- **Diverse content types** test the embedding model's ability to capture different semantic patterns\n",
    "- **Varying document lengths** help tune chunking parameters\n",
    "- **Multiple categories** enable testing of filtered retrieval\n",
    "- **Realistic content** ensures the lab reflects production scenarios\n",
    "\n",
    "### Document Categories\n",
    "\n",
    "We generate three types of documents commonly found in enterprise knowledge bases:\n",
    "\n",
    "| Category | Count | Purpose | Example Queries |\n",
    "|----------|-------|---------|-----------------|\n",
    "| **Technical Documents** | 4 | API docs, architecture guides, troubleshooting | \"How do I authenticate?\", \"What is the data pipeline?\" |\n",
    "| **Internal Wikis** | 3 | Process docs, team guidelines, policies | \"What are code review practices?\", \"Who is on-call?\" |\n",
    "| **Operational Reports** | 4 | Incident reports, performance summaries | \"What was the outage impact?\", \"What is Q4 uptime?\" |\n",
    "\n",
    "### Document Schema\n",
    "\n",
    "Each document contains:\n",
    "- `doc_id` - Unique identifier for tracking and retrieval\n",
    "- `title` - Human-readable document title\n",
    "- `category` - Classification for filtered search\n",
    "- `content` - The actual document text (will be chunked)\n",
    "- `author` - Document owner for governance\n",
    "- `last_updated` - Timestamp for freshness tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae5a85e-138a-467d-967d-dd243a43b64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Technical Documents\n",
    "\n",
    "Technical documents include API references, architecture guides, and troubleshooting manuals. These documents typically contain:\n",
    "- Structured information (endpoints, parameters, error codes)\n",
    "- Step-by-step procedures\n",
    "- Technical terminology that requires precise semantic matching\n",
    "\n",
    "The embedding model must capture both the technical vocabulary and the procedural nature of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cdadca-2527-48b7-a694-43f231ea1c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAMPLE DATA GENERATION - Technical Documents\n",
    "# ============================================================\n",
    "\n",
    "technical_documents = [\n",
    "    {\n",
    "        \"doc_id\": \"TECH-001\",\n",
    "        \"title\": \"User Authentication API Reference\",\n",
    "        \"category\": \"technical\",\n",
    "        \"content\": \"\"\"The User Authentication API provides secure endpoints for user login, registration, and session management.\n",
    "\n",
    "Authentication Flow:\n",
    "1. Client sends POST request to /api/v2/auth/login with credentials\n",
    "2. Server validates credentials against the identity provider\n",
    "3. Upon success, server returns JWT token with 24-hour expiration\n",
    "4. Client includes token in Authorization header for subsequent requests\n",
    "\n",
    "Endpoints:\n",
    "- POST /api/v2/auth/login: Authenticate user and receive JWT token\n",
    "- POST /api/v2/auth/register: Create new user account\n",
    "- POST /api/v2/auth/refresh: Refresh expired JWT token\n",
    "- POST /api/v2/auth/logout: Invalidate current session\n",
    "- GET /api/v2/auth/profile: Retrieve authenticated user profile\n",
    "\n",
    "Error Codes:\n",
    "- 401: Invalid credentials or expired token\n",
    "- 403: Insufficient permissions\n",
    "- 429: Rate limit exceeded (max 100 requests/minute)\n",
    "\n",
    "Security Considerations:\n",
    "All authentication endpoints require HTTPS. Tokens are signed using RS256 algorithm. Failed login attempts are logged and may trigger account lockout after 5 consecutive failures.\"\"\",\n",
    "        \"author\": \"Platform Team\",\n",
    "        \"last_updated\": \"2024-01-15\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"TECH-002\",\n",
    "        \"title\": \"Data Pipeline Architecture Guide\",\n",
    "        \"category\": \"technical\",\n",
    "        \"content\": \"\"\"This document describes the enterprise data pipeline architecture used for processing and transforming data across our analytics platform.\n",
    "\n",
    "Architecture Overview:\n",
    "The pipeline follows a medallion architecture pattern with Bronze, Silver, and Gold layers:\n",
    "\n",
    "Bronze Layer (Raw Data):\n",
    "- Ingests data from 50+ source systems\n",
    "- Stores data in original format with minimal transformation\n",
    "- Retention period: 90 days\n",
    "- Storage format: Delta Lake with Z-ordering on timestamp\n",
    "\n",
    "Silver Layer (Cleaned Data):\n",
    "- Applies data quality rules and standardization\n",
    "- Deduplication using composite keys\n",
    "- Schema enforcement and type casting\n",
    "- Incremental processing with watermarking\n",
    "\n",
    "Gold Layer (Business-Ready):\n",
    "- Aggregated metrics and KPIs\n",
    "- Dimensional models for reporting\n",
    "- Optimized for query performance\n",
    "- Materialized views for common queries\n",
    "\n",
    "Performance Metrics:\n",
    "- Daily data volume: 2.5 TB\n",
    "- Average latency: 15 minutes end-to-end\n",
    "- SLA: 99.5% availability\"\"\",\n",
    "        \"author\": \"Data Engineering Team\",\n",
    "        \"last_updated\": \"2024-02-20\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"TECH-003\",\n",
    "        \"title\": \"Kubernetes Deployment Troubleshooting\",\n",
    "        \"category\": \"technical\",\n",
    "        \"content\": \"\"\"Troubleshooting guide for common Kubernetes deployment issues in our production environment.\n",
    "\n",
    "Common Issues and Solutions:\n",
    "\n",
    "1. Pod CrashLoopBackOff:\n",
    "   - Check logs: kubectl logs <pod-name> --previous\n",
    "   - Verify resource limits are not too restrictive\n",
    "   - Check liveness/readiness probe configurations\n",
    "   - Ensure environment variables are correctly set\n",
    "\n",
    "2. ImagePullBackOff:\n",
    "   - Verify image name and tag exist in registry\n",
    "   - Check imagePullSecrets are configured\n",
    "   - Ensure network connectivity to container registry\n",
    "\n",
    "3. Pending Pods:\n",
    "   - Check node resources: kubectl describe nodes\n",
    "   - Verify PersistentVolumeClaims are bound\n",
    "   - Check node selectors and tolerations\n",
    "\n",
    "4. Service Not Accessible:\n",
    "   - Verify selector labels match pod labels\n",
    "   - Check endpoint status: kubectl get endpoints\n",
    "   - Validate network policies allow traffic\n",
    "\n",
    "Debugging Commands:\n",
    "- kubectl describe pod <pod-name>\n",
    "- kubectl get events --sort-by='.lastTimestamp'\n",
    "- kubectl exec -it <pod-name> -- /bin/sh\"\"\",\n",
    "        \"author\": \"DevOps Team\",\n",
    "        \"last_updated\": \"2024-03-10\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"TECH-004\",\n",
    "        \"title\": \"Machine Learning Model Deployment Guide\",\n",
    "        \"category\": \"technical\",\n",
    "        \"content\": \"\"\"This guide covers the end-to-end process for deploying machine learning models to production using MLflow and Databricks Model Serving.\n",
    "\n",
    "Model Registration:\n",
    "1. Train your model using any ML framework (scikit-learn, PyTorch, TensorFlow)\n",
    "2. Log the model to MLflow with appropriate signature\n",
    "3. Register the model in Unity Catalog\n",
    "4. Add model version description and tags\n",
    "\n",
    "Deployment Options:\n",
    "- Real-time serving: Low-latency predictions via REST API\n",
    "- Batch inference: Process large datasets using Spark\n",
    "- Streaming inference: Real-time predictions on streaming data\n",
    "\n",
    "Model Serving Configuration:\n",
    "- Compute size: Small (4 CPU, 16GB RAM) to Large (16 CPU, 64GB RAM)\n",
    "- Scale to zero: Enable for cost optimization\n",
    "- Auto-scaling: Configure min/max replicas based on traffic\n",
    "\n",
    "Monitoring:\n",
    "- Track prediction latency and throughput\n",
    "- Monitor model drift using statistical tests\n",
    "- Set up alerts for performance degradation\n",
    "- Log predictions for audit and debugging\"\"\",\n",
    "        \"author\": \"ML Platform Team\",\n",
    "        \"last_updated\": \"2024-03-25\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(technical_documents)} technical documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03308c14-c7cf-4b66-aa46-af4b3ca4dceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Internal Wiki Documents\n",
    "\n",
    "These documents represent internal knowledge base articles covering processes, guidelines, and best practices used across the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c25c716-d565-4160-9bc3-c357ca5ed54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAMPLE DATA GENERATION - Internal Wiki Documents\n",
    "# ============================================================\n",
    "\n",
    "wiki_documents = [\n",
    "    {\n",
    "        \"doc_id\": \"WIKI-001\",\n",
    "        \"title\": \"Code Review Best Practices\",\n",
    "        \"category\": \"wiki\",\n",
    "        \"content\": \"\"\"This wiki outlines the code review process and best practices for our engineering teams.\n",
    "\n",
    "Code Review Checklist:\n",
    "□ Code follows team style guidelines\n",
    "□ Unit tests cover new functionality\n",
    "□ No hardcoded secrets or credentials\n",
    "□ Error handling is comprehensive\n",
    "□ Documentation is updated\n",
    "□ Performance implications considered\n",
    "\n",
    "Review Process:\n",
    "1. Author creates pull request with clear description\n",
    "2. Automated checks run (linting, tests, security scan)\n",
    "3. At least 2 reviewers must approve\n",
    "4. Address all comments before merging\n",
    "5. Squash commits and merge to main branch\n",
    "\n",
    "Response Time Expectations:\n",
    "- Initial review: Within 24 hours\n",
    "- Follow-up reviews: Within 4 hours\n",
    "- Urgent fixes: Within 2 hours\n",
    "\n",
    "Common Feedback Categories:\n",
    "- Logic errors or edge cases\n",
    "- Performance concerns\n",
    "- Security vulnerabilities\n",
    "- Code readability improvements\n",
    "- Missing test coverage\"\"\",\n",
    "        \"author\": \"Engineering Standards Team\",\n",
    "        \"last_updated\": \"2024-02-01\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"WIKI-002\",\n",
    "        \"title\": \"On-Call Rotation Guidelines\",\n",
    "        \"category\": \"wiki\",\n",
    "        \"content\": \"\"\"Guidelines for engineers participating in the on-call rotation for production systems.\n",
    "\n",
    "On-Call Responsibilities:\n",
    "- Monitor alerting channels during shift\n",
    "- Acknowledge alerts within 15 minutes\n",
    "- Escalate issues that cannot be resolved within 1 hour\n",
    "- Document all incidents in the incident tracker\n",
    "- Participate in post-incident reviews\n",
    "\n",
    "Shift Schedule:\n",
    "- Primary on-call: 7 days, 24/7 coverage\n",
    "- Secondary on-call: Backup for escalations\n",
    "- Handoff meeting: Every Monday at 10 AM\n",
    "\n",
    "Escalation Path:\n",
    "1. Primary on-call engineer\n",
    "2. Secondary on-call engineer\n",
    "3. Team lead\n",
    "4. Engineering manager\n",
    "5. VP of Engineering (critical incidents only)\n",
    "\n",
    "Tools and Access:\n",
    "- PagerDuty for alerting\n",
    "- Slack #incidents channel\n",
    "- Runbook repository in Confluence\n",
    "- VPN access for remote debugging\"\"\",\n",
    "        \"author\": \"SRE Team\",\n",
    "        \"last_updated\": \"2024-01-20\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"WIKI-003\",\n",
    "        \"title\": \"Data Governance Policies\",\n",
    "        \"category\": \"wiki\",\n",
    "        \"content\": \"\"\"This document outlines data governance policies for handling sensitive information.\n",
    "\n",
    "Data Classification Levels:\n",
    "1. Public: No restrictions on access or sharing\n",
    "2. Internal: Available to all employees\n",
    "3. Confidential: Restricted to specific teams\n",
    "4. Restricted: Highly sensitive, need-to-know basis\n",
    "\n",
    "PII Handling Requirements:\n",
    "- Encrypt PII at rest and in transit\n",
    "- Mask PII in non-production environments\n",
    "- Log all access to PII data\n",
    "- Retain PII only as long as necessary\n",
    "- Obtain consent before collecting PII\n",
    "\n",
    "Data Retention Policies:\n",
    "- Transaction data: 7 years\n",
    "- User activity logs: 90 days\n",
    "- Analytics data: 2 years\n",
    "- Backup data: 30 days after deletion\n",
    "\n",
    "Compliance Requirements:\n",
    "- GDPR for EU customers\n",
    "- CCPA for California residents\n",
    "- SOC 2 Type II certification\n",
    "- Annual security audits\"\"\",\n",
    "        \"author\": \"Compliance Team\",\n",
    "        \"last_updated\": \"2024-03-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(wiki_documents)} wiki documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3701dbbc-95e1-467c-80a5-40d8c8b33684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Operational Reports\n",
    "\n",
    "Operational reports represent time-sensitive, factual content that analysts frequently query. These include:\n",
    "- **Incident Reports** - Post-mortems with root cause analysis and impact metrics\n",
    "- **Performance Summaries** - Quarterly metrics, SLA compliance, system health\n",
    "- **Status Updates** - Weekly progress reports, project milestones\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Contains specific numbers, dates, and metrics\n",
    "- Often queried with time-based filters (\"Q4 performance\", \"last incident\")\n",
    "- Requires precise retrieval to avoid mixing data from different time periods\n",
    "\n",
    "**RAG Challenge:** The system must distinguish between similar reports from different time periods and return the most relevant one based on the query context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cb2ea1-5ff2-40b2-be2d-d79e3696c1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAMPLE DATA GENERATION - Operational Reports\n",
    "# ============================================================\n",
    "\n",
    "operational_reports = [\n",
    "    {\n",
    "        \"doc_id\": \"OPS-001\",\n",
    "        \"title\": \"Q4 2024 System Performance Report\",\n",
    "        \"category\": \"operational\",\n",
    "        \"content\": \"\"\"Quarterly performance report for production systems covering October-December 2024.\n",
    "\n",
    "Executive Summary:\n",
    "Overall system availability exceeded targets with 99.97% uptime. Response times improved by 15% compared to Q3 due to infrastructure optimizations.\n",
    "\n",
    "Key Metrics:\n",
    "- Uptime: 99.97% (target: 99.9%)\n",
    "- Average response time: 145ms (target: 200ms)\n",
    "- Error rate: 0.02% (target: 0.1%)\n",
    "- Peak concurrent users: 125,000\n",
    "- Total API calls: 2.3 billion\n",
    "\n",
    "Infrastructure Changes:\n",
    "- Migrated 40% of workloads to new Kubernetes cluster\n",
    "- Upgraded database instances to latest generation\n",
    "- Implemented CDN for static assets\n",
    "- Added 3 new edge locations\n",
    "\n",
    "Incidents:\n",
    "- Total incidents: 12\n",
    "- P1 incidents: 1 (database failover)\n",
    "- P2 incidents: 4\n",
    "- Mean time to resolution: 23 minutes\n",
    "\n",
    "Recommendations:\n",
    "1. Increase database connection pool size\n",
    "2. Implement request queuing for traffic spikes\n",
    "3. Add circuit breakers for external dependencies\"\"\",\n",
    "        \"author\": \"Platform Operations\",\n",
    "        \"last_updated\": \"2024-01-05\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"OPS-002\",\n",
    "        \"title\": \"Incident Report: Payment Processing Outage\",\n",
    "        \"category\": \"operational\",\n",
    "        \"content\": \"\"\"Incident Report - Payment Processing Service Outage\n",
    "Date: March 15, 2024\n",
    "Duration: 47 minutes\n",
    "Severity: P1\n",
    "\n",
    "Impact:\n",
    "- Payment processing unavailable for 47 minutes\n",
    "- Approximately 12,000 transactions affected\n",
    "- Estimated revenue impact: $450,000\n",
    "- Customer complaints: 234\n",
    "\n",
    "Timeline:\n",
    "14:23 - Monitoring alerts triggered for payment service errors\n",
    "14:25 - On-call engineer acknowledged alert\n",
    "14:32 - Root cause identified: database connection exhaustion\n",
    "14:45 - Temporary fix applied: increased connection pool\n",
    "15:10 - Service fully restored\n",
    "15:30 - All queued transactions processed\n",
    "\n",
    "Root Cause:\n",
    "A deployment at 14:15 introduced a connection leak in the payment service. Each request opened a new database connection without properly closing it, exhausting the connection pool within 8 minutes.\n",
    "\n",
    "Corrective Actions:\n",
    "1. Reverted problematic deployment\n",
    "2. Added connection pool monitoring\n",
    "3. Implemented connection timeout settings\n",
    "4. Updated deployment checklist to include connection testing\n",
    "5. Scheduled post-incident review for March 18\"\"\",\n",
    "        \"author\": \"Incident Response Team\",\n",
    "        \"last_updated\": \"2024-03-16\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"OPS-003\",\n",
    "        \"title\": \"Weekly Infrastructure Status Update\",\n",
    "        \"category\": \"operational\",\n",
    "        \"content\": \"\"\"Weekly Infrastructure Status Report - Week of March 18, 2024\n",
    "\n",
    "Overall Status: GREEN ✅\n",
    "\n",
    "Compute Resources:\n",
    "- CPU utilization: 45% average (healthy)\n",
    "- Memory utilization: 62% average (healthy)\n",
    "- Disk I/O: Normal levels\n",
    "- Network throughput: 2.3 Gbps average\n",
    "\n",
    "Database Health:\n",
    "- Primary cluster: Healthy\n",
    "- Read replicas: 3/3 healthy\n",
    "- Replication lag: <100ms\n",
    "- Storage utilization: 72%\n",
    "\n",
    "Kubernetes Clusters:\n",
    "- Production: 48/50 nodes healthy\n",
    "- Staging: 12/12 nodes healthy\n",
    "- 2 nodes in production under maintenance\n",
    "\n",
    "Upcoming Maintenance:\n",
    "- March 22: Database version upgrade (2 AM - 4 AM)\n",
    "- March 25: Network switch replacement (minimal impact)\n",
    "- March 28: SSL certificate renewal\n",
    "\n",
    "Action Items:\n",
    "- Investigate memory growth in analytics service\n",
    "- Plan capacity increase for Q2 traffic projections\n",
    "- Review and update disaster recovery runbooks\"\"\",\n",
    "        \"author\": \"Infrastructure Team\",\n",
    "        \"last_updated\": \"2024-03-18\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"OPS-004\",\n",
    "        \"title\": \"Cost Optimization Analysis Report\",\n",
    "        \"category\": \"operational\",\n",
    "        \"content\": \"\"\"Monthly Cloud Cost Optimization Report - February 2024\n",
    "\n",
    "Total Cloud Spend: $847,000 (5% under budget)\n",
    "\n",
    "Cost Breakdown by Service:\n",
    "- Compute (EC2/VMs): $412,000 (49%)\n",
    "- Storage (S3/Blob): $178,000 (21%)\n",
    "- Database (RDS/SQL): $156,000 (18%)\n",
    "- Networking: $67,000 (8%)\n",
    "- Other services: $34,000 (4%)\n",
    "\n",
    "Optimization Achievements:\n",
    "1. Reserved instance coverage increased to 72%\n",
    "2. Implemented auto-scaling for dev environments\n",
    "3. Archived 15TB of cold data to glacier storage\n",
    "4. Consolidated 12 underutilized instances\n",
    "\n",
    "Savings Realized: $123,000 (13% reduction from baseline)\n",
    "\n",
    "Recommendations for Next Month:\n",
    "1. Migrate remaining workloads to spot instances where applicable\n",
    "2. Implement S3 intelligent tiering for analytics data\n",
    "3. Right-size database instances based on usage patterns\n",
    "4. Enable auto-shutdown for non-production environments\n",
    "\n",
    "Cost Anomalies Detected:\n",
    "- Unusual spike in data transfer costs on Feb 15\n",
    "- Investigation revealed: Large dataset export for compliance audit\"\"\",\n",
    "        \"author\": \"FinOps Team\",\n",
    "        \"last_updated\": \"2024-03-05\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(operational_reports)} operational reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba255cf-6b18-4052-b977-68c835805470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Combine and Save Documents to Delta Table\n",
    "\n",
    "This cell performs the following operations:\n",
    "\n",
    "**1. Combine All Documents:**\n",
    "- Merges technical documents, wiki documents, and operational reports into a single list\n",
    "- This unified corpus will be processed through the embedding pipeline\n",
    "\n",
    "**2. Create Spark DataFrame:**\n",
    "- Converts the Python list of dictionaries to a distributed Spark DataFrame\n",
    "- Enables parallel processing for large document collections\n",
    "\n",
    "**3. Add Metadata Columns:**\n",
    "- `created_at` - Timestamp for tracking when documents were ingested\n",
    "- `embedding_status` - Status flag for tracking embedding generation progress\n",
    "\n",
    "**4. Save as Delta Table:**\n",
    "- Writes to Unity Catalog with `overwrite` mode for idempotent execution\n",
    "- `overwriteSchema` allows schema evolution if document structure changes\n",
    "- Delta format enables ACID transactions and time travel\n",
    "\n",
    "**Why Delta Lake?**\n",
    "- Vector Search requires Delta tables with Change Data Feed enabled\n",
    "- Delta provides automatic versioning and rollback capabilities\n",
    "- Optimized for both batch and streaming updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f4773f-6604-44f3-a228-3adb21e42f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBINE ALL DOCUMENTS AND SAVE TO DELTA TABLE\n",
    "# ============================================================\n",
    "\n",
    "# Combine all document types into a single corpus\n",
    "all_documents = technical_documents + wiki_documents + operational_reports\n",
    "\n",
    "# Convert to Spark DataFrame for distributed processing\n",
    "documents_df = spark.createDataFrame(all_documents)\n",
    "\n",
    "# Add metadata columns for tracking and governance\n",
    "documents_df = documents_df.withColumn(\"created_at\", current_timestamp()) \\\n",
    "                           .withColumn(\"embedding_status\", lit(\"pending\"))\n",
    "\n",
    "# Display document statistics\n",
    "print(f\"\uD83D\uDCCA Total documents: {documents_df.count()}\")\n",
    "print(f\"\\n\uD83D\uDCCB Document categories:\")\n",
    "documents_df.groupBy(\"category\").count().show()\n",
    "\n",
    "# Save to Delta table in Unity Catalog\n",
    "# Using overwrite mode for idempotent execution (safe to re-run)\n",
    "documents_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(SOURCE_TABLE_PATH)\n",
    "\n",
    "print(f\"\\n✅ Documents saved to {SOURCE_TABLE_PATH}\")\n",
    "\n",
    "# Display the saved table\n",
    "display(spark.table(SOURCE_TABLE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bc6dcd-2087-42ec-8a10-479c5d678e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Building the Embedding Pipeline\n",
    "\n",
    "The embedding pipeline is the foundation of any RAG system. It transforms text documents into numerical vectors that capture semantic meaning, enabling similarity-based retrieval.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "```\n",
    "┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n",
    "│   Documents  │───▶│   Chunking   │───▶│  Embedding   │───▶│  Delta Table │\n",
    "│   (Source)   │    │  (Splitting) │    │  (Vectors)   │    │  (Storage)   │\n",
    "└──────────────┘    └──────────────┘    └──────────────┘    └──────────────┘\n",
    "```\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Documents are typically too long to embed as a single unit:\n",
    "- **Token limits**: Embedding models have maximum input lengths (typically 512-8192 tokens)\n",
    "- **Retrieval precision**: Smaller chunks enable more precise matching\n",
    "- **Context relevance**: Large chunks may contain irrelevant information that dilutes the embedding\n",
    "\n",
    "### Key Decisions in This Section\n",
    "\n",
    "| Decision | Our Choice | Rationale |\n",
    "|----------|------------|-----------|\n",
    "| Chunk size | 1000 chars | Balances context richness with retrieval precision |\n",
    "| Chunk overlap | 200 chars | Prevents losing context at chunk boundaries |\n",
    "| Embedding model | BGE-large-en | High-quality, 1024-dim vectors, optimized for English |\n",
    "| Batch size | 32 | Optimizes throughput while managing memory |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa22ed4-8918-404a-88ee-731674c86fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Document Chunking Strategy\n",
    "\n",
    "We use `RecursiveCharacterTextSplitter` from LangChain, which is the recommended splitter for most use cases.\n",
    "\n",
    "**How RecursiveCharacterTextSplitter Works:**\n",
    "1. Attempts to split on the first separator (`\\n\\n` - paragraph breaks)\n",
    "2. If chunks are still too large, tries the next separator (`\\n` - line breaks)\n",
    "3. Continues through separators (`. `, ` `, `\"\"`) until chunks fit within `chunk_size`\n",
    "4. Adds `chunk_overlap` characters from the previous chunk to maintain context\n",
    "\n",
    "**Separator Hierarchy:**\n",
    "```python\n",
    "separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "#           ↑       ↑     ↑    ↑    ↑\n",
    "#      Paragraphs Lines Sentences Words Characters\n",
    "```\n",
    "\n",
    "**Metadata Preservation:**\n",
    "Each chunk retains metadata from the parent document:\n",
    "- `chunk_id` - Unique identifier for the chunk\n",
    "- `doc_id` - Reference to the source document\n",
    "- `chunk_index` - Position within the document (useful for context reconstruction)\n",
    "- `total_chunks` - Total chunks from this document\n",
    "- `char_count` - Character count for analysis\n",
    "\n",
    "**Chapter 9 Tuning Insight:** Chunk size significantly impacts retrieval quality. Too small = fragmented context. Too large = diluted relevance. Start with 1000 chars and adjust based on your content type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79663f64-2e3f-4e9f-9a0d-1b5288a22a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# ============================================================\n",
    "\n",
    "# Initialize the text splitter with our tuning parameters\n",
    "# RecursiveCharacterTextSplitter tries to split on natural boundaries\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,           # Maximum characters per chunk\n",
    "    chunk_overlap=CHUNK_OVERLAP,     # Characters shared between adjacent chunks\n",
    "    length_function=len,             # Use character count (not token count)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority order for splitting\n",
    ")\n",
    "\n",
    "def chunk_document(doc_id: str, title: str, content: str, category: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split a document into chunks while preserving metadata.\n",
    "\n",
    "    Args:\n",
    "        doc_id: Unique document identifier\n",
    "        title: Document title (preserved in each chunk)\n",
    "        content: Full document text to be chunked\n",
    "        category: Document category for filtered retrieval\n",
    "\n",
    "    Returns:\n",
    "        List of chunk dictionaries with metadata\n",
    "    \"\"\"\n",
    "    # Split the content into chunks\n",
    "    chunks = text_splitter.split_text(content)\n",
    "\n",
    "    # Create chunk records with metadata\n",
    "    return [\n",
    "        {\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",  # Unique chunk identifier\n",
    "            \"doc_id\": doc_id,                   # Parent document reference\n",
    "            \"title\": title,                     # Preserved for display\n",
    "            \"category\": category,               # For filtered retrieval\n",
    "            \"chunk_index\": i,                   # Position in document\n",
    "            \"total_chunks\": len(chunks),        # Total chunks from this doc\n",
    "            \"content\": chunk,                   # The actual chunk text\n",
    "            \"char_count\": len(chunk)            # For analysis\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "# Load documents from Delta table\n",
    "documents = spark.table(SOURCE_TABLE_PATH).collect()\n",
    "\n",
    "# Process each document through the chunking pipeline\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunk_document(\n",
    "        doc_id=doc[\"doc_id\"],\n",
    "        title=doc[\"title\"],\n",
    "        content=doc[\"content\"],\n",
    "        category=doc[\"category\"]\n",
    "    )\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Display chunking statistics\n",
    "print(f\"\uD83D\uDCC4 Original documents: {len(documents)}\")\n",
    "print(f\"\uD83D\uDCE6 Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"\uD83D\uDCCA Average chunks per document: {len(all_chunks) / len(documents):.1f}\")\n",
    "\n",
    "# Analyze chunk size distribution (important for tuning)\n",
    "chunk_sizes = [c[\"char_count\"] for c in all_chunks]\n",
    "print(f\"\\n\uD83D\uDCCF Chunk size statistics:\")\n",
    "print(f\"   Min: {min(chunk_sizes)} chars\")\n",
    "print(f\"   Max: {max(chunk_sizes)} chars\")\n",
    "print(f\"   Avg: {sum(chunk_sizes) / len(chunk_sizes):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344203a7-c150-4cf0-9412-f7af167de009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Embeddings Using Databricks Foundation Model\n",
    "\n",
    "Embeddings are dense vector representations that capture the semantic meaning of text. We use the Databricks-hosted `databricks-bge-large-en` model.\n",
    "\n",
    "**About BGE-large-en:**\n",
    "| Property | Value | Implication |\n",
    "|----------|-------|-------------|\n",
    "| Dimensions | 1024 | Higher dimensionality = more semantic nuance |\n",
    "| Max tokens | 512 | Longer chunks may be truncated |\n",
    "| Language | English | Optimized for English text |\n",
    "| Architecture | BERT-based | Bidirectional context understanding |\n",
    "\n",
    "**Why Batching Matters (Chapter 9 Pattern):**\n",
    "\n",
    "Embedding generation is often a bottleneck in RAG pipelines. Batching provides:\n",
    "1. **Reduced API overhead** - Fewer HTTP requests\n",
    "2. **Better GPU utilization** - Models process batches more efficiently\n",
    "3. **Predictable throughput** - Easier to estimate processing time\n",
    "\n",
    "**Batch Size Trade-offs:**\n",
    "- **Too small (1-8)**: High API overhead, underutilized compute\n",
    "- **Too large (128+)**: Memory pressure, potential timeouts\n",
    "- **Optimal (16-64)**: Balances throughput and reliability\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Initializes the `DatabricksEmbeddings` wrapper for the BGE model\n",
    "2. Defines a batched embedding function with progress tracking\n",
    "3. Processes all chunks and measures throughput\n",
    "4. Reports embedding statistics for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3553c06f-fa95-4d68-a1b8-ca119ee3bb13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EMBEDDING GENERATION\n",
    "# ============================================================\n",
    "\n",
    "# Initialize Databricks embedding model\n",
    "# DatabricksEmbeddings automatically handles authentication within the workspace\n",
    "embedding_model = DatabricksEmbeddings(\n",
    "    endpoint=EMBEDDING_MODEL_NAME,\n",
    "    # No API key needed when running in Databricks workspace\n",
    ")\n",
    "\n",
    "def generate_embeddings_batch(texts: List[str], batch_size: int = BATCH_SIZE) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings in batches for better throughput.\n",
    "\n",
    "    Batching reduces API overhead and improves GPU utilization on the\n",
    "    serving endpoint. Progress is logged every 5 batches.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to process per API call\n",
    "\n",
    "    Returns:\n",
    "        List of embedding vectors (each is a list of floats)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"\uD83D\uDD04 Generating embeddings for {len(texts)} chunks in {total_batches} batches...\")\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "\n",
    "        # Time each batch for throughput analysis\n",
    "        start_time = time.time()\n",
    "        embeddings = embedding_model.embed_documents(batch)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "        # Log progress periodically\n",
    "        if batch_num % 5 == 0 or batch_num == total_batches:\n",
    "            print(f\"   Batch {batch_num}/{total_batches} completed ({elapsed:.2f}s)\")\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# Extract text content from all chunks\n",
    "chunk_texts = [chunk[\"content\"] for chunk in all_chunks]\n",
    "\n",
    "# Generate embeddings with timing\n",
    "start_time = time.time()\n",
    "embeddings = generate_embeddings_batch(chunk_texts)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Report embedding statistics\n",
    "print(f\"\\n✅ Embedding generation complete!\")\n",
    "print(f\"⏱️  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"\uD83D\uDCCA Throughput: {len(chunk_texts) / total_time:.1f} chunks/second\")\n",
    "print(f\"\uD83D\uDCD0 Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fb0792-1ffc-4fac-9747-8acc4f50b9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save Chunks with Embeddings to Delta Table\n",
    "\n",
    "This cell persists the chunks and their embeddings to a Delta table in Unity Catalog.\n",
    "\n",
    "**Schema Design:**\n",
    "| Column | Type | Purpose |\n",
    "|--------|------|---------|\n",
    "| `chunk_id` | STRING | Primary key for the Vector Search index |\n",
    "| `doc_id` | STRING | Foreign key to source document |\n",
    "| `title` | STRING | Displayed in search results |\n",
    "| `category` | STRING | Enables filtered retrieval |\n",
    "| `chunk_index` | INTEGER | Position within document |\n",
    "| `content` | STRING | Text content for LLM context |\n",
    "| `embedding` | ARRAY<FLOAT> | 1024-dim vector for similarity search |\n",
    "| `indexed_at` | TIMESTAMP | Audit trail for data freshness |\n",
    "\n",
    "**Critical Configuration - Change Data Feed:**\n",
    "\n",
    "The `delta.enableChangeDataFeed = true` property is **required** for Delta Sync indices. It enables:\n",
    "- Automatic detection of inserts, updates, and deletes\n",
    "- Incremental synchronization (only changed rows are re-indexed)\n",
    "- Near real-time index updates without full rebuilds\n",
    "\n",
    "**Why Delta Lake for Vector Storage?**\n",
    "1. **ACID transactions** - Consistent reads during updates\n",
    "2. **Time travel** - Rollback to previous versions if needed\n",
    "3. **Schema evolution** - Add columns without breaking the index\n",
    "4. **Unified governance** - Same access controls as other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3973ee1e-a2c7-4583-8fa0-5d8f3930cb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE CHUNKS WITH EMBEDDINGS TO DELTA TABLE\n",
    "# ============================================================\n",
    "\n",
    "# Attach embeddings to chunk records\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk[\"embedding\"] = embeddings[i]\n",
    "\n",
    "# Define explicit schema for the chunks table\n",
    "# This ensures correct types, especially for the embedding array\n",
    "chunks_schema = StructType([\n",
    "    StructField(\"chunk_id\", StringType(), False),      # Primary key (NOT NULL)\n",
    "    StructField(\"doc_id\", StringType(), False),        # Foreign key (NOT NULL)\n",
    "    StructField(\"title\", StringType(), True),          # Display field\n",
    "    StructField(\"category\", StringType(), True),       # Filter field\n",
    "    StructField(\"chunk_index\", IntegerType(), True),   # Position in document\n",
    "    StructField(\"total_chunks\", IntegerType(), True),  # Total chunks from doc\n",
    "    StructField(\"content\", StringType(), True),        # Text for LLM context\n",
    "    StructField(\"char_count\", IntegerType(), True),    # For analysis\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), True)  # Vector for search\n",
    "])\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "chunks_df = spark.createDataFrame(all_chunks, schema=chunks_schema)\n",
    "\n",
    "# Add indexing timestamp for audit trail\n",
    "chunks_df = chunks_df.withColumn(\"indexed_at\", current_timestamp())\n",
    "\n",
    "# Write to Delta table in Unity Catalog\n",
    "chunks_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(CHUNKS_TABLE_PATH)\n",
    "\n",
    "# CRITICAL: Enable Change Data Feed for Vector Search synchronization\n",
    "# Without this, Delta Sync indices cannot detect changes\n",
    "spark.sql(f\"ALTER TABLE {CHUNKS_TABLE_PATH} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "print(f\"✅ Chunks with embeddings saved to {CHUNKS_TABLE_PATH}\")\n",
    "print(f\"\uD83D\uDCCA Total rows: {spark.table(CHUNKS_TABLE_PATH).count()}\")\n",
    "\n",
    "# Display sample (excluding embedding column for readability)\n",
    "display(spark.table(CHUNKS_TABLE_PATH).select(\"chunk_id\", \"doc_id\", \"title\", \"category\", \"char_count\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bba6cf-7415-40f6-969a-ac9c0e24059a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating the Vector Search Index\n",
    "\n",
    "Vector Search is the core retrieval component of our RAG system. It enables fast similarity search over millions of vectors.\n",
    "\n",
    "### Vector Search Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Vector Search Endpoint                        │\n",
    "│  ┌─────────────────────────────────────────────────────────┐    │\n",
    "│  │                    Delta Sync Index                      │    │\n",
    "│  │  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐  │    │\n",
    "│  │  │   Vectors   │    │  Metadata   │    │   Content   │  │    │\n",
    "│  │  │  (1024-dim) │    │ (category)  │    │   (text)    │  │    │\n",
    "│  │  └─────────────┘    └─────────────┘    └─────────────┘  │    │\n",
    "│  └─────────────────────────────────────────────────────────┘    │\n",
    "│                              ▲                                   │\n",
    "│                              │ Auto-sync                         │\n",
    "│                    ┌─────────┴─────────┐                        │\n",
    "│                    │   Delta Table     │                        │\n",
    "│                    │ (Change Data Feed)│                        │\n",
    "│                    └───────────────────┘                        │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Endpoint** | Managed compute resource that hosts indices (like a server) |\n",
    "| **Delta Sync Index** | Index that auto-syncs with a Delta table |\n",
    "| **Direct Access Index** | Index managed via API (no auto-sync) |\n",
    "| **Primary Key** | Unique identifier for each vector (chunk_id) |\n",
    "| **Embedding Column** | Column containing the vector data |\n",
    "\n",
    "### Why Delta Sync Index?\n",
    "\n",
    "We use Delta Sync (not Direct Access) because:\n",
    "1. **Automatic updates** - No manual re-indexing when data changes\n",
    "2. **Consistency** - Index always reflects the latest table state\n",
    "3. **Simplicity** - Single source of truth in Delta table\n",
    "4. **Governance** - Inherits Unity Catalog permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf66cd83-c382-49e5-a44d-6b5aafdb5650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Vector Search Endpoint\n",
    "\n",
    "The Vector Search endpoint is a managed compute resource that hosts your vector indices.\n",
    "\n",
    "**Endpoint Characteristics:**\n",
    "- **Shared resource**: Multiple indices can run on one endpoint\n",
    "- **Auto-scaling**: Scales based on query load\n",
    "- **High availability**: Managed by Databricks\n",
    "- **Provisioning time**: 5-10 minutes for new endpoints\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Checks if the endpoint already exists\n",
    "2. If not, creates a new STANDARD endpoint\n",
    "3. Waits for the endpoint to reach ONLINE state\n",
    "4. Reports the final status\n",
    "\n",
    "**Note:** If you already have an endpoint in your workspace, you can reuse it by updating `VECTOR_SEARCH_ENDPOINT_NAME` in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73043a0-26ec-4a98-8df0-d3b13c054129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE VECTOR SEARCH ENDPOINT\n",
    "# ============================================================\n",
    "\n",
    "def create_vector_search_endpoint(endpoint_name: str) -> None:\n",
    "    \"\"\"Create a Vector Search endpoint if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        # Check if endpoint exists\n",
    "        endpoint = vector_search_client.get_endpoint(endpoint_name)\n",
    "        print(f\"✅ Endpoint '{endpoint_name}' already exists\")\n",
    "        print(f\"   Status: {endpoint.get('endpoint_status', {}).get('state', 'UNKNOWN')}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e) or \"NOT_FOUND\" in str(e):\n",
    "            print(f\"\uD83D\uDD04 Creating endpoint '{endpoint_name}'...\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Create new endpoint\n",
    "    vector_search_client.create_endpoint(\n",
    "        name=endpoint_name,\n",
    "        endpoint_type=\"STANDARD\"\n",
    "    )\n",
    "\n",
    "    print(f\"⏳ Endpoint creation initiated. This may take 5-10 minutes...\")\n",
    "\n",
    "    # Wait for endpoint to be ready\n",
    "    max_wait_time = 600  # 10 minutes\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        try:\n",
    "            endpoint = vector_search_client.get_endpoint(endpoint_name)\n",
    "            state = endpoint.get(\"endpoint_status\", {}).get(\"state\", \"UNKNOWN\")\n",
    "\n",
    "            if state == \"ONLINE\":\n",
    "                print(f\"✅ Endpoint '{endpoint_name}' is ready!\")\n",
    "                return\n",
    "            else:\n",
    "                print(f\"   Current state: {state}...\")\n",
    "                time.sleep(30)\n",
    "        except Exception as e:\n",
    "            print(f\"   Waiting for endpoint... ({str(e)[:50]})\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    print(f\"⚠️ Endpoint creation timed out. Please check the Databricks UI.\")\n",
    "\n",
    "# Create the endpoint\n",
    "create_vector_search_endpoint(VECTOR_SEARCH_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb67c20-cb55-4eab-84a9-4cb031040660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Delta Sync Vector Index\n",
    "\n",
    "We create a **Delta Sync Index** that automatically synchronizes with our Delta table.\n",
    "\n",
    "**Index Configuration Parameters:**\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `endpoint_name` | Your endpoint | Where to host the index |\n",
    "| `index_name` | Full Unity Catalog path | Unique identifier for the index |\n",
    "| `source_table_name` | Chunks table path | Delta table to sync from |\n",
    "| `primary_key` | `chunk_id` | Unique identifier for each vector |\n",
    "| `embedding_vector_column` | `embedding` | Column containing vectors |\n",
    "| `embedding_dimension` | 1024 | Must match embedding model output |\n",
    "| `pipeline_type` | `TRIGGERED` | Manual sync control (vs. CONTINUOUS) |\n",
    "| `columns_to_sync` | Metadata columns | Columns available for filtering/display |\n",
    "\n",
    "**Pipeline Types:**\n",
    "- **TRIGGERED**: Sync runs when you call `sync()` - better for batch updates\n",
    "- **CONTINUOUS**: Syncs automatically every few minutes - better for streaming\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Checks if the index already exists (idempotent)\n",
    "2. Creates a Delta Sync index with the specified configuration\n",
    "3. Waits for the index to become ready (initial sync)\n",
    "4. Reports the number of indexed rows\n",
    "\n",
    "**Note:** Initial index creation includes the first sync, which may take several minutes depending on data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373dc5f1-beed-4d61-845f-a8bfd7919c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE VECTOR SEARCH INDEX\n",
    "# ============================================================\n",
    "\n",
    "def create_vector_index(\n",
    "    endpoint_name: str,\n",
    "    index_name: str,\n",
    "    source_table: str,\n",
    "    primary_key: str,\n",
    "    embedding_column: str,\n",
    "    embedding_dimension: int\n",
    ") -> None:\n",
    "    \"\"\"Create a Delta Sync Vector Search index.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Check if index exists\n",
    "        index = vector_search_client.get_index(endpoint_name, index_name)\n",
    "        index_info = index.describe()\n",
    "        print(f\"✅ Index '{index_name}' already exists\")\n",
    "        print(f\"   Status: {index_info.get('status', {}).get('ready', False)}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e) or \"NOT_FOUND\" in str(e):\n",
    "            print(f\"\uD83D\uDD04 Creating index '{index_name}'...\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Create the index\n",
    "    vector_search_client.create_delta_sync_index(\n",
    "        endpoint_name=endpoint_name,\n",
    "        index_name=index_name,\n",
    "        source_table_name=source_table,\n",
    "        primary_key=primary_key,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        embedding_vector_column=embedding_column,\n",
    "        pipeline_type=\"TRIGGERED\",  # Use TRIGGERED for manual sync control\n",
    "        columns_to_sync=[\"chunk_id\", \"doc_id\", \"title\", \"category\", \"content\", \"chunk_index\"]\n",
    "    )\n",
    "\n",
    "    print(f\"⏳ Index creation initiated. This may take a few minutes...\")\n",
    "\n",
    "    # Wait for index to be ready\n",
    "    max_wait_time = 600  # 10 minutes\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        try:\n",
    "            index = vector_search_client.get_index(endpoint_name, index_name)\n",
    "            index_info = index.describe()\n",
    "            status = index_info.get(\"status\", {})\n",
    "            ready = status.get(\"ready\", False)\n",
    "\n",
    "            if ready:\n",
    "                print(f\"✅ Index '{index_name}' is ready!\")\n",
    "                print(f\"   Indexed rows: {status.get('num_rows', 'N/A')}\")\n",
    "                return\n",
    "            else:\n",
    "                state = status.get(\"detailed_state\", \"UNKNOWN\")\n",
    "                print(f\"   Current state: {state}...\")\n",
    "                time.sleep(30)\n",
    "        except Exception as e:\n",
    "            print(f\"   Waiting for index... ({str(e)[:50]})\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    print(f\"⚠️ Index creation timed out. Please check the Databricks UI.\")\n",
    "\n",
    "# Create the vector index\n",
    "create_vector_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=VECTOR_INDEX_PATH,\n",
    "    source_table=CHUNKS_TABLE_PATH,\n",
    "    primary_key=\"chunk_id\",\n",
    "    embedding_column=\"embedding\",\n",
    "    embedding_dimension=EMBEDDING_DIMENSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce85cbf-9d72-4451-9b99-0d6549d750cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify Index Status and Test Query\n",
    "\n",
    "Before proceeding, we verify the index is operational by:\n",
    "1. Checking the index status and metadata\n",
    "2. Running a test similarity search\n",
    "3. Validating the result format\n",
    "\n",
    "**Understanding Similarity Scores:**\n",
    "- Scores range from 0 to 1 (higher = more similar)\n",
    "- Scores above 0.7 typically indicate strong relevance\n",
    "- Scores below 0.5 may indicate weak matches\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Gets a reference to the index object\n",
    "2. Calls `describe()` to retrieve index metadata\n",
    "3. Generates an embedding for a test query\n",
    "4. Executes a similarity search with `num_results=TOP_K_RESULTS`\n",
    "5. Displays results with scores and content previews\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If status is not \"ready\", wait for sync to complete\n",
    "- If no results, verify the embedding dimension matches\n",
    "- If scores are low, check that the query is relevant to your content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78deedc-1266-4b61-8b06-d76471bc6167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY INDEX AND TEST QUERY\n",
    "# ============================================================\n",
    "\n",
    "# Get a reference to the index object\n",
    "index = vector_search_client.get_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=VECTOR_INDEX_PATH\n",
    ")\n",
    "\n",
    "# Retrieve and display index metadata\n",
    "index_info = index.describe()\n",
    "print(\"\uD83D\uDCCA Index Information:\")\n",
    "print(f\"   Name: {VECTOR_INDEX_PATH}\")\n",
    "print(f\"   Endpoint: {VECTOR_SEARCH_ENDPOINT_NAME}\")\n",
    "print(f\"   Status: {index_info.get('status', {})}\")\n",
    "\n",
    "# Test query to verify the index is working\n",
    "test_query = \"How do I authenticate users with the API?\"\n",
    "print(f\"\\n\uD83D\uDD0D Test Query: '{test_query}'\")\n",
    "\n",
    "# Generate embedding for the query using the same model used for indexing\n",
    "query_embedding = embedding_model.embed_query(test_query)\n",
    "\n",
    "# Execute similarity search against the index\n",
    "results = index.similarity_search(\n",
    "    query_vector=query_embedding,\n",
    "    num_results=TOP_K_RESULTS,\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"title\", \"category\", \"content\"]\n",
    ")\n",
    "\n",
    "# Display results with similarity scores\n",
    "print(f\"\\n\uD83D\uDCCB Top {TOP_K_RESULTS} Results:\")\n",
    "for i, result in enumerate(results.get(\"result\", {}).get(\"data_array\", [])):\n",
    "    # Result format: [chunk_id, doc_id, title, category, content, score]\n",
    "    print(f\"\\n{i+1}. {result[2]} (Score: {result[-1]:.4f})\")\n",
    "    print(f\"   Category: {result[3]}\")\n",
    "    print(f\"   Content preview: {result[4][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e38b4c-d878-4866-a68d-a075eaa71157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 5: Configuring the Retriever for Semantic Search\n",
    "\n",
    "The retriever is the bridge between user queries and the Vector Search index. It provides a clean, LangChain-compatible interface for the RAG pipeline.\n",
    "\n",
    "### Retriever Architecture\n",
    "\n",
    "```\n",
    "┌──────────────┐    ┌──────────────────────┐    ┌──────────────┐\n",
    "│  User Query  │───▶│  DatabricksVector    │───▶│   Vector     │\n",
    "│   (text)     │    │  Search Retriever    │    │   Search     │\n",
    "└──────────────┘    └──────────────────────┘    │   Index      │\n",
    "                              │                  └──────────────┘\n",
    "                              ▼\n",
    "                    ┌──────────────────────┐\n",
    "                    │  LangChain Documents │\n",
    "                    │  (page_content +     │\n",
    "                    │   metadata)          │\n",
    "                    └──────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Configuration Options\n",
    "\n",
    "| Parameter | Description | Our Setting |\n",
    "|-----------|-------------|-------------|\n",
    "| `text_column` | Column containing document text | `content` |\n",
    "| `columns` | Metadata columns to include | chunk_id, doc_id, title, category |\n",
    "| `search_type` | Similarity algorithm | `similarity` (cosine) |\n",
    "| `k` | Number of results to return | 5 (configurable) |\n",
    "| `filters` | Metadata filters | Optional (e.g., category) |\n",
    "\n",
    "**Why LangChain Integration?**\n",
    "- Standardized interface compatible with any LLM\n",
    "- Easy to swap retrieval backends\n",
    "- Built-in support for chains and agents\n",
    "- Consistent document format (page_content + metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d11fae8-5bb0-4e04-9903-d1d29be2c6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the LangChain Retriever\n",
    "\n",
    "This cell creates a `DatabricksVectorSearch` wrapper and converts it to a LangChain retriever.\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Creates a `DatabricksVectorSearch` object wrapping our index\n",
    "2. Specifies which column contains the text content\n",
    "3. Lists metadata columns to include in results\n",
    "4. Converts to a retriever with search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c89d650-3753-4a97-ae0a-72a491dee333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE LANGCHAIN RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "# Create DatabricksVectorSearch wrapper\n",
    "# This provides LangChain compatibility for our Vector Search index\n",
    "vector_store = DatabricksVectorSearch(\n",
    "    index=index,                    # The Vector Search index object\n",
    "    embedding=embedding_model,      # Same embedding model used for indexing\n",
    "    text_column=\"content\",          # Column containing document text\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"title\", \"category\", \"chunk_index\"]  # Metadata\n",
    ")\n",
    "\n",
    "# Convert to a LangChain retriever\n",
    "# The retriever provides a simple invoke(query) interface\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",       # Use cosine similarity\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K_RESULTS,         # Number of results to return\n",
    "        # Optional: Add filter for metadata-based filtering\n",
    "        # \"filter\": {\"category\": \"technical\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ Retriever configured successfully!\")\n",
    "print(f\"   Search type: similarity\")\n",
    "print(f\"   Top-K results: {TOP_K_RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d829b438-a62d-481b-8ebf-67e4c33ac14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the Retriever\n",
    "\n",
    "We test the retriever with diverse queries to validate:\n",
    "1. **Relevance** - Are the returned documents related to the query?\n",
    "2. **Diversity** - Do results come from different document types?\n",
    "3. **Metadata** - Is metadata correctly attached to results?\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Defines a set of test queries covering different topics\n",
    "2. Invokes the retriever for each query\n",
    "3. Displays the top 3 results with metadata\n",
    "4. Allows you to assess retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957e1431-7e7b-4a86-a3ed-194b3a746b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST RETRIEVER WITH SAMPLE QUERIES\n",
    "# ============================================================\n",
    "\n",
    "# Test queries covering different document categories\n",
    "test_queries = [\n",
    "    \"How do I troubleshoot Kubernetes pod crashes?\",\n",
    "    \"What is the data retention policy for PII?\",\n",
    "    \"What was the root cause of the payment outage?\",\n",
    "    \"How do I deploy a machine learning model?\",\n",
    "    \"What are the code review best practices?\"\n",
    "]\n",
    "\n",
    "print(\"\uD83D\uDD0D Testing Retriever with Sample Queries\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\uD83D\uDCDD Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Invoke the retriever - returns List[Document]\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    # Display top 3 results\n",
    "    for i, doc in enumerate(docs[:3]):\n",
    "        print(f\"\\n   Result {i+1}: {doc.metadata.get('title', 'N/A')}\")\n",
    "        print(f\"   Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "        print(f\"   Preview: {doc.page_content[:100]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32148031-9bb3-40ec-9b86-9997543e8911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement Filtered Retrieval\n",
    "\n",
    "Filtering narrows search results based on metadata. This is essential for:\n",
    "- **Multi-tenant systems** - Restrict results to user's organization\n",
    "- **Category-specific search** - Search only technical docs or policies\n",
    "- **Time-based filtering** - Find recent documents only\n",
    "- **Access control** - Enforce document-level permissions\n",
    "\n",
    "**Filter Syntax:**\n",
    "```python\n",
    "filters = {\"category\": \"technical\"}           # Exact match\n",
    "filters = {\"category IN\": [\"technical\", \"wiki\"]}  # Multiple values\n",
    "```\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Defines a helper function for filtered retrieval\n",
    "2. Demonstrates filtering by category\n",
    "3. Shows how filters affect result sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf5ec75-eae1-4f5a-aa3f-6612d9a8ba6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FILTERED RETRIEVAL EXAMPLES\n",
    "# ============================================================\n",
    "\n",
    "def retrieve_with_filter(query: str, category_filter: str = None, top_k: int = 5) -> List:\n",
    "    \"\"\"Retrieve documents with optional category filtering.\"\"\"\n",
    "\n",
    "    search_kwargs = {\"k\": top_k}\n",
    "\n",
    "    if category_filter:\n",
    "        search_kwargs[\"filter\"] = {\"category\": category_filter}\n",
    "\n",
    "    filtered_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "\n",
    "    return filtered_retriever.invoke(query)\n",
    "\n",
    "# Test filtered retrieval\n",
    "print(\"\uD83D\uDD0D Filtered Retrieval Examples\\n\")\n",
    "\n",
    "# Search only in technical documents\n",
    "print(\"\uD83D\uDCC1 Category: technical\")\n",
    "print(\"-\" * 40)\n",
    "tech_results = retrieve_with_filter(\n",
    "    \"How do I handle authentication errors?\",\n",
    "    category_filter=\"technical\"\n",
    ")\n",
    "for doc in tech_results[:3]:\n",
    "    print(f\"   • {doc.metadata.get('title')}\")\n",
    "\n",
    "# Search only in operational reports\n",
    "print(\"\\n\uD83D\uDCC1 Category: operational\")\n",
    "print(\"-\" * 40)\n",
    "ops_results = retrieve_with_filter(\n",
    "    \"What incidents occurred recently?\",\n",
    "    category_filter=\"operational\"\n",
    ")\n",
    "for doc in ops_results[:3]:\n",
    "    print(f\"   • {doc.metadata.get('title')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5b554a-2d7f-4d51-82cb-582aff4bfb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 6: Serving LLM and Embedding Models with Mosaic AI\n",
    "\n",
    "This section brings together retrieval and generation to create a complete RAG system.\n",
    "\n",
    "### RAG Pipeline Architecture\n",
    "\n",
    "```\n",
    "┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n",
    "│  User Query  │───▶│  Retriever   │───▶│   Prompt     │───▶│     LLM      │\n",
    "│              │    │  (Vector     │    │  Template    │    │  (Llama 3.3) │\n",
    "│              │    │   Search)    │    │              │    │              │\n",
    "└──────────────┘    └──────────────┘    └──────────────┘    └──────────────┘\n",
    "                           │                   │                    │\n",
    "                           ▼                   ▼                    ▼\n",
    "                    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n",
    "                    │  Retrieved   │    │   Context +  │    │   Generated  │\n",
    "                    │  Documents   │    │   Question   │    │   Response   │\n",
    "                    └──────────────┘    └──────────────┘    └──────────────┘\n",
    "```\n",
    "\n",
    "### Mosaic AI Foundation Models\n",
    "\n",
    "Databricks provides pre-deployed Foundation Models with:\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Pay-per-token** | No idle compute costs |\n",
    "| **Auto-scaling** | Handles traffic spikes automatically |\n",
    "| **Low latency** | Optimized inference infrastructure |\n",
    "| **Enterprise security** | VPC, encryption, audit logs |\n",
    "| **No management** | Databricks handles updates and maintenance |\n",
    "\n",
    "### Available Models in Your Workspace\n",
    "\n",
    "Based on the endpoint discovery at the time of this lab from previous cell you have access to:\n",
    "- **Llama 3.3 70B** - Excellent balance of quality and speed (configured)\n",
    "- **Llama 3.1 405B** - Highest quality, slower\n",
    "- **Claude Sonnet 4** - Strong reasoning capabilities\n",
    "- **Gemma 3 12B** - Fastest, good for simple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea408d8-ae70-4875-abe5-9578a175ed40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure LLM for Response Generation\n",
    "\n",
    "We configure the `ChatDatabricks` wrapper to use the Llama 3.3 70B model.\n",
    "\n",
    "**LLM Parameters:**\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `endpoint` | databricks-meta-llama-3-3-70b-instruct | Model to use |\n",
    "| `temperature` | 0.1 | Low = more deterministic, focused responses |\n",
    "| `max_tokens` | 1024 | Maximum response length |\n",
    "\n",
    "**Temperature Tuning (Chapter 9 Insight):**\n",
    "- **0.0-0.3**: Factual, consistent responses (best for RAG)\n",
    "- **0.4-0.7**: Balanced creativity and accuracy\n",
    "- **0.8-1.0**: Creative, varied responses (not recommended for RAG)\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Initializes the ChatDatabricks LLM wrapper\n",
    "2. Sends a test query to verify the endpoint is working\n",
    "3. Displays the response to confirm connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a15161f-9ed0-4e25-85fa-c0fc3bd5f474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE LLM FOR RESPONSE GENERATION\n",
    "# ============================================================\n",
    "\n",
    "# Initialize the LLM with tuned parameters\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_MODEL_NAME,    # Foundation Model endpoint\n",
    "    temperature=0.1,            # Low temperature for focused, factual responses\n",
    "    max_tokens=1024,            # Maximum tokens in the response\n",
    ")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"What is retrieval-augmented generation in one sentence?\")\n",
    "print(\"\uD83E\uDD16 LLM Test Response:\")\n",
    "print(f\"   {test_response.content}\")\n",
    "print(f\"\\n✅ LLM configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0e47fa-5c12-40ed-b06d-c029de79419a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build the Complete RAG Pipeline\n",
    "\n",
    "Now we combine the retriever and LLM into a complete RAG pipeline using LangChain Expression Language (LCEL).\n",
    "\n",
    "**Pipeline Components:**\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `retriever` | Fetches relevant documents from Vector Search |\n",
    "| `format_docs` | Converts Document objects to a context string |\n",
    "| `rag_prompt` | Template that combines context and question |\n",
    "| `llm` | Generates the final response |\n",
    "| `StrOutputParser` | Extracts the text content from the response |\n",
    "\n",
    "**Prompt Engineering Best Practices:**\n",
    "1. **Clear instructions** - Tell the model exactly what to do\n",
    "2. **Context first** - Place retrieved documents before the question\n",
    "3. **Explicit constraints** - \"Use only the information from the context\"\n",
    "4. **Fallback behavior** - \"If the context doesn't contain enough information, say so\"\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Imports LangChain core components\n",
    "2. Defines a RAG prompt template with clear instructions\n",
    "3. Creates a document formatting function\n",
    "4. Builds the chain using LCEL pipe syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2367805-b207-4d1b-bc5d-94faf354ffb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD RAG PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the RAG prompt template\n",
    "# This template instructs the LLM to answer based only on provided context\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "Use only the information from the context to answer the question. If the context doesn't contain\n",
    "enough information to answer the question, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear, concise answer based on the context above.\"\"\"\n",
    "\n",
    "# Create the prompt template object\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single context string.\n",
    "\n",
    "    Each document is labeled with its title for attribution.\n",
    "    This helps the LLM understand the source of information.\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted.append(f\"[Document {i+1}: {doc.metadata.get('title', 'Unknown')}]\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Build the RAG chain using LangChain Expression Language (LCEL)\n",
    "# The pipe (|) operator chains components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs, then format\n",
    "        \"question\": RunnablePassthrough()    # Pass question through unchanged\n",
    "    }\n",
    "    | rag_prompt    # Combine into prompt\n",
    "    | llm           # Generate response\n",
    "    | StrOutputParser()  # Extract text content\n",
    ")\n",
    "\n",
    "print(\"✅ RAG pipeline built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9a4bb2-eaab-49fa-bd84-215da7760602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the RAG Pipeline\n",
    "\n",
    "Now we test the complete end-to-end RAG pipeline with diverse queries.\n",
    "\n",
    "**What to Look For:**\n",
    "1. **Accuracy** - Does the answer match the source documents?\n",
    "2. **Relevance** - Is the answer focused on the question?\n",
    "3. **Attribution** - Can you trace the answer to specific documents?\n",
    "4. **Latency** - Is the response time acceptable?\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Defines test queries covering different document categories\n",
    "2. Invokes the RAG chain for each query\n",
    "3. Measures and displays response time\n",
    "4. Shows the generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b8310c-d902-45a3-912e-18b8a59c47d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST RAG PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "# Test queries covering different document types and topics\n",
    "rag_test_queries = [\n",
    "    \"What are the steps to authenticate a user using the API?\",\n",
    "    \"How should I handle a Pod CrashLoopBackOff error in Kubernetes?\",\n",
    "    \"What was the impact of the payment processing outage?\",\n",
    "    \"What are the data retention policies for different types of data?\",\n",
    "    \"How do I deploy a machine learning model to production?\"\n",
    "]\n",
    "\n",
    "print(\"\uD83D\uDE80 Testing RAG Pipeline\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in rag_test_queries:\n",
    "    print(f\"\\n❓ Question: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Invoke the RAG chain and measure latency\n",
    "    start_time = time.time()\n",
    "    response = rag_chain.invoke(query)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n\uD83D\uDCAC Answer:\\n{response}\")\n",
    "    print(f\"\\n⏱️  Response time: {elapsed:.2f} seconds\")\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d307ce7b-a1f4-4690-8b00-e8781adc77a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 7: Batching and Context Length Optimization\n",
    "\n",
    "Production RAG systems must handle high query volumes efficiently. This section covers key optimization techniques from Chapter 9.\n",
    "\n",
    "### Optimization Strategies Overview\n",
    "\n",
    "| Strategy | Benefit | Trade-off |\n",
    "|----------|---------|-----------|\n",
    "| **Batch processing** | Higher throughput | Increased latency for individual queries |\n",
    "| **Parallel execution** | Reduced total time | Higher resource consumption |\n",
    "| **Context length tuning** | Lower token costs | Potential loss of relevant context |\n",
    "| **Caching** | Faster repeated queries | Memory usage, staleness |\n",
    "\n",
    "### Why These Optimizations Matter\n",
    "\n",
    "Consider a production scenario:\n",
    "- **1000 queries/day** at 3 seconds each = 50 minutes of compute\n",
    "- **With 4x parallelism** = 12.5 minutes of compute\n",
    "- **With caching (50% hit rate)** = 6.25 minutes of compute\n",
    "\n",
    "These optimizations directly impact cost and user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f476c7ce-4c2d-4294-957a-776a46b9602c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch Query Processing\n",
    "\n",
    "Batch processing enables handling multiple queries efficiently using parallel execution.\n",
    "\n",
    "**Concurrency Considerations:**\n",
    "- **max_workers=1**: Sequential processing, lowest resource usage\n",
    "- **max_workers=4**: Good balance for most workloads\n",
    "- **max_workers=8+**: High throughput, but may hit rate limits\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Defines a single-query processing function with timing\n",
    "2. Implements a batch processor using ThreadPoolExecutor\n",
    "3. Tracks progress and calculates throughput metrics\n",
    "4. Returns structured results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bf7150-e678-4d68-9f1e-c4e01a06074d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BATCH QUERY PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "import concurrent.futures\n",
    "from typing import Tuple\n",
    "\n",
    "def process_query(query: str) -> Tuple[str, str, float]:\n",
    "    \"\"\"Process a single query and return the result with timing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = rag_chain.invoke(query)\n",
    "    elapsed = time.time() - start_time\n",
    "    return query, response, elapsed\n",
    "\n",
    "def batch_process_queries(\n",
    "    queries: List[str],\n",
    "    max_workers: int = 4,\n",
    "    show_progress: bool = True\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Process multiple queries in parallel.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"\uD83D\uDD04 Processing {len(queries)} queries with {max_workers} workers...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_query = {\n",
    "            executor.submit(process_query, query): query\n",
    "            for query in queries\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_query):\n",
    "            query, response, elapsed = future.result()\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"latency\": elapsed\n",
    "            })\n",
    "\n",
    "            if show_progress:\n",
    "                print(f\"   ✓ Completed: {query[:50]}... ({elapsed:.2f}s)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"\\n✅ Batch processing complete!\")\n",
    "        print(f\"   Total time: {total_time:.2f}s\")\n",
    "        print(f\"   Throughput: {len(queries) / total_time:.2f} queries/second\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test batch processing\n",
    "batch_queries = [\n",
    "    \"What is the authentication flow?\",\n",
    "    \"How do I check Kubernetes pod logs?\",\n",
    "    \"What is the data pipeline architecture?\",\n",
    "    \"What are the on-call responsibilities?\",\n",
    "    \"What was the Q4 system uptime?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_process_queries(batch_queries, max_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97609319-49ca-4778-94fe-19f4b9d15969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Context Length Tuning\n",
    "\n",
    "Context length is a critical tuning parameter that affects:\n",
    "1. **Response quality** - More context = more information for the LLM\n",
    "2. **Token costs** - More context = higher API costs\n",
    "3. **Latency** - More context = longer processing time\n",
    "4. **Relevance dilution** - Too much context = irrelevant information\n",
    "\n",
    "**Tuning Strategy (Chapter 9 Insight):**\n",
    "\n",
    "| Scenario | Recommended Setting |\n",
    "|----------|---------------------|\n",
    "| Simple factual queries | top_k=3, 2000 chars |\n",
    "| Complex multi-part questions | top_k=5-8, 4000-6000 chars |\n",
    "| Summarization tasks | top_k=10+, 8000+ chars |\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Creates a function to build RAG chains with custom parameters\n",
    "2. Implements context truncation to enforce character limits\n",
    "3. Tests three configurations (small, medium, large)\n",
    "4. Compares response quality and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91f5f97-3a79-45a4-bae7-719f96073c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONTEXT LENGTH TUNING EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "def create_tuned_rag_chain(top_k: int, max_context_chars: int = 4000):\n",
    "    \"\"\"Create a RAG chain with tuned context parameters.\"\"\"\n",
    "\n",
    "    # Create retriever with custom top-k\n",
    "    tuned_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": top_k}\n",
    "    )\n",
    "\n",
    "    def format_docs_with_limit(docs):\n",
    "        \"\"\"Format docs with character limit to control context length.\"\"\"\n",
    "        formatted = []\n",
    "        total_chars = 0\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_text = f\"[Document {i+1}: {doc.metadata.get('title', 'Unknown')}]\\n{doc.page_content}\"\n",
    "\n",
    "            if total_chars + len(doc_text) > max_context_chars:\n",
    "                # Truncate if exceeding limit\n",
    "                remaining = max_context_chars - total_chars\n",
    "                if remaining > 100:  # Only add if meaningful content remains\n",
    "                    formatted.append(doc_text[:remaining] + \"...\")\n",
    "                break\n",
    "\n",
    "            formatted.append(doc_text)\n",
    "            total_chars += len(doc_text)\n",
    "\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    # Build tuned chain\n",
    "    tuned_chain = (\n",
    "        {\n",
    "            \"context\": tuned_retriever | format_docs_with_limit,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return tuned_chain\n",
    "\n",
    "# Experiment with different configurations\n",
    "configurations = [\n",
    "    {\"top_k\": 3, \"max_context_chars\": 2000, \"name\": \"Small Context\"},\n",
    "    {\"top_k\": 5, \"max_context_chars\": 4000, \"name\": \"Medium Context\"},\n",
    "    {\"top_k\": 8, \"max_context_chars\": 6000, \"name\": \"Large Context\"},\n",
    "]\n",
    "\n",
    "test_query = \"What are the best practices for code review and what should be included in the checklist?\"\n",
    "\n",
    "print(\"\uD83D\uDCCA Context Length Tuning Experiments\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\n\uD83D\uDD27 Configuration: {config['name']}\")\n",
    "    print(f\"   Top-K: {config['top_k']}, Max Context: {config['max_context_chars']} chars\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    tuned_chain = create_tuned_rag_chain(\n",
    "        top_k=config[\"top_k\"],\n",
    "        max_context_chars=config[\"max_context_chars\"]\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = tuned_chain.invoke(test_query)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n   Response preview: {response[:200]}...\")\n",
    "    print(f\"   Response length: {len(response)} chars\")\n",
    "    print(f\"   Latency: {elapsed:.2f}s\")\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d2cf9a-6d03-4f3a-8c9d-0e77ee1ccac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 8: Performance Profiling and Bottleneck Identification\n",
    "\n",
    "Performance profiling is essential for optimizing RAG systems. This section implements comprehensive profiling to identify bottlenecks.\n",
    "\n",
    "### RAG Pipeline Latency Breakdown\n",
    "\n",
    "A typical RAG query involves four stages, each with different latency characteristics:\n",
    "\n",
    "| Stage | Typical Latency | Optimization Levers |\n",
    "|-------|-----------------|---------------------|\n",
    "| **Query Embedding** | 50-200ms | Batch queries, cache embeddings |\n",
    "| **Vector Search** | 10-100ms | Index tuning, reduce top-k |\n",
    "| **Context Formatting** | 1-10ms | Optimize string operations |\n",
    "| **LLM Generation** | 500-5000ms | Smaller model, shorter context |\n",
    "\n",
    "**Key Insight:** LLM generation is typically the bottleneck (60-90% of total latency). Focus optimization efforts there first.\n",
    "\n",
    "### Profiling Methodology\n",
    "\n",
    "We measure:\n",
    "1. **Latency per stage** - Where is time spent?\n",
    "2. **P50/P95 percentiles** - What's the typical vs. worst-case latency?\n",
    "3. **Throughput** - How many queries per second?\n",
    "4. **Resource utilization** - Are we hitting rate limits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b33033-7c27-4d3d-868f-745f32d176b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Component-Level Profiling\n",
    "\n",
    "The `RAGProfiler` class instruments each stage of the pipeline to collect detailed metrics.\n",
    "\n",
    "**Metrics Collected:**\n",
    "- `embedding_latency` - Time to convert query to vector\n",
    "- `search_latency` - Time for Vector Search similarity query\n",
    "- `formatting_latency` - Time to format documents into context\n",
    "- `generation_latency` - Time for LLM to generate response\n",
    "- `total_latency` - End-to-end query time\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Defines the `RAGProfiler` class with timing instrumentation\n",
    "2. Implements `profile_query()` to measure a single query\n",
    "3. Implements `get_summary()` to aggregate statistics\n",
    "4. Provides percentile calculations for SLA monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0500dd60-9477-4090-b8e7-e7968ef9ac2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT-LEVEL PROFILING\n",
    "# ============================================================\n",
    "\n",
    "class RAGProfiler:\n",
    "    \"\"\"Profiler for measuring RAG pipeline component latencies.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model, vector_index, llm, retriever):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_index = vector_index\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.metrics = []\n",
    "\n",
    "    def profile_query(self, query: str, top_k: int = 5) -> Dict:\n",
    "        \"\"\"Profile a single query through all pipeline stages.\"\"\"\n",
    "        metrics = {\"query\": query, \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        # Stage 1: Query Embedding\n",
    "        start = time.time()\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        metrics[\"embedding_latency\"] = time.time() - start\n",
    "\n",
    "        # Stage 2: Vector Search\n",
    "        start = time.time()\n",
    "        search_results = self.vector_index.similarity_search(\n",
    "            query_vector=query_embedding,\n",
    "            num_results=top_k,\n",
    "            columns=[\"chunk_id\", \"doc_id\", \"title\", \"category\", \"content\"]\n",
    "        )\n",
    "        metrics[\"search_latency\"] = time.time() - start\n",
    "        metrics[\"results_count\"] = len(search_results.get(\"result\", {}).get(\"data_array\", []))\n",
    "\n",
    "        # Stage 3: Context Formatting\n",
    "        start = time.time()\n",
    "        docs = self.retriever.invoke(query)\n",
    "        context = format_docs(docs)\n",
    "        metrics[\"formatting_latency\"] = time.time() - start\n",
    "        metrics[\"context_length\"] = len(context)\n",
    "\n",
    "        # Stage 4: LLM Generation\n",
    "        start = time.time()\n",
    "        prompt = rag_prompt.format(context=context, question=query)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        metrics[\"generation_latency\"] = time.time() - start\n",
    "        metrics[\"response_length\"] = len(response.content)\n",
    "\n",
    "        # Total latency\n",
    "        metrics[\"total_latency\"] = (\n",
    "            metrics[\"embedding_latency\"] +\n",
    "            metrics[\"search_latency\"] +\n",
    "            metrics[\"formatting_latency\"] +\n",
    "            metrics[\"generation_latency\"]\n",
    "        )\n",
    "\n",
    "        self.metrics.append(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary statistics for all profiled queries.\"\"\"\n",
    "        if not self.metrics:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(self.metrics)\n",
    "\n",
    "        summary = {\n",
    "            \"Metric\": [\"Embedding\", \"Search\", \"Formatting\", \"Generation\", \"Total\"],\n",
    "            \"Mean (s)\": [\n",
    "                df[\"embedding_latency\"].mean(),\n",
    "                df[\"search_latency\"].mean(),\n",
    "                df[\"formatting_latency\"].mean(),\n",
    "                df[\"generation_latency\"].mean(),\n",
    "                df[\"total_latency\"].mean()\n",
    "            ],\n",
    "            \"P50 (s)\": [\n",
    "                df[\"embedding_latency\"].median(),\n",
    "                df[\"search_latency\"].median(),\n",
    "                df[\"formatting_latency\"].median(),\n",
    "                df[\"generation_latency\"].median(),\n",
    "                df[\"total_latency\"].median()\n",
    "            ],\n",
    "            \"P95 (s)\": [\n",
    "                df[\"embedding_latency\"].quantile(0.95),\n",
    "                df[\"search_latency\"].quantile(0.95),\n",
    "                df[\"formatting_latency\"].quantile(0.95),\n",
    "                df[\"generation_latency\"].quantile(0.95),\n",
    "                df[\"total_latency\"].quantile(0.95)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(summary)\n",
    "\n",
    "# Initialize profiler\n",
    "profiler = RAGProfiler(\n",
    "    embedding_model=embedding_model,\n",
    "    vector_index=index,\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Profiler initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3f564b-db2e-43f1-84f9-59d93110453a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Profiling Benchmark\n",
    "\n",
    "Let's profile multiple queries to gather performance statistics and identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc29e35-8836-47bf-a399-95ff093fd50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN PROFILING BENCHMARK\n",
    "# ============================================================\n",
    "\n",
    "profiling_queries = [\n",
    "    \"What is the authentication API endpoint for login?\",\n",
    "    \"How do I troubleshoot ImagePullBackOff errors?\",\n",
    "    \"What is the medallion architecture?\",\n",
    "    \"What are the code review response time expectations?\",\n",
    "    \"What was the root cause of the payment outage?\",\n",
    "    \"How do I register a model in Unity Catalog?\",\n",
    "    \"What is the data retention policy for transaction data?\",\n",
    "    \"What are the on-call escalation paths?\",\n",
    "    \"What was the Q4 uptime percentage?\",\n",
    "    \"How do I configure liveness probes in Kubernetes?\"\n",
    "]\n",
    "\n",
    "print(\"\uD83D\uDD2C Running Profiling Benchmark\\n\")\n",
    "print(f\"   Queries: {len(profiling_queries)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, query in enumerate(profiling_queries):\n",
    "    metrics = profiler.profile_query(query)\n",
    "    print(f\"\\n{i+1}. {query[:50]}...\")\n",
    "    print(f\"   Embedding: {metrics['embedding_latency']:.3f}s | \"\n",
    "          f\"Search: {metrics['search_latency']:.3f}s | \"\n",
    "          f\"Generation: {metrics['generation_latency']:.3f}s | \"\n",
    "          f\"Total: {metrics['total_latency']:.2f}s\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83D\uDCCA Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "summary_df = profiler.get_summary()\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f655e7-a250-4896-b7e9-51813cb02202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identify Bottlenecks\n",
    "\n",
    "This cell analyzes the profiling data to identify the primary bottleneck and provide targeted optimization recommendations.\n",
    "\n",
    "**Bottleneck Analysis Approach:**\n",
    "1. Calculate total time spent in each stage across all queries\n",
    "2. Compute percentage distribution\n",
    "3. Identify the stage consuming the most time\n",
    "4. Provide specific recommendations based on the bottleneck\n",
    "\n",
    "**Common Bottleneck Patterns:**\n",
    "\n",
    "| Bottleneck | Typical Cause | Quick Wins |\n",
    "|------------|---------------|------------|\n",
    "| LLM Generation | Large context, complex queries | Reduce context, use faster model |\n",
    "| Vector Search | Large index, high top-k | Reduce top-k, add filters |\n",
    "| Embedding | No caching, large batches | Cache common queries |\n",
    "| Formatting | Complex logic | Simplify, pre-compute |\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Aggregates timing data from all profiled queries\n",
    "2. Calculates percentage of time in each stage\n",
    "3. Visualizes the distribution with a bar chart\n",
    "4. Identifies the primary bottleneck\n",
    "5. Provides actionable optimization recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac2f69a-d679-43cc-9fb5-bfe352124761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BOTTLENECK ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "metrics_df = pd.DataFrame(profiler.metrics)\n",
    "\n",
    "# Calculate percentage of time spent in each stage\n",
    "total_time = metrics_df[\"total_latency\"].sum()\n",
    "stage_times = {\n",
    "    \"Embedding\": metrics_df[\"embedding_latency\"].sum(),\n",
    "    \"Vector Search\": metrics_df[\"search_latency\"].sum(),\n",
    "    \"Formatting\": metrics_df[\"formatting_latency\"].sum(),\n",
    "    \"LLM Generation\": metrics_df[\"generation_latency\"].sum()\n",
    "}\n",
    "\n",
    "print(\"\uD83D\uDD0D Bottleneck Analysis\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\uD83D\uDCCA Time Distribution by Component:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "bottleneck = None\n",
    "max_pct = 0\n",
    "\n",
    "for stage, time_spent in stage_times.items():\n",
    "    pct = (time_spent / total_time) * 100\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"   {stage:15} {pct:5.1f}% {bar}\")\n",
    "\n",
    "    if pct > max_pct:\n",
    "        max_pct = pct\n",
    "        bottleneck = stage\n",
    "\n",
    "print(f\"\\n⚠️  Primary Bottleneck: {bottleneck} ({max_pct:.1f}% of total time)\")\n",
    "\n",
    "# Recommendations based on bottleneck\n",
    "print(\"\\n\uD83D\uDCA1 Optimization Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if bottleneck == \"LLM Generation\":\n",
    "    print(\"\"\"\n",
    "   • Consider using a smaller/faster model for simple queries\n",
    "   • Reduce max_tokens if responses are being truncated anyway\n",
    "   • Implement response caching for common queries\n",
    "   • Use streaming for better perceived latency\n",
    "\"\"\")\n",
    "elif bottleneck == \"Vector Search\":\n",
    "    print(\"\"\"\n",
    "   • Reduce the number of results (top-k)\n",
    "   • Optimize index configuration (e.g., HNSW parameters)\n",
    "   • Consider using approximate search for faster results\n",
    "   • Pre-filter using metadata to reduce search space\n",
    "\"\"\")\n",
    "elif bottleneck == \"Embedding\":\n",
    "    print(\"\"\"\n",
    "   • Cache embeddings for frequently asked queries\n",
    "   • Use a smaller embedding model if accuracy permits\n",
    "   • Batch multiple queries together\n",
    "   • Consider using quantized embeddings\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "   • Optimize document formatting logic\n",
    "   • Reduce context length if possible\n",
    "   • Pre-compute formatted chunks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83cd94d-2ef7-4b9e-a090-6a0dfc13e4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 9: Optimization Strategies from Chapter 9\n",
    "\n",
    "In this section, we apply the optimization strategies introduced in Chapter 9:\n",
    "\n",
    "1. **Context-Length Tuning**: Optimize the amount of context passed to the LLM\n",
    "2. **Embedding Dimensionality Adjustments**: Trade-off between accuracy and speed\n",
    "3. **Batching Optimization**: Improve throughput with efficient batch processing\n",
    "4. **Index-Level Performance Improvements**: Optimize Vector Search configuration\n",
    "5. **Query Caching**: Reduce redundant computations for repeated queries\n",
    "6. **Hybrid Search**: Combine semantic and keyword search for better relevance\n",
    "7. **Adaptive Retrieval**: Dynamically adjust parameters based on query type\n",
    "\n",
    "These patterns address common RAG performance and quality issues identified through profiling.\n",
    "\n",
    "### Strategy Overview\n",
    "\n",
    "| Strategy | Benefit | Best For |\n",
    "|----------|---------|----------|\n",
    "| **Context-Length Tuning** | Balance quality vs. latency/cost | All RAG applications |\n",
    "| **Embedding Dimensionality** | Reduce storage and search time | Large-scale deployments |\n",
    "| **Batching Optimization** | Higher throughput | High-volume workloads |\n",
    "| **Index-Level Tuning** | Faster vector search | Latency-sensitive apps |\n",
    "| **Caching** | 10-100x faster for repeated queries | FAQ-style applications |\n",
    "| **Hybrid Search** | Better precision for technical terms | Code/API documentation |\n",
    "| **Adaptive Retrieval** | Optimized per-query performance | Diverse query types |\n",
    "\n",
    "> **Note:** Context-length tuning and batching optimization were covered in Part 7. This section focuses on additional strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26f6d39-7ebe-4d05-842e-68e52c424f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 1: Embedding Dimensionality Adjustments\n",
    "\n",
    "Embedding dimensionality directly impacts storage, search speed, and semantic accuracy. Higher dimensions capture more nuance but increase computational cost.\n",
    "\n",
    "**Dimensionality Trade-offs:**\n",
    "\n",
    "| Dimension | Storage per Vector | Search Speed | Semantic Quality |\n",
    "|-----------|-------------------|--------------|------------------|\n",
    "| 384 | 1.5 KB | Fastest | Good for simple queries |\n",
    "| 768 | 3 KB | Fast | Balanced performance |\n",
    "| 1024 | 4 KB | Moderate | High semantic nuance |\n",
    "| 1536+ | 6+ KB | Slower | Maximum accuracy |\n",
    "\n",
    "**When to Reduce Dimensionality:**\n",
    "- Large-scale deployments (millions of vectors)\n",
    "- Latency-critical applications\n",
    "- Simple, factual queries\n",
    "\n",
    "**Techniques for Dimensionality Reduction:**\n",
    "1. **Choose a smaller model**: Use `bge-base` (768d) instead of `bge-large` (1024d)\n",
    "2. **PCA/SVD reduction**: Reduce dimensions post-embedding\n",
    "3. **Matryoshka embeddings**: Models trained to work at multiple dimensions\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Demonstrates the impact of dimensionality on search performance\n",
    "2. Shows how to compare different embedding models\n",
    "3. Provides guidance on choosing the right dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4adbb4-2d73-43d2-a206-212ec131928a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 1: EMBEDDING DIMENSIONALITY ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Compare embedding model options available in Databricks\n",
    "embedding_options = [\n",
    "    {\"model\": \"databricks-bge-large-en\", \"dimensions\": 1024, \"max_tokens\": 512},\n",
    "    {\"model\": \"databricks-gte-large-en\", \"dimensions\": 1024, \"max_tokens\": 512},\n",
    "]\n",
    "\n",
    "print(\"\uD83D\uDCD0 Embedding Model Comparison\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<30} {'Dimensions':<12} {'Max Tokens':<12} {'Use Case':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for opt in embedding_options:\n",
    "    use_case = \"High accuracy\" if opt[\"dimensions\"] >= 1024 else \"Balanced\"\n",
    "    print(f\"{opt['model']:<30} {opt['dimensions']:<12} {opt['max_tokens']:<12} {use_case:<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n\uD83D\uDCA1 Current Configuration:\")\n",
    "print(f\"   Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"   Dimensions: {EMBEDDING_DIMENSION}\")\n",
    "print(f\"\\n\uD83D\uDCCA Storage Impact:\")\n",
    "print(f\"   Per vector: {EMBEDDING_DIMENSION * 4 / 1024:.1f} KB (float32)\")\n",
    "print(f\"   For 1M vectors: {EMBEDDING_DIMENSION * 4 * 1_000_000 / (1024**3):.1f} GB\")\n",
    "\n",
    "print(\"\\n\uD83D\uDD27 Optimization Tips:\")\n",
    "print(\"   • For faster search: Consider models with 384-768 dimensions\")\n",
    "print(\"   • For higher accuracy: Use 1024+ dimensions (current setting)\")\n",
    "print(\"   • For cost savings: Smaller dimensions reduce storage and compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de275756-b9d0-4032-ae6d-0c7f96f03c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 2: Index-Level Performance Improvements\n",
    "\n",
    "Vector Search index configuration significantly impacts query latency and throughput. This section covers key tuning parameters.\n",
    "\n",
    "**Index Configuration Options:**\n",
    "\n",
    "| Parameter | Options | Impact |\n",
    "|-----------|---------|--------|\n",
    "| **Pipeline Type** | TRIGGERED vs CONTINUOUS | Sync frequency vs. freshness |\n",
    "| **Sync Compute** | Serverless vs Provisioned | Cost vs. control |\n",
    "| **Index Type** | Delta Sync vs Direct | Automatic sync vs. manual |\n",
    "\n",
    "**Performance Tuning Strategies:**\n",
    "\n",
    "1. **Reduce search scope with filters**: Pre-filter by metadata before vector search\n",
    "2. **Optimize top-k**: Lower k = faster search, but may miss relevant docs\n",
    "3. **Use TRIGGERED sync**: Better control over when index updates occur\n",
    "4. **Monitor index health**: Check sync status and row counts regularly\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Analyzes current index configuration\n",
    "2. Provides recommendations for performance improvement\n",
    "3. Demonstrates metadata filtering for faster searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f2e8cf-038b-4a56-8f57-3ebb1b4423f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 2: INDEX-LEVEL PERFORMANCE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\uD83D\uDD0D Index-Level Performance Analysis\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get current index configuration\n",
    "try:\n",
    "    index_info = index.describe()\n",
    "\n",
    "    print(\"\uD83D\uDCCB Current Index Configuration:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Index Name: {VECTOR_INDEX_PATH}\")\n",
    "    print(f\"   Endpoint: {VECTOR_SEARCH_ENDPOINT_NAME}\")\n",
    "    print(f\"   Status: {index_info.get('status', {}).get('ready', 'Unknown')}\")\n",
    "    print(f\"   Indexed Rows: {index_info.get('status', {}).get('num_rows', 'N/A')}\")\n",
    "\n",
    "    # Analyze configuration\n",
    "    print(\"\\n\uD83D\uDD27 Performance Recommendations:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Check pipeline type\n",
    "    pipeline_type = index_info.get('delta_sync_index_spec', {}).get('pipeline_type', 'UNKNOWN')\n",
    "    print(f\"\\n   Pipeline Type: {pipeline_type}\")\n",
    "    if pipeline_type == \"TRIGGERED\":\n",
    "        print(\"   ✅ Good: TRIGGERED allows controlled sync timing\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Consider TRIGGERED for better control over sync costs\")\n",
    "\n",
    "    # Check embedding dimension\n",
    "    embed_dim = index_info.get('delta_sync_index_spec', {}).get('embedding_dimension', EMBEDDING_DIMENSION)\n",
    "    print(f\"\\n   Embedding Dimension: {embed_dim}\")\n",
    "    if embed_dim > 768:\n",
    "        print(\"   \uD83D\uDCA1 Tip: Consider smaller dimensions for faster search if accuracy permits\")\n",
    "    else:\n",
    "        print(\"   ✅ Good: Balanced dimension for performance\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Could not retrieve index info: {str(e)[:50]}\")\n",
    "\n",
    "# Demonstrate filtered search for performance\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n\uD83D\uDCCA Filtered Search Performance Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_query_embedding = embedding_model.embed_query(\"authentication API\")\n",
    "\n",
    "# Unfiltered search\n",
    "start = time.time()\n",
    "unfiltered_results = index.similarity_search(\n",
    "    query_vector=test_query_embedding,\n",
    "    num_results=5,\n",
    "    columns=[\"chunk_id\", \"title\", \"category\", \"content\"]\n",
    ")\n",
    "unfiltered_time = time.time() - start\n",
    "\n",
    "# Filtered search (by category)\n",
    "start = time.time()\n",
    "filtered_results = index.similarity_search(\n",
    "    query_vector=test_query_embedding,\n",
    "    num_results=5,\n",
    "    columns=[\"chunk_id\", \"title\", \"category\", \"content\"],\n",
    "    filters={\"category\": \"technical\"}\n",
    ")\n",
    "filtered_time = time.time() - start\n",
    "\n",
    "print(f\"\\n   Unfiltered search: {unfiltered_time:.3f}s\")\n",
    "print(f\"   Filtered search:   {filtered_time:.3f}s\")\n",
    "print(f\"   Speedup:           {unfiltered_time/filtered_time:.1f}x\" if filtered_time > 0 else \"   N/A\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Key Insight: Metadata filters reduce search scope and improve latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7f50c9-85df-4026-8cfa-45e94cbd976b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 3: Query Caching\n",
    "\n",
    "Caching stores responses for previously seen queries, providing instant responses for repeated questions.\n",
    "\n",
    "**Cache Design Considerations:**\n",
    "- **Key generation**: Normalize queries (lowercase, strip whitespace)\n",
    "- **Cache size**: Balance memory usage vs. hit rate\n",
    "- **Eviction policy**: LRU (Least Recently Used) is common\n",
    "- **TTL (Time-to-Live)**: Invalidate stale responses\n",
    "\n",
    "**When to Use Caching:**\n",
    "- High query repetition (FAQ-style applications)\n",
    "- Stable underlying data (infrequent updates)\n",
    "- Latency-sensitive applications\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Implements a `CachedRAGPipeline` class with LRU-like caching\n",
    "2. Uses MD5 hashing for cache key generation\n",
    "3. Tracks cache hits/misses for monitoring\n",
    "4. Demonstrates cache effectiveness with repeated queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2979c6e-f234-418b-93f7-22ae3d2013e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 3: QUERY CACHING\n",
    "# ============================================================\n",
    "\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedRAGPipeline:\n",
    "    \"\"\"RAG pipeline with query caching for improved performance.\"\"\"\n",
    "\n",
    "    def __init__(self, rag_chain, cache_size: int = 100):\n",
    "        self.rag_chain = rag_chain\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "\n",
    "    def _get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate a cache key for the query.\"\"\"\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n",
    "\n",
    "    def invoke(self, query: str) -> str:\n",
    "        \"\"\"Invoke the RAG pipeline with caching.\"\"\"\n",
    "        cache_key = self._get_cache_key(query)\n",
    "\n",
    "        if cache_key in self.cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        self.cache_misses += 1\n",
    "        response = self.rag_chain.invoke(query)\n",
    "\n",
    "        # Add to cache (simple LRU-like behavior)\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            # Remove oldest entry\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            del self.cache[oldest_key]\n",
    "\n",
    "        self.cache[cache_key] = response\n",
    "        return response\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        hit_rate = (self.cache_hits / total * 100) if total > 0 else 0\n",
    "        return {\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_misses\": self.cache_misses,\n",
    "            \"hit_rate\": f\"{hit_rate:.1f}%\",\n",
    "            \"cache_size\": len(self.cache)\n",
    "        }\n",
    "\n",
    "# Create cached pipeline\n",
    "cached_pipeline = CachedRAGPipeline(rag_chain, cache_size=50)\n",
    "\n",
    "# Test caching\n",
    "print(\"\uD83D\uDD04 Testing Query Caching\\n\")\n",
    "\n",
    "test_queries_with_repeats = [\n",
    "    \"What is the authentication flow?\",\n",
    "    \"How do I deploy a model?\",\n",
    "    \"What is the authentication flow?\",  # Repeat\n",
    "    \"What are the code review practices?\",\n",
    "    \"How do I deploy a model?\",  # Repeat\n",
    "    \"What is the authentication flow?\",  # Repeat\n",
    "]\n",
    "\n",
    "for query in test_queries_with_repeats:\n",
    "    start = time.time()\n",
    "    response = cached_pipeline.invoke(query)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   Query: {query[:40]}... | Time: {elapsed:.3f}s\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Cache Statistics: {cached_pipeline.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff5eee2-3463-449e-9323-584ec9e26b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 4: Hybrid Search with Keyword Boosting\n",
    "\n",
    "Semantic search excels at understanding meaning, but can miss exact keyword matches. Hybrid search combines both approaches.\n",
    "\n",
    "**Why Hybrid Search?**\n",
    "- **Semantic search** understands \"authentication\" ≈ \"login\" ≈ \"sign in\"\n",
    "- **Keyword search** ensures exact matches for technical terms like \"CrashLoopBackOff\"\n",
    "- **Combined** provides the best of both worlds\n",
    "\n",
    "**Weighting Strategy:**\n",
    "- `semantic_weight=0.7` - Prioritize semantic understanding\n",
    "- `keyword_weight=0.3` - Boost exact keyword matches\n",
    "- Adjust based on your content type (more technical = higher keyword weight)\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Retrieves more results than needed (top_k * 2) for re-ranking\n",
    "2. Extracts keywords from the query (removing stop words)\n",
    "3. Calculates keyword match score for each result\n",
    "4. Combines semantic and keyword scores with configurable weights\n",
    "5. Re-ranks and returns the top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5db060c-442b-4bf8-b5a8-bd161297befe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 4: HYBRID SEARCH SIMULATION\n",
    "# ============================================================\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    semantic_weight: float = 0.7,\n",
    "    keyword_weight: float = 0.3,\n",
    "    top_k: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simulate hybrid search by combining semantic and keyword scores.\n",
    "\n",
    "    In production, you would use Databricks Vector Search's built-in\n",
    "    hybrid search capabilities or combine with a keyword index.\n",
    "    \"\"\"\n",
    "    # Get semantic search results\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    semantic_results = index.similarity_search(\n",
    "        query_vector=query_embedding,\n",
    "        num_results=top_k * 2,  # Get more results for re-ranking\n",
    "        columns=[\"chunk_id\", \"doc_id\", \"title\", \"category\", \"content\"]\n",
    "    )\n",
    "\n",
    "    # Extract keywords from query (simple approach)\n",
    "    query_keywords = set(query.lower().split())\n",
    "    stop_words = {\"what\", \"how\", \"is\", \"the\", \"a\", \"an\", \"to\", \"do\", \"i\", \"for\", \"in\", \"of\"}\n",
    "    query_keywords = query_keywords - stop_words\n",
    "\n",
    "    # Score and re-rank results\n",
    "    scored_results = []\n",
    "    for result in semantic_results.get(\"result\", {}).get(\"data_array\", []):\n",
    "        content = result[4].lower()  # content column\n",
    "\n",
    "        # Calculate keyword score\n",
    "        keyword_matches = sum(1 for kw in query_keywords if kw in content)\n",
    "        keyword_score = keyword_matches / len(query_keywords) if query_keywords else 0\n",
    "\n",
    "        # Combine scores\n",
    "        semantic_score = result[-1]  # similarity score\n",
    "        combined_score = (semantic_weight * semantic_score) + (keyword_weight * keyword_score)\n",
    "\n",
    "        scored_results.append({\n",
    "            \"chunk_id\": result[0],\n",
    "            \"title\": result[2],\n",
    "            \"category\": result[3],\n",
    "            \"content\": result[4],\n",
    "            \"semantic_score\": semantic_score,\n",
    "            \"keyword_score\": keyword_score,\n",
    "            \"combined_score\": combined_score\n",
    "        })\n",
    "\n",
    "    # Sort by combined score and return top-k\n",
    "    scored_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    return scored_results[:top_k]\n",
    "\n",
    "# Test hybrid search\n",
    "print(\"\uD83D\uDD0D Hybrid Search Results\\n\")\n",
    "test_query = \"kubectl pod CrashLoopBackOff troubleshooting\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "hybrid_results = hybrid_search(test_query)\n",
    "\n",
    "for i, result in enumerate(hybrid_results):\n",
    "    print(f\"{i+1}. {result['title']}\")\n",
    "    print(f\"   Semantic: {result['semantic_score']:.4f} | \"\n",
    "          f\"Keyword: {result['keyword_score']:.4f} | \"\n",
    "          f\"Combined: {result['combined_score']:.4f}\")\n",
    "    print(f\"   Preview: {result['content'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c987f528-db04-49af-83c3-2350e92e85d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 5: Adaptive Retrieval\n",
    "\n",
    "Different queries benefit from different retrieval strategies. Adaptive retrieval analyzes the query and adjusts parameters accordingly.\n",
    "\n",
    "**Query Type Detection:**\n",
    "\n",
    "| Query Type | Indicators | Optimal Strategy |\n",
    "|------------|------------|------------------|\n",
    "| **Specific** | \"error\", \"API\", \"endpoint\" | Low top-k (3), high precision |\n",
    "| **Broad** | \"overview\", \"explain\", \"what is\" | High top-k (8), more context |\n",
    "| **Troubleshooting** | \"fix\", \"issue\", \"problem\" | Medium top-k (5), category filter |\n",
    "\n",
    "**Adaptive Parameters:**\n",
    "- `top_k` - Number of results to retrieve\n",
    "- `max_context_chars` - Context length limit\n",
    "- `category_filter` - Restrict to relevant document types\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Analyzes the query to detect its type (specific, broad, troubleshooting)\n",
    "2. Selects optimal retrieval parameters based on query type\n",
    "3. Creates a customized RAG chain for the query\n",
    "4. Demonstrates how different queries get different treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb103da-87be-43c7-9cff-3b3d02a32ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 5: ADAPTIVE RETRIEVAL\n",
    "# ============================================================\n",
    "\n",
    "def analyze_query(query: str) -> Dict:\n",
    "    \"\"\"Analyze query to determine optimal retrieval strategy.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Detect query type\n",
    "    is_specific = any(word in query_lower for word in [\"error\", \"code\", \"api\", \"endpoint\", \"command\"])\n",
    "    is_broad = any(word in query_lower for word in [\"overview\", \"explain\", \"describe\", \"what is\"])\n",
    "    is_troubleshooting = any(word in query_lower for word in [\"troubleshoot\", \"fix\", \"error\", \"issue\", \"problem\"])\n",
    "\n",
    "    # Determine parameters\n",
    "    if is_troubleshooting:\n",
    "        return {\n",
    "            \"type\": \"troubleshooting\",\n",
    "            \"top_k\": 7,\n",
    "            \"max_context\": 5000,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    elif is_specific:\n",
    "        return {\n",
    "            \"type\": \"specific\",\n",
    "            \"top_k\": 3,\n",
    "            \"max_context\": 2000,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    elif is_broad:\n",
    "        return {\n",
    "            \"type\": \"broad\",\n",
    "            \"top_k\": 5,\n",
    "            \"max_context\": 4000,\n",
    "            \"temperature\": 0.2\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"type\": \"default\",\n",
    "            \"top_k\": 5,\n",
    "            \"max_context\": 3000,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "\n",
    "def adaptive_rag(query: str) -> str:\n",
    "    \"\"\"Execute RAG with adaptive parameters based on query analysis.\"\"\"\n",
    "    params = analyze_query(query)\n",
    "\n",
    "    print(f\"   Query type: {params['type']}\")\n",
    "    print(f\"   Parameters: top_k={params['top_k']}, max_context={params['max_context']}\")\n",
    "\n",
    "    # Create chain with adaptive parameters\n",
    "    adaptive_chain = create_tuned_rag_chain(\n",
    "        top_k=params[\"top_k\"],\n",
    "        max_context_chars=params[\"max_context\"]\n",
    "    )\n",
    "\n",
    "    return adaptive_chain.invoke(query)\n",
    "\n",
    "# Test adaptive retrieval\n",
    "print(\"\uD83C\uDFAF Adaptive Retrieval Examples\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "adaptive_queries = [\n",
    "    \"What is the data pipeline architecture?\",  # Broad\n",
    "    \"How do I fix ImagePullBackOff error?\",  # Troubleshooting\n",
    "    \"What is the API endpoint for user login?\",  # Specific\n",
    "]\n",
    "\n",
    "for query in adaptive_queries:\n",
    "    print(f\"\\n❓ Query: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = adaptive_rag(query)\n",
    "    print(f\"\\n\uD83D\uDCAC Response: {response[:200]}...\")\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5774e8ed-922f-47ba-850a-399a9697bcdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Strategy 6: Performance Monitoring Dashboard\n",
    "\n",
    "Production RAG systems require monitoring to ensure reliability and performance. This strategy implements a simple monitoring dashboard.\n",
    "\n",
    "**Key Metrics to Monitor:**\n",
    "\n",
    "| Metric | Purpose | Alert Threshold |\n",
    "|--------|---------|-----------------|\n",
    "| **Success Rate** | System reliability | < 99% |\n",
    "| **P50 Latency** | Typical user experience | > 3s |\n",
    "| **P95 Latency** | Worst-case experience | > 10s |\n",
    "| **Error Count** | System health | > 0 |\n",
    "| **Requests/min** | Load tracking | Varies |\n",
    "\n",
    "**Production Monitoring Considerations:**\n",
    "- Integrate with Databricks Lakehouse Monitoring\n",
    "- Set up alerts for SLA violations\n",
    "- Track token usage for cost monitoring\n",
    "- Log queries for debugging and improvement\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Implements a `RAGMonitor` class to track requests\n",
    "2. Logs latency, success/failure, and errors\n",
    "3. Calculates percentile latencies (P50, P95, P99)\n",
    "4. Generates a dashboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec26860-d150-4bb0-946a-b0d4c3a1d73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 6: PERFORMANCE MONITORING\n",
    "# ============================================================\n",
    "\n",
    "class RAGMonitor:\n",
    "    \"\"\"Simple monitoring for RAG system performance.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.requests = []\n",
    "        self.errors = []\n",
    "\n",
    "    def log_request(self, query: str, latency: float, success: bool, error: str = None):\n",
    "        \"\"\"Log a request for monitoring.\"\"\"\n",
    "        self.requests.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"query\": query,\n",
    "            \"latency\": latency,\n",
    "            \"success\": success,\n",
    "            \"error\": error\n",
    "        })\n",
    "        if not success:\n",
    "            self.errors.append({\"timestamp\": datetime.now(), \"error\": error})\n",
    "\n",
    "    def get_dashboard(self) -> Dict:\n",
    "        \"\"\"Generate monitoring dashboard metrics.\"\"\"\n",
    "        if not self.requests:\n",
    "            return {\"status\": \"No data\"}\n",
    "\n",
    "        df = pd.DataFrame(self.requests)\n",
    "\n",
    "        return {\n",
    "            \"total_requests\": len(self.requests),\n",
    "            \"success_rate\": f\"{(df['success'].sum() / len(df) * 100):.1f}%\",\n",
    "            \"avg_latency\": f\"{df['latency'].mean():.2f}s\",\n",
    "            \"p50_latency\": f\"{df['latency'].median():.2f}s\",\n",
    "            \"p95_latency\": f\"{df['latency'].quantile(0.95):.2f}s\",\n",
    "            \"p99_latency\": f\"{df['latency'].quantile(0.99):.2f}s\",\n",
    "            \"error_count\": len(self.errors),\n",
    "            \"requests_per_minute\": len(df) / max(1, (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 60)\n",
    "        }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = RAGMonitor()\n",
    "\n",
    "# Simulate some requests\n",
    "print(\"\uD83D\uDCCA Simulating RAG Requests for Monitoring\\n\")\n",
    "\n",
    "simulation_queries = [\n",
    "    \"What is the authentication API?\",\n",
    "    \"How do I deploy models?\",\n",
    "    \"What are the data governance policies?\",\n",
    "    \"Explain the medallion architecture\",\n",
    "    \"What was the payment outage impact?\"\n",
    "]\n",
    "\n",
    "for query in simulation_queries:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = rag_chain.invoke(query)\n",
    "        latency = time.time() - start\n",
    "        monitor.log_request(query, latency, success=True)\n",
    "        print(f\"   ✓ {query[:40]}... ({latency:.2f}s)\")\n",
    "    except Exception as e:\n",
    "        monitor.log_request(query, 0, success=False, error=str(e))\n",
    "        print(f\"   ✗ {query[:40]}... (Error: {str(e)[:30]})\")\n",
    "\n",
    "# Display dashboard\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83D\uDCC8 Performance Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "dashboard = monitor.get_dashboard()\n",
    "for metric, value in dashboard.items():\n",
    "    print(f\"   {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95eb893e-f457-4c87-8697-8e6724e18670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 10: Cleanup (Optional)\n",
    "\n",
    "This section provides cleanup utilities to remove all resources created during the lab.\n",
    "\n",
    "### Resources Created in This Lab\n",
    "\n",
    "| Resource Type | Name | Purpose |\n",
    "|---------------|------|---------|\n",
    "| **Vector Search Index** | `{catalog}.{schema}.chapter9_vector_index` | Stores embeddings for similarity search |\n",
    "| **Delta Table (Chunks)** | `{catalog}.{schema}.chapter9_chunks` | Chunked documents with embeddings |\n",
    "| **Delta Table (Source)** | `{catalog}.{schema}.chapter9_documents` | Original source documents |\n",
    "| **Schema** | `{catalog}.{schema}` | Container for all lab objects |\n",
    "\n",
    "### When to Clean Up\n",
    "\n",
    "- **Keep resources** if you plan to continue experimenting\n",
    "- **Clean up** if you're done with the lab and want to free resources\n",
    "- **Note**: The Vector Search endpoint is shared and should NOT be deleted if other indices use it\n",
    "\n",
    "**⚠️ Warning:** Cleanup is irreversible. All data will be permanently deleted.\n",
    "\n",
    "**What This Cell Does:**\n",
    "1. Deletes the Vector Search index (not the endpoint)\n",
    "2. Drops the chunks Delta table\n",
    "3. Drops the source documents Delta table\n",
    "4. Optionally drops the schema if empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c621a62e-8ebc-4ea8-be6c-15c29da82c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLEANUP RESOURCES (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "def cleanup_resources(confirm: bool = False):\n",
    "    \"\"\"Clean up all resources created during the lab.\"\"\"\n",
    "\n",
    "    if not confirm:\n",
    "        print(\"⚠️  Cleanup not confirmed. Set confirm=True to proceed.\")\n",
    "        return\n",
    "\n",
    "    print(\"\uD83E\uDDF9 Cleaning up resources...\\n\")\n",
    "\n",
    "    # Delete Vector Search index\n",
    "    try:\n",
    "        vector_search_client.delete_index(\n",
    "            endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "            index_name=VECTOR_INDEX_PATH\n",
    "        )\n",
    "        print(f\"   ✓ Deleted index: {VECTOR_INDEX_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Could not delete index: {str(e)[:50]}\")\n",
    "\n",
    "    # Delete tables\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {CHUNKS_TABLE_PATH}\")\n",
    "        print(f\"   ✓ Deleted table: {CHUNKS_TABLE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Could not delete table: {str(e)[:50]}\")\n",
    "\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {SOURCE_TABLE_PATH}\")\n",
    "        print(f\"   ✓ Deleted table: {SOURCE_TABLE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Could not delete table: {str(e)[:50]}\")\n",
    "\n",
    "    # Optionally delete schema\n",
    "    try:\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "        print(f\"   ✓ Deleted schema: {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Could not delete schema: {str(e)[:50]}\")\n",
    "\n",
    "    print(\"\\n✅ Cleanup complete!\")\n",
    "\n",
    "# Uncomment the line below to run cleanup\n",
    "# cleanup_resources(confirm=True)\n",
    "print(\"ℹ️  To clean up resources, uncomment and run: cleanup_resources(confirm=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2f3030-ed38-4044-b6f2-97f353b97049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Lab Summary and Key Takeaways\n",
    "\n",
    "Congratulations! \uD83C\uDF89 You have successfully completed this hands-on lab on building a scalable Vector Search and Retrieval System.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "1. **Created an Embedding Pipeline**: Chunked documents and generated embeddings using a Databricks-hosted embedding model\n",
    "2. **Built a Vector Search Index**: Set up a Delta Sync index and populated it with documents\n",
    "3. **Configured a Retriever**: Built a LangChain retriever to power semantic search\n",
    "4. **Served LLM and Embedding Models**: Integrated Mosaic AI Model Serving for generation\n",
    "5. **Implemented Batching and Context Tuning**: Optimized throughput with parallel processing and context length adjustments\n",
    "6. **Profiled RAG Performance**: Identified bottlenecks using component-level profiling\n",
    "7. **Applied Chapter 9 Optimization Strategies**: Including context-length tuning, embedding dimensionality adjustments, batching optimization, and index-level performance improvements\n",
    "\n",
    "### Key Optimization Strategies from Chapter 9\n",
    "\n",
    "| Strategy | What You Learned | Implementation |\n",
    "|----------|------------------|----------------|\n",
    "| **Context-Length Tuning** | Balance quality vs. latency/cost | Adjustable top-k and max context chars |\n",
    "| **Embedding Dimensionality** | Trade-off between accuracy and speed | Model selection and dimension analysis |\n",
    "| **Batching Optimization** | Higher throughput for volume workloads | ThreadPoolExecutor with configurable workers |\n",
    "| **Index-Level Improvements** | Faster vector search with filters | Metadata filtering and index configuration |\n",
    "| **Query Caching** | Instant responses for repeated queries | LRU cache with hit rate tracking |\n",
    "| **Hybrid Search** | Better precision for technical terms | Combined semantic + keyword scoring |\n",
    "\n",
    "### Bottleneck Diagnosis Patterns\n",
    "\n",
    "| Bottleneck | Symptoms | Root-Cause Adjustments |\n",
    "|------------|----------|------------------------|\n",
    "| Slow Vector Search | High search latency, timeouts | Reduce top-k, use metadata filters, optimize index |\n",
    "| Model Execution Delays | High generation latency | Use smaller models, reduce context, enable streaming |\n",
    "| Inefficient Batching | Low throughput, high costs | Increase batch size, use async processing |\n",
    "| Data Retrieval Slowdowns | Slow embedding generation | Cache embeddings, use smaller embedding models |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale Testing**: Test with larger document collections (10K+ documents)\n",
    "2. **A/B Testing**: Compare different embedding models and chunk sizes\n",
    "3. **Production Deployment**: Set up monitoring, alerting, and auto-scaling\n",
    "4. **Fine-tuning**: Consider fine-tuning embedding models for domain-specific content\n",
    "5. **Evaluation**: Implement systematic evaluation using relevance metrics\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Databricks Vector Search Documentation](https://docs.databricks.com/en/generative-ai/vector-search.html)\n",
    "- [Mosaic AI Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html)\n",
    "- [LangChain Databricks Integration](https://python.langchain.com/docs/integrations/providers/databricks)\n",
    "- [MLflow Model Registry](https://docs.databricks.com/en/mlflow/model-registry.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this lab!**\n",
    "\n",
    "If you have questions or feedback, please reach out to your instructor or refer to the Chapter 9 materials for additional context."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Chapter9_HandsOn_Lab_Vector_Search_RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}