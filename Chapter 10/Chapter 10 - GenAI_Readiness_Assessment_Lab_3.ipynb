{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5fd034-475f-4935-a6f7-4aef61bce1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDE80 Hands-On Lab: End-to-End Generative AI Readiness Assessment\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCCB Lab Scenario\n",
    "\n",
    "You are a **Generative AI engineer** preparing to lead a **production readiness review** for an enterprise LLM assistant used by:\n",
    "- **Operations analysts** - for retrieving internal policies\n",
    "- **Customer-support teams** - for summarizing operational reports\n",
    "- **Internal executives** - for guided responses to procedural questions\n",
    "\n",
    "### Business Context\n",
    "Leadership has mandated that all AI systems must:\n",
    "1. ✅ Adhere to **traceability requirements**\n",
    "2. ✅ Maintain **consistent response quality** under variable load\n",
    "3. ✅ Demonstrate **measurable safeguards** against hallucinations and sensitive-data exposure\n",
    "\n",
    "Your organization wants to determine whether your end-to-end generative AI workflow is **production-ready**.\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83C\uDFAF Lab Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "| # | Objective | Skills Validated |\n",
    "|---|-----------|------------------|\n",
    "| 1 | Apply **blueprint-driven reasoning** to evaluate the completeness of a generative AI workflow | Architecture Review |\n",
    "| 2 | Design and execute **structured simulation tests** to measure model performance and identify gaps | Performance Testing |\n",
    "| 3 | Build and validate a **small RAG workflow** using embeddings, vector retrieval, and prompt construction | RAG Implementation |\n",
    "| 4 | Deploy and configure an **LLM endpoint** in Databricks Model Serving with operational parameters | Model Serving |\n",
    "| 5 | Enable **governance features** such as inference tables, PII redaction, and Unity Catalog traceability | Governance & Compliance |\n",
    "| 6 | **Diagnose and resolve** common issues in real-world GenAI workflows | Troubleshooting |\n",
    "| 7 | Perform a **final readiness assessment** combining technical validation and operational testing | Production Readiness |\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Estimated Time: 90-120 minutes\n",
    "\n",
    "## \uD83D\uDCDA Prerequisites\n",
    "- Databricks workspace with Unity Catalog enabled\n",
    "- Access to a cluster with ML Runtime 14.0+\n",
    "- Foundation Model APIs access (or external LLM API key)\n",
    "- Basic understanding of LLMs, embeddings, and vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f70cd1-4dde-471e-97e8-d42823338238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDCE6 Part 1: Environment Setup and Configuration\n",
    "\n",
    "## 1.1 Install Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries for our GenAI readiness assessment. This includes:\n",
    "- **langchain** - For building LLM applications and RAG pipelines\n",
    "- **chromadb** - Lightweight vector database for embeddings storage\n",
    "- **tiktoken** - Token counting for context window management\n",
    "- **presidio-analyzer** - For PII detection and redaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc905d2-d36c-4e3d-b9b0-0035ec95a64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: We use Databricks native Vector Search - no external vector DB needed\n",
    "# All other packages are minimal to maintain compatibility with Databricks runtime\n",
    "%pip install databricks-vectorsearch tiktoken faker --quiet\n",
    "\n",
    "# Restart Python to pick up new packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b832cb3-b4bb-4eb2-a13f-26851408da52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Import Libraries and Configure Environment\n",
    "\n",
    "Now we import all necessary libraries and set up our environment variables. We'll configure:\n",
    "- Databricks workspace connection\n",
    "- Catalog and schema for Unity Catalog\n",
    "- Logging configuration for traceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "454e2cab-9e92-4142-b9d3-cc54fd1e8ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Databricks imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lit, current_timestamp, explode\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType, ArrayType, IntegerType\n",
    "\n",
    "# ML and AI imports\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# Math for vector operations\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Databricks Vector Search\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# PII Detection - Using regex-based detection (Presidio has numpy compatibility issues)\n",
    "import re\n",
    "\n",
    "# Data generation\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"\uD83D\uDCC5 Lab started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f972b748-97f2-4af0-9e89-e855c9f6a34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.3 Configure Unity Catalog Settings\n",
    "\n",
    "We'll set up our Unity Catalog configuration for:\n",
    "- **Catalog**: The top-level container for our data assets\n",
    "- **Schema**: The database where we'll store our tables\n",
    "- **Volume**: For storing sample documents\n",
    "\n",
    "⚠️ **Important**: Update these values to match your Databricks environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650a2a0d-72c9-4a05-8a78-0662f15de958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "# Unity Catalog settings\n",
    "CATALOG_NAME = \"genai_lab\"  # Change to your catalog\n",
    "SCHEMA_NAME = \"readiness_assessment\"  # Change to your schema\n",
    "\n",
    "# Model serving settings\n",
    "MODEL_ENDPOINT_NAME = \"genai-assistant-endpoint\"\n",
    "\n",
    "# Create catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "print(f\"✅ Using Catalog: {CATALOG_NAME}\")\n",
    "print(f\"✅ Using Schema: {SCHEMA_NAME}\")\n",
    "print(f\"✅ Full path: {CATALOG_NAME}.{SCHEMA_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0d8581-7157-4be6-bdda-b59ee16dbe51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDCCA Part 2: Sample Data Generation (Prerequisites)\n",
    "\n",
    "## 2.1 Generate Internal Policy Documents\n",
    "\n",
    "We'll create realistic sample data that simulates an enterprise environment. This includes:\n",
    "- **Internal Policies**: HR policies, IT security guidelines, compliance procedures\n",
    "- **Operational Reports**: Daily/weekly operational summaries\n",
    "- **Procedural Documents**: Step-by-step guides for common tasks\n",
    "\n",
    "This data will be used throughout the lab to test our RAG pipeline and LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22660f5d-88e7-47f0-8dcf-aa71403761c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Faker for realistic data generation\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # For reproducibility\n",
    "\n",
    "# ============================================\n",
    "# INTERNAL POLICY DOCUMENTS\n",
    "# ============================================\n",
    "\n",
    "policy_documents = [\n",
    "    {\n",
    "        \"doc_id\": \"POL-001\",\n",
    "        \"title\": \"Data Classification and Handling Policy\",\n",
    "        \"category\": \"Information Security\",\n",
    "        \"content\": \"\"\"Data Classification and Handling Policy\n",
    "\n",
    "1. PURPOSE\n",
    "This policy establishes guidelines for classifying and handling organizational data to ensure appropriate protection levels.\n",
    "\n",
    "2. CLASSIFICATION LEVELS\n",
    "- PUBLIC: Information that can be freely shared externally\n",
    "- INTERNAL: Information for internal use only, not for external distribution\n",
    "- CONFIDENTIAL: Sensitive business information requiring restricted access\n",
    "- RESTRICTED: Highly sensitive data including PII, financial records, and trade secrets\n",
    "\n",
    "3. HANDLING REQUIREMENTS\n",
    "- RESTRICTED data must be encrypted at rest and in transit\n",
    "- Access to CONFIDENTIAL data requires manager approval\n",
    "- All data transfers must be logged in the audit system\n",
    "- PII data must be anonymized before use in analytics or AI systems\n",
    "\n",
    "4. AI SYSTEM REQUIREMENTS\n",
    "- AI models must not be trained on RESTRICTED data without explicit approval\n",
    "- All AI inference requests must be logged for traceability\n",
    "- PII must be redacted from AI system inputs and outputs\n",
    "- Model outputs must include confidence scores when available\n",
    "\n",
    "5. COMPLIANCE\n",
    "Violations of this policy may result in disciplinary action up to and including termination.\n",
    "\"\"\",\n",
    "        \"effective_date\": \"2024-01-01\",\n",
    "        \"last_updated\": \"2024-06-15\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"POL-002\",\n",
    "        \"title\": \"AI System Governance Policy\",\n",
    "        \"category\": \"AI Governance\",\n",
    "        \"content\": \"\"\"AI System Governance Policy\n",
    "\n",
    "1. PURPOSE\n",
    "This policy establishes governance requirements for all AI systems deployed within the organization.\n",
    "\n",
    "2. SCOPE\n",
    "Applies to all machine learning models, large language models, and automated decision systems.\n",
    "\n",
    "3. REQUIREMENTS\n",
    "\n",
    "3.1 Traceability\n",
    "- All AI systems must maintain complete audit trails\n",
    "- Inference requests and responses must be logged with timestamps\n",
    "- Model versions must be tracked in a central registry\n",
    "\n",
    "3.2 Performance Monitoring\n",
    "- Response latency must be monitored continuously\n",
    "- Accuracy metrics must be tracked and reported weekly\n",
    "- Drift detection must be implemented for all production models\n",
    "\n",
    "3.3 Safety Controls\n",
    "- Hallucination detection mechanisms must be in place\n",
    "- Content filtering must prevent harmful outputs\n",
    "- Rate limiting must protect against abuse\n",
    "\n",
    "3.4 Human Oversight\n",
    "- High-stakes decisions require human review\n",
    "- Escalation procedures must be documented\n",
    "- Regular model audits must be conducted quarterly\n",
    "\n",
    "4. PRODUCTION READINESS CRITERIA\n",
    "Before deployment, AI systems must demonstrate:\n",
    "- 99.5% uptime capability\n",
    "- P95 latency under 2 seconds\n",
    "- Hallucination rate below 5%\n",
    "- Complete PII redaction coverage\n",
    "\"\"\",\n",
    "        \"effective_date\": \"2024-03-01\",\n",
    "        \"last_updated\": \"2024-09-01\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"POL-003\",\n",
    "        \"title\": \"Customer Support Response Guidelines\",\n",
    "        \"category\": \"Operations\",\n",
    "        \"content\": \"\"\"Customer Support Response Guidelines\n",
    "\n",
    "1. RESPONSE TIME STANDARDS\n",
    "- Priority 1 (Critical): Response within 15 minutes\n",
    "- Priority 2 (High): Response within 1 hour\n",
    "- Priority 3 (Medium): Response within 4 hours\n",
    "- Priority 4 (Low): Response within 24 hours\n",
    "\n",
    "2. AI-ASSISTED RESPONSES\n",
    "When using AI assistants for customer support:\n",
    "- Always verify AI-generated responses before sending\n",
    "- Do not share customer PII with AI systems\n",
    "- Flag uncertain responses for human review\n",
    "- Document AI usage in ticket notes\n",
    "\n",
    "3. ESCALATION PROCEDURES\n",
    "- Escalate to Tier 2 if unresolved after 2 interactions\n",
    "- Escalate to management for complaints about AI responses\n",
    "- Document all escalations in the tracking system\n",
    "\n",
    "4. QUALITY STANDARDS\n",
    "- Responses must be professional and empathetic\n",
    "- Technical accuracy is mandatory\n",
    "- Follow-up within 24 hours for complex issues\n",
    "\"\"\",\n",
    "        \"effective_date\": \"2024-02-01\",\n",
    "        \"last_updated\": \"2024-08-15\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(policy_documents)} policy documents\")\n",
    "for doc in policy_documents:\n",
    "    print(f\"   \uD83D\uDCC4 {doc['doc_id']}: {doc['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03af738c-4585-4014-9cee-57e713f6b103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Generate Operational Reports\n",
    "\n",
    "Next, we create sample operational reports that simulate daily and weekly summaries. These reports contain:\n",
    "- System performance metrics\n",
    "- Incident summaries\n",
    "- Key performance indicators (KPIs)\n",
    "\n",
    "These will be used to test the LLM's ability to summarize and extract insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39931cad-5f53-40bd-a7af-c21cf1ba26bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPERATIONAL REPORTS\n",
    "# ============================================\n",
    "\n",
    "operational_reports = [\n",
    "    {\n",
    "        \"report_id\": \"OPS-2024-W45\",\n",
    "        \"title\": \"Weekly Operations Summary - Week 45\",\n",
    "        \"report_type\": \"Weekly Summary\",\n",
    "        \"content\": \"\"\"Weekly Operations Summary - Week 45 (November 4-10, 2024)\n",
    "\n",
    "EXECUTIVE SUMMARY\n",
    "Overall system availability: 99.7%\n",
    "Total customer tickets: 1,247\n",
    "AI-assisted resolutions: 68%\n",
    "Average response time: 2.3 hours\n",
    "\n",
    "KEY METRICS\n",
    "- API Gateway uptime: 99.9%\n",
    "- Database response time (P95): 45ms\n",
    "- LLM endpoint latency (P95): 1.8 seconds\n",
    "- Cache hit rate: 78%\n",
    "\n",
    "INCIDENTS\n",
    "1. INC-4521: Brief API latency spike on Tuesday (15 min duration)\n",
    "   Root cause: Increased batch processing load\n",
    "   Resolution: Auto-scaling triggered successfully\n",
    "\n",
    "2. INC-4523: LLM response quality degradation detected\n",
    "   Root cause: Context window overflow on long documents\n",
    "   Resolution: Implemented chunking strategy\n",
    "\n",
    "AI SYSTEM PERFORMANCE\n",
    "- Total inference requests: 45,230\n",
    "- Average tokens per request: 1,250\n",
    "- Hallucination flags: 127 (0.28%)\n",
    "- PII detection triggers: 89\n",
    "\n",
    "RECOMMENDATIONS\n",
    "1. Increase LLM endpoint concurrency from 4 to 8\n",
    "2. Implement request batching for efficiency\n",
    "3. Review hallucination patterns for model fine-tuning\n",
    "\"\"\",\n",
    "        \"report_date\": \"2024-11-10\",\n",
    "        \"author\": \"Operations Team\"\n",
    "    },\n",
    "    {\n",
    "        \"report_id\": \"OPS-2024-W46\",\n",
    "        \"title\": \"Weekly Operations Summary - Week 46\",\n",
    "        \"report_type\": \"Weekly Summary\",\n",
    "        \"content\": \"\"\"Weekly Operations Summary - Week 46 (November 11-17, 2024)\n",
    "\n",
    "EXECUTIVE SUMMARY\n",
    "Overall system availability: 99.8%\n",
    "Total customer tickets: 1,189\n",
    "AI-assisted resolutions: 72%\n",
    "Average response time: 1.9 hours\n",
    "\n",
    "KEY METRICS\n",
    "- API Gateway uptime: 99.95%\n",
    "- Database response time (P95): 42ms\n",
    "- LLM endpoint latency (P95): 1.5 seconds\n",
    "- Cache hit rate: 82%\n",
    "\n",
    "IMPROVEMENTS FROM LAST WEEK\n",
    "- Response time improved by 17%\n",
    "- AI resolution rate increased by 4%\n",
    "- LLM latency reduced by 16%\n",
    "\n",
    "INCIDENTS\n",
    "1. INC-4530: Scheduled maintenance window (planned)\n",
    "   Duration: 30 minutes\n",
    "   Impact: Minimal, off-peak hours\n",
    "\n",
    "AI SYSTEM PERFORMANCE\n",
    "- Total inference requests: 48,750\n",
    "- Average tokens per request: 1,180\n",
    "- Hallucination flags: 98 (0.20%)\n",
    "- PII detection triggers: 76\n",
    "\n",
    "NOTES\n",
    "Concurrency increase implemented successfully.\n",
    "Batching optimization showing positive results.\n",
    "\"\"\",\n",
    "        \"report_date\": \"2024-11-17\",\n",
    "        \"author\": \"Operations Team\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(operational_reports)} operational reports\")\n",
    "for report in operational_reports:\n",
    "    print(f\"   \uD83D\uDCCA {report['report_id']}: {report['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f69930e-ef21-4685-a4a1-6e0c60df5c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3 Generate Procedural Documents\n",
    "\n",
    "We create step-by-step procedural guides that the LLM assistant will use to answer procedural questions. These documents test:\n",
    "- The RAG system's ability to retrieve relevant procedures\n",
    "- The LLM's ability to provide accurate, step-by-step guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7c8c58-920f-435c-8563-0b5866535426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROCEDURAL DOCUMENTS\n",
    "# ============================================\n",
    "\n",
    "procedural_documents = [\n",
    "    {\n",
    "        \"proc_id\": \"PROC-001\",\n",
    "        \"title\": \"Incident Response Procedure\",\n",
    "        \"category\": \"IT Operations\",\n",
    "        \"content\": \"\"\"Incident Response Procedure\n",
    "\n",
    "STEP 1: INCIDENT DETECTION\n",
    "- Monitor alerting systems for anomalies\n",
    "- Review automated incident tickets\n",
    "- Check customer-reported issues\n",
    "\n",
    "STEP 2: INITIAL ASSESSMENT\n",
    "- Determine incident severity (P1-P4)\n",
    "- Identify affected systems and users\n",
    "- Document initial findings in ticket\n",
    "\n",
    "STEP 3: ESCALATION (if needed)\n",
    "- P1/P2: Immediately notify on-call engineer\n",
    "- P1: Activate incident bridge within 15 minutes\n",
    "- Notify stakeholders per communication matrix\n",
    "\n",
    "STEP 4: INVESTIGATION\n",
    "- Gather logs from affected systems\n",
    "- Review recent changes and deployments\n",
    "- Identify root cause or contributing factors\n",
    "\n",
    "STEP 5: RESOLUTION\n",
    "- Implement fix or workaround\n",
    "- Verify resolution with affected users\n",
    "- Document resolution steps\n",
    "\n",
    "STEP 6: POST-INCIDENT\n",
    "- Complete incident report within 48 hours\n",
    "- Schedule post-mortem for P1/P2 incidents\n",
    "- Update runbooks if needed\n",
    "\"\"\",\n",
    "        \"version\": \"2.1\",\n",
    "        \"last_updated\": \"2024-07-01\"\n",
    "    },\n",
    "    {\n",
    "        \"proc_id\": \"PROC-002\",\n",
    "        \"title\": \"AI Model Deployment Procedure\",\n",
    "        \"category\": \"MLOps\",\n",
    "        \"content\": \"\"\"AI Model Deployment Procedure\n",
    "\n",
    "STEP 1: PRE-DEPLOYMENT CHECKLIST\n",
    "- Verify model is registered in MLflow\n",
    "- Confirm model passed all validation tests\n",
    "- Review model card and documentation\n",
    "- Obtain deployment approval from ML Lead\n",
    "\n",
    "STEP 2: STAGING DEPLOYMENT\n",
    "- Deploy model to staging endpoint\n",
    "- Configure serving parameters:\n",
    "  * Temperature: 0.7 (default)\n",
    "  * Max tokens: 2048\n",
    "  * Concurrency: 4\n",
    "- Run integration tests\n",
    "\n",
    "STEP 3: VALIDATION\n",
    "- Execute test suite against staging\n",
    "- Verify latency meets SLA (P95 < 2s)\n",
    "- Check for hallucination patterns\n",
    "- Validate PII redaction is working\n",
    "\n",
    "STEP 4: PRODUCTION DEPLOYMENT\n",
    "- Schedule deployment window\n",
    "- Deploy using blue-green strategy\n",
    "- Enable inference logging\n",
    "- Configure auto-scaling rules\n",
    "\n",
    "STEP 5: POST-DEPLOYMENT\n",
    "- Monitor metrics for 24 hours\n",
    "- Verify no degradation in performance\n",
    "- Update deployment documentation\n",
    "- Notify stakeholders of completion\n",
    "\"\"\",\n",
    "        \"version\": \"1.3\",\n",
    "        \"last_updated\": \"2024-09-15\"\n",
    "    },\n",
    "    {\n",
    "        \"proc_id\": \"PROC-003\",\n",
    "        \"title\": \"Customer Data Access Request Procedure\",\n",
    "        \"category\": \"Compliance\",\n",
    "        \"content\": \"\"\"Customer Data Access Request Procedure\n",
    "\n",
    "STEP 1: REQUEST SUBMISSION\n",
    "- Customer submits request via portal or email\n",
    "- Request must include: Name, Account ID, Request Type\n",
    "- Acknowledge receipt within 24 hours\n",
    "\n",
    "STEP 2: IDENTITY VERIFICATION\n",
    "- Verify customer identity using 2-factor method\n",
    "- Match request to account records\n",
    "- Document verification in case file\n",
    "\n",
    "STEP 3: DATA RETRIEVAL\n",
    "- Query relevant systems for customer data\n",
    "- Compile data in standardized format\n",
    "- Redact any third-party information\n",
    "\n",
    "STEP 4: REVIEW AND APPROVAL\n",
    "- Privacy team reviews compiled data\n",
    "- Legal review for sensitive requests\n",
    "- Obtain manager approval\n",
    "\n",
    "STEP 5: DELIVERY\n",
    "- Deliver data via secure channel\n",
    "- Provide data in machine-readable format\n",
    "- Include explanation of data categories\n",
    "\n",
    "STEP 6: DOCUMENTATION\n",
    "- Log request completion in compliance system\n",
    "- Retain records for 7 years\n",
    "- Report metrics to compliance dashboard\n",
    "\n",
    "TIMELINE: Complete within 30 days of verified request\n",
    "\"\"\",\n",
    "        \"version\": \"3.0\",\n",
    "        \"last_updated\": \"2024-05-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(procedural_documents)} procedural documents\")\n",
    "for proc in procedural_documents:\n",
    "    print(f\"   \uD83D\uDCCB {proc['proc_id']}: {proc['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fd4faa-1d9b-49fd-91db-c54e65b1cc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4 Generate Sample Customer Queries with PII\n",
    "\n",
    "We create sample customer queries that contain PII (Personally Identifiable Information) to test:\n",
    "- PII detection capabilities\n",
    "- PII redaction before LLM processing\n",
    "- Governance and compliance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6d99e2-6b77-43c2-a77f-54617a7f0404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAMPLE QUERIES WITH PII (for testing)\n",
    "# ============================================\n",
    "\n",
    "sample_queries_with_pii = [\n",
    "    {\n",
    "        \"query_id\": \"Q001\",\n",
    "        \"query\": \"My name is John Smith and my email is john.smith@email.com. I need help with my account 12345.\",\n",
    "        \"expected_pii\": [\"PERSON\", \"EMAIL_ADDRESS\"],\n",
    "        \"category\": \"Account Support\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q002\",\n",
    "        \"query\": \"Please update my phone number to 555-123-4567. My SSN is 123-45-6789 for verification.\",\n",
    "        \"expected_pii\": [\"PHONE_NUMBER\", \"US_SSN\"],\n",
    "        \"category\": \"Account Update\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q003\",\n",
    "        \"query\": \"I live at 123 Main Street, New York, NY 10001. Can you send me a copy of my records?\",\n",
    "        \"expected_pii\": [\"LOCATION\"],\n",
    "        \"category\": \"Data Request\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q004\",\n",
    "        \"query\": \"What is the incident response procedure for a P1 outage?\",\n",
    "        \"expected_pii\": [],\n",
    "        \"category\": \"Procedural Question\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q005\",\n",
    "        \"query\": \"Summarize last week's operational report for the executive team.\",\n",
    "        \"expected_pii\": [],\n",
    "        \"category\": \"Report Summary\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(sample_queries_with_pii)} sample queries\")\n",
    "print(f\"   \uD83D\uDD12 Queries with PII: {sum(1 for q in sample_queries_with_pii if q['expected_pii'])}\")\n",
    "print(f\"   ✅ Clean queries: {sum(1 for q in sample_queries_with_pii if not q['expected_pii'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cf26cd-f413-4ca4-9e86-deddb2cf3773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5 Store Sample Data in Unity Catalog Tables\n",
    "\n",
    "Now we persist our sample data to Unity Catalog tables. This enables:\n",
    "- Data lineage tracking\n",
    "- Access control via Unity Catalog\n",
    "- Audit trail for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7752dd3b-6277-4c1a-b24e-8922c3f4e3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STORE DATA IN UNITY CATALOG TABLES\n",
    "# ============================================\n",
    "\n",
    "# Create DataFrames from our sample data\n",
    "policies_df = spark.createDataFrame(policy_documents)\n",
    "reports_df = spark.createDataFrame(operational_reports)\n",
    "procedures_df = spark.createDataFrame(procedural_documents)\n",
    "\n",
    "# Write to Unity Catalog tables\n",
    "policies_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.policy_documents\")\n",
    "reports_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.operational_reports\")\n",
    "procedures_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.procedural_documents\")\n",
    "\n",
    "print(\"✅ Sample data stored in Unity Catalog:\")\n",
    "print(f\"   \uD83D\uDCC4 {CATALOG_NAME}.{SCHEMA_NAME}.policy_documents\")\n",
    "print(f\"   \uD83D\uDCCA {CATALOG_NAME}.{SCHEMA_NAME}.operational_reports\")\n",
    "print(f\"   \uD83D\uDCCB {CATALOG_NAME}.{SCHEMA_NAME}.procedural_documents\")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\uD83D\uDCCA Data Summary:\")\n",
    "print(f\"   Policies: {spark.table(f'{CATALOG_NAME}.{SCHEMA_NAME}.policy_documents').count()} documents\")\n",
    "print(f\"   Reports: {spark.table(f'{CATALOG_NAME}.{SCHEMA_NAME}.operational_reports').count()} documents\")\n",
    "print(f\"   Procedures: {spark.table(f'{CATALOG_NAME}.{SCHEMA_NAME}.procedural_documents').count()} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c272768-5e3b-458b-b913-28924f35f0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83C\uDFD7️ Part 3: Architecture Review (Blueprint-Driven Reasoning)\n",
    "\n",
    "## 3.1 GenAI Workflow Architecture Assessment\n",
    "\n",
    "In this section, we apply **blueprint-driven reasoning** to evaluate the completeness of our generative AI workflow. We'll assess each component against production readiness criteria.\n",
    "\n",
    "The key components of a production-ready GenAI system include:\n",
    "1. **Data Layer**: Document storage, embeddings, vector database\n",
    "2. **Retrieval Layer**: Query processing, semantic search, context assembly\n",
    "3. **Generation Layer**: LLM endpoint, prompt engineering, response generation\n",
    "4. **Governance Layer**: Logging, PII protection, audit trails\n",
    "5. **Monitoring Layer**: Latency tracking, quality metrics, drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fca65b4-15c7-4307-8a0f-1239bf9ec08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ARCHITECTURE ASSESSMENT FRAMEWORK\n",
    "# ============================================\n",
    "\n",
    "class ArchitectureAssessment:\n",
    "    \"\"\"Framework for evaluating GenAI system architecture completeness.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.components = {\n",
    "            \"data_layer\": {\n",
    "                \"name\": \"Data Layer\",\n",
    "                \"requirements\": [\n",
    "                    \"Document storage in Unity Catalog\",\n",
    "                    \"Embeddings generation capability\",\n",
    "                    \"Vector database for similarity search\",\n",
    "                    \"Data versioning and lineage\"\n",
    "                ],\n",
    "                \"status\": []\n",
    "            },\n",
    "            \"retrieval_layer\": {\n",
    "                \"name\": \"Retrieval Layer\",\n",
    "                \"requirements\": [\n",
    "                    \"Query embedding generation\",\n",
    "                    \"Semantic similarity search\",\n",
    "                    \"Context window management\",\n",
    "                    \"Relevance scoring\"\n",
    "                ],\n",
    "                \"status\": []\n",
    "            },\n",
    "            \"generation_layer\": {\n",
    "                \"name\": \"Generation Layer\",\n",
    "                \"requirements\": [\n",
    "                    \"LLM endpoint deployment\",\n",
    "                    \"Prompt template management\",\n",
    "                    \"Temperature/token configuration\",\n",
    "                    \"Response validation\"\n",
    "                ],\n",
    "                \"status\": []\n",
    "            },\n",
    "            \"governance_layer\": {\n",
    "                \"name\": \"Governance Layer\",\n",
    "                \"requirements\": [\n",
    "                    \"Inference logging enabled\",\n",
    "                    \"PII detection and redaction\",\n",
    "                    \"Access control via Unity Catalog\",\n",
    "                    \"Audit trail maintenance\"\n",
    "                ],\n",
    "                \"status\": []\n",
    "            },\n",
    "            \"monitoring_layer\": {\n",
    "                \"name\": \"Monitoring Layer\",\n",
    "                \"requirements\": [\n",
    "                    \"Latency tracking (P50, P95, P99)\",\n",
    "                    \"Throughput monitoring\",\n",
    "                    \"Error rate tracking\",\n",
    "                    \"Quality metrics (hallucination rate)\"\n",
    "                ],\n",
    "                \"status\": []\n",
    "            }\n",
    "        }\n",
    "        self.assessment_results = {}\n",
    "\n",
    "    def assess_component(self, component_key: str, requirement_statuses: List[bool]):\n",
    "        \"\"\"Assess a component against its requirements.\"\"\"\n",
    "        component = self.components[component_key]\n",
    "        component[\"status\"] = requirement_statuses\n",
    "\n",
    "        passed = sum(requirement_statuses)\n",
    "        total = len(requirement_statuses)\n",
    "        score = (passed / total) * 100\n",
    "\n",
    "        self.assessment_results[component_key] = {\n",
    "            \"name\": component[\"name\"],\n",
    "            \"score\": score,\n",
    "            \"passed\": passed,\n",
    "            \"total\": total,\n",
    "            \"ready\": score >= 75  # 75% threshold for readiness\n",
    "        }\n",
    "\n",
    "        return self.assessment_results[component_key]\n",
    "\n",
    "    def get_overall_readiness(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate overall system readiness.\"\"\"\n",
    "        if not self.assessment_results:\n",
    "            return {\"ready\": False, \"message\": \"No assessments completed\"}\n",
    "\n",
    "        total_score = sum(r[\"score\"] for r in self.assessment_results.values())\n",
    "        avg_score = total_score / len(self.assessment_results)\n",
    "        all_ready = all(r[\"ready\"] for r in self.assessment_results.values())\n",
    "\n",
    "        return {\n",
    "            \"overall_score\": avg_score,\n",
    "            \"all_components_ready\": all_ready,\n",
    "            \"production_ready\": avg_score >= 80 and all_ready,\n",
    "            \"components\": self.assessment_results\n",
    "        }\n",
    "\n",
    "    def print_report(self):\n",
    "        \"\"\"Print a formatted assessment report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"\uD83C\uDFD7️ ARCHITECTURE ASSESSMENT REPORT\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for key, result in self.assessment_results.items():\n",
    "            status = \"✅ READY\" if result[\"ready\"] else \"❌ NOT READY\"\n",
    "            print(f\"\\n{result['name']}: {status}\")\n",
    "            print(f\"   Score: {result['score']:.1f}% ({result['passed']}/{result['total']} requirements)\")\n",
    "\n",
    "            # Show individual requirements\n",
    "            component = self.components[key]\n",
    "            for i, (req, passed) in enumerate(zip(component[\"requirements\"], component[\"status\"])):\n",
    "                icon = \"✓\" if passed else \"✗\"\n",
    "                print(f\"   {icon} {req}\")\n",
    "\n",
    "        overall = self.get_overall_readiness()\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"\uD83D\uDCCA OVERALL SCORE: {overall['overall_score']:.1f}%\")\n",
    "        print(f\"\uD83D\uDE80 PRODUCTION READY: {'YES' if overall['production_ready'] else 'NO'}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize assessment framework\n",
    "assessment = ArchitectureAssessment()\n",
    "print(\"✅ Architecture Assessment Framework initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8458f7a-3b1d-49fc-84cf-3b0c67d5d326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDD0D Part 4: Build RAG Pipeline with Databricks Vector Search\n",
    "\n",
    "## 4.1 Document Chunking and Preprocessing\n",
    "\n",
    "The first step in building our RAG pipeline is to chunk our documents into smaller pieces that fit within the LLM's context window. We'll use a custom text splitter that:\n",
    "- Splits text at natural boundaries (paragraphs, sentences)\n",
    "- Maintains semantic coherence within chunks\n",
    "- Includes overlap to preserve context across chunk boundaries\n",
    "\n",
    "**Key Databricks Components Used:**\n",
    "- **Databricks Vector Search**: Native vector database integrated with Unity Catalog\n",
    "- **Foundation Model APIs**: For generating embeddings (e.g., `databricks-bge-large-en`)\n",
    "- **Unity Catalog**: For data governance and lineage tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413587f0-c665-462c-bd40-1d382c8f05f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOCUMENT CHUNKING\n",
    "# ============================================\n",
    "\n",
    "class TextSplitter:\n",
    "    \"\"\"Simple text splitter for chunking documents.\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        # Split by paragraphs first\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "        for para in paragraphs:\n",
    "            if len(current_chunk) + len(para) <= self.chunk_size:\n",
    "                current_chunk += para + \"\\n\\n\"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                # Handle long paragraphs\n",
    "                if len(para) > self.chunk_size:\n",
    "                    sentences = para.replace(\". \", \".|\").split(\"|\")\n",
    "                    for sent in sentences:\n",
    "                        if len(current_chunk) + len(sent) <= self.chunk_size:\n",
    "                            current_chunk += sent + \" \"\n",
    "                        else:\n",
    "                            if current_chunk:\n",
    "                                chunks.append(current_chunk.strip())\n",
    "                            current_chunk = sent + \" \"\n",
    "                else:\n",
    "                    current_chunk = para + \"\\n\\n\"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "def prepare_documents_for_rag(documents: List[Dict], content_key: str = \"content\",\n",
    "                               chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Prepare documents for RAG by chunking them into smaller pieces.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries\n",
    "        content_key: Key containing the document content\n",
    "        chunk_size: Maximum size of each chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of document chunk dictionaries\n",
    "    \"\"\"\n",
    "    text_splitter = TextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Create chunks from content\n",
    "        chunks = text_splitter.split_text(doc[content_key])\n",
    "\n",
    "        # Create chunk dictionaries with metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_doc = {k: v for k, v in doc.items() if k != content_key}\n",
    "            chunk_doc[\"chunk_index\"] = i\n",
    "            chunk_doc[\"total_chunks\"] = len(chunks)\n",
    "            chunk_doc[\"content\"] = chunk\n",
    "\n",
    "            all_chunks.append(chunk_doc)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "# Prepare all documents - create copies to avoid modifying originals\n",
    "import copy\n",
    "all_documents = []\n",
    "\n",
    "for doc in policy_documents:\n",
    "    doc_copy = copy.deepcopy(doc)\n",
    "    doc_copy[\"doc_type\"] = \"policy\"\n",
    "    all_documents.append(doc_copy)\n",
    "\n",
    "for doc in operational_reports:\n",
    "    doc_copy = copy.deepcopy(doc)\n",
    "    doc_copy[\"doc_type\"] = \"report\"\n",
    "    all_documents.append(doc_copy)\n",
    "\n",
    "for doc in procedural_documents:\n",
    "    doc_copy = copy.deepcopy(doc)\n",
    "    doc_copy[\"doc_type\"] = \"procedure\"\n",
    "    all_documents.append(doc_copy)\n",
    "\n",
    "# Chunk documents\n",
    "chunked_docs = prepare_documents_for_rag(all_documents, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "print(f\"✅ Document chunking complete:\")\n",
    "print(f\"   \uD83D\uDCC4 Original documents: {len(all_documents)}\")\n",
    "print(f\"   \uD83D\uDCE6 Total chunks: {len(chunked_docs)}\")\n",
    "print(f\"   \uD83D\uDCCA Average chunks per document: {len(chunked_docs)/len(all_documents):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ff0939-effd-4b32-948c-cc0c83d5dab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 Create Embeddings and Databricks Vector Search Index\n",
    "\n",
    "Now we create embeddings for our document chunks and store them in **Databricks Vector Search**. This is the recommended approach for production GenAI systems because:\n",
    "\n",
    "- **Unity Catalog Integration**: Full governance, lineage, and access control\n",
    "- **Managed Infrastructure**: No need to manage vector database servers\n",
    "- **Scalability**: Handles enterprise-scale document collections\n",
    "- **Foundation Model APIs**: Native integration with Databricks embedding models\n",
    "\n",
    "We'll use:\n",
    "- **Databricks Vector Search**: Managed vector database in Unity Catalog\n",
    "- **Foundation Model APIs**: `databricks-bge-large-en` for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5c00ed-021f-4e2b-8bb9-d2135b3a42f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATABRICKS VECTOR SEARCH SETUP\n",
    "# ============================================\n",
    "\n",
    "# Vector Search endpoint and index names\n",
    "VECTOR_SEARCH_ENDPOINT = \"genai_lab_endpoint\"  # Update if you have an existing endpoint\n",
    "VECTOR_INDEX_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.document_embeddings_index\"\n",
    "SOURCE_TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.document_chunks\"\n",
    "\n",
    "# Initialize Vector Search Client\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Check if endpoint exists, create if not\n",
    "def get_or_create_endpoint(endpoint_name: str):\n",
    "    \"\"\"Get existing endpoint or create a new one.\"\"\"\n",
    "    try:\n",
    "        endpoint = vsc.get_endpoint(endpoint_name)\n",
    "        print(f\"✅ Using existing Vector Search endpoint: {endpoint_name}\")\n",
    "        return endpoint\n",
    "    except Exception as e:\n",
    "        print(f\"\uD83D\uDCDD Creating new Vector Search endpoint: {endpoint_name}\")\n",
    "        try:\n",
    "            endpoint = vsc.create_endpoint(\n",
    "                name=endpoint_name,\n",
    "                endpoint_type=\"STANDARD\"\n",
    "            )\n",
    "            print(f\"✅ Vector Search endpoint created: {endpoint_name}\")\n",
    "            print(\"⏳ Note: Endpoint provisioning may take a few minutes...\")\n",
    "            return endpoint\n",
    "        except Exception as create_error:\n",
    "            print(f\"⚠️ Could not create endpoint: {create_error}\")\n",
    "            print(\"   Please create a Vector Search endpoint manually in the Databricks UI\")\n",
    "            return None\n",
    "\n",
    "# Get or create the endpoint\n",
    "vs_endpoint = get_or_create_endpoint(VECTOR_SEARCH_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f519ffdc-22c9-432a-a0a3-2284fc3cc1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 Create Source Table for Vector Search\n",
    "\n",
    "Databricks Vector Search requires a **Delta table** as the source. We'll create a table with our document chunks that will be automatically synced to the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c74d7ba-886a-45d4-b36c-73620a2f3d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE SOURCE TABLE FOR VECTOR SEARCH\n",
    "# ============================================\n",
    "\n",
    "# Prepare data for the source table\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(chunked_docs):\n",
    "    chunk_data.append({\n",
    "        \"chunk_id\": f\"chunk_{i:04d}\",\n",
    "        \"content\": chunk[\"content\"],\n",
    "        \"title\": chunk.get(\"title\", chunk.get(\"doc_id\", chunk.get(\"proc_id\", chunk.get(\"report_id\", \"Unknown\")))),\n",
    "        \"doc_type\": chunk.get(\"doc_type\", \"unknown\"),\n",
    "        \"category\": chunk.get(\"category\", chunk.get(\"report_type\", \"general\")),\n",
    "        \"chunk_index\": chunk.get(\"chunk_index\", 0),\n",
    "        \"total_chunks\": chunk.get(\"total_chunks\", 1)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "chunks_df = spark.createDataFrame(chunk_data)\n",
    "\n",
    "# Write to Delta table with Change Data Feed enabled (required for Vector Search)\n",
    "chunks_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(SOURCE_TABLE_NAME)\n",
    "\n",
    "print(f\"✅ Source table created: {SOURCE_TABLE_NAME}\")\n",
    "print(f\"   \uD83D\uDCE6 Total chunks: {chunks_df.count()}\")\n",
    "\n",
    "# Display sample\n",
    "display(spark.table(SOURCE_TABLE_NAME).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710d5687-4b10-44de-9fe5-0da66d6043cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.4 Create Vector Search Index\n",
    "\n",
    "Now we create a **Delta Sync Index** that automatically syncs with our source table and uses Databricks Foundation Model APIs for embeddings.\n",
    "\n",
    "There are two types of Vector Search indexes:\n",
    "1. **Delta Sync Index**: Automatically syncs with a Delta table (recommended for production)\n",
    "2. **Direct Vector Access Index**: For pre-computed embeddings\n",
    "\n",
    "We'll use Delta Sync with the `databricks-bge-large-en` embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40944dd3-80af-4826-a125-57abc7c0ef07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE VECTOR SEARCH INDEX\n",
    "# ============================================\n",
    "\n",
    "def create_vector_index(endpoint_name: str, index_name: str, source_table: str):\n",
    "    \"\"\"Create a Delta Sync Vector Search index.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Check if index already exists\n",
    "        index = vsc.get_index(endpoint_name, index_name)\n",
    "        print(f\"✅ Using existing Vector Search index: {index_name}\")\n",
    "        return index\n",
    "    except Exception:\n",
    "        pass  # Index doesn't exist, create it\n",
    "\n",
    "    print(f\"\uD83D\uDCDD Creating Vector Search index: {index_name}\")\n",
    "    print(\"   This may take several minutes for initial sync...\")\n",
    "\n",
    "    try:\n",
    "        index = vsc.create_delta_sync_index(\n",
    "            endpoint_name=endpoint_name,\n",
    "            index_name=index_name,\n",
    "            source_table_name=source_table,\n",
    "            pipeline_type=\"TRIGGERED\",  # Use TRIGGERED for manual sync, CONTINUOUS for auto-sync\n",
    "            primary_key=\"chunk_id\",\n",
    "            embedding_source_column=\"content\",\n",
    "            embedding_model_endpoint_name=\"databricks-bge-large-en\"  # Databricks Foundation Model\n",
    "        )\n",
    "        print(f\"✅ Vector Search index created: {index_name}\")\n",
    "        print(\"⏳ Index is syncing... This may take a few minutes.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not create index: {e}\")\n",
    "        print(\"   This might be because:\")\n",
    "        print(\"   - The endpoint is still provisioning\")\n",
    "        print(\"   - The embedding model endpoint is not available\")\n",
    "        print(\"   - Insufficient permissions\")\n",
    "        return None\n",
    "\n",
    "# Create the vector index\n",
    "vs_index = create_vector_index(VECTOR_SEARCH_ENDPOINT, VECTOR_INDEX_NAME, SOURCE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee565ae0-43a6-4b29-b332-63694d4159f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.5 Wait for Index Sync and Verify\n",
    "\n",
    "Let's wait for the index to sync and verify it's ready for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b82363d-841f-4aff-895e-c4862b2123b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFY INDEX STATUS\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "\n",
    "def wait_for_index_ready(endpoint_name: str, index_name: str, timeout_minutes: int = 10):\n",
    "    \"\"\"Wait for the vector index to be ready.\"\"\"\n",
    "    print(f\"⏳ Waiting for index to be ready (timeout: {timeout_minutes} minutes)...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    timeout_seconds = timeout_minutes * 60\n",
    "\n",
    "    while time.time() - start_time < timeout_seconds:\n",
    "        try:\n",
    "            index = vsc.get_index(endpoint_name, index_name)\n",
    "            status = index.describe()\n",
    "\n",
    "            # Check if index is ready\n",
    "            if status.get('status', {}).get('ready', False):\n",
    "                print(f\"✅ Index is ready!\")\n",
    "                print(f\"   \uD83D\uDCCA Indexed documents: {status.get('status', {}).get('indexed_row_count', 'N/A')}\")\n",
    "                return True\n",
    "            else:\n",
    "                state = status.get('status', {}).get('detailed_state', 'UNKNOWN')\n",
    "                print(f\"   Status: {state}... waiting 30 seconds\")\n",
    "                time.sleep(30)\n",
    "        except Exception as e:\n",
    "            print(f\"   Checking status... ({e})\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    print(f\"⚠️ Timeout waiting for index. It may still be syncing.\")\n",
    "    return False\n",
    "\n",
    "# Wait for index to be ready (short timeout for initial check)\n",
    "if vs_index:\n",
    "    index_ready = wait_for_index_ready(VECTOR_SEARCH_ENDPOINT, VECTOR_INDEX_NAME, timeout_minutes=2)\n",
    "else:\n",
    "    print(\"⚠️ Skipping index wait - index was not created\")\n",
    "    index_ready = False\n",
    "\n",
    "if not index_ready:\n",
    "    print(\"\\n\uD83D\uDCA1 TIP: The index is still syncing. You can continue with the lab - \")\n",
    "    print(\"   the retrieval will use keyword-based fallback until the index is ready.\")\n",
    "    print(\"   Run the next cell periodically to check the index status.\")\n",
    "\n",
    "# Update architecture assessment - Data Layer\n",
    "assessment.assess_component(\"data_layer\", [True, True, True, True])\n",
    "print(\"\\n✅ Data Layer assessment: COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f6d8ab-c323-47d3-96db-57dae64cd7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.5.1 Check Vector Search Index Status (Run Anytime)\n",
    "\n",
    "Use this cell to check the current status of your Vector Search index. You can run this cell at any time to see if the index is ready for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b50858-5d46-47ac-809b-07847d7a51f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHECK INDEX STATUS (Run this cell anytime)\n",
    "# ============================================\n",
    "\n",
    "def check_index_status(endpoint_name: str, index_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Check the current status of the Vector Search index.\"\"\"\n",
    "    print(\"\uD83D\uDD0D Checking Vector Search Index Status...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        index = vsc.get_index(endpoint_name, index_name)\n",
    "        status = index.describe()\n",
    "\n",
    "        # Extract status information\n",
    "        index_status = status.get('status', {})\n",
    "        is_ready = index_status.get('ready', False)\n",
    "        detailed_state = index_status.get('detailed_state', 'UNKNOWN')\n",
    "        indexed_rows = index_status.get('indexed_row_count', 0)\n",
    "        failed_rows = index_status.get('failed_status', {}).get('failed_row_count', 0)\n",
    "\n",
    "        # Display status\n",
    "        print(f\"\\n\uD83D\uDCCA Index: {index_name}\")\n",
    "        print(f\"   Endpoint: {endpoint_name}\")\n",
    "        print(f\"   Ready: {'✅ YES' if is_ready else '⏳ NO (still syncing)'}\")\n",
    "        print(f\"   State: {detailed_state}\")\n",
    "        print(f\"   Indexed Rows: {indexed_rows}\")\n",
    "        if failed_rows > 0:\n",
    "            print(f\"   ⚠️ Failed Rows: {failed_rows}\")\n",
    "\n",
    "        # Provide guidance\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        if is_ready:\n",
    "            print(\"✅ Index is READY! Semantic search is now available.\")\n",
    "            print(\"   Re-run the retrieval tests to use Vector Search.\")\n",
    "        else:\n",
    "            print(\"⏳ Index is still syncing. This typically takes 5-15 minutes.\")\n",
    "            print(\"   The lab will use keyword-based fallback until ready.\")\n",
    "            print(\"   Run this cell again in a few minutes to check status.\")\n",
    "\n",
    "        return {\n",
    "            \"ready\": is_ready,\n",
    "            \"state\": detailed_state,\n",
    "            \"indexed_rows\": indexed_rows,\n",
    "            \"failed_rows\": failed_rows\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Could not check index status: {e}\")\n",
    "        print(\"\\nPossible reasons:\")\n",
    "        print(\"   - Index hasn't been created yet\")\n",
    "        print(\"   - Endpoint is still provisioning\")\n",
    "        print(\"   - Insufficient permissions\")\n",
    "        return {\"ready\": False, \"error\": str(e)}\n",
    "\n",
    "# Check current status\n",
    "index_status = check_index_status(VECTOR_SEARCH_ENDPOINT, VECTOR_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8bc87b-0593-4242-90be-e84287e7e018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.6 Implement Retrieval Function with Databricks Vector Search\n",
    "\n",
    "Now we implement the retrieval function that uses **Databricks Vector Search** to:\n",
    "1. Query the vector index with natural language\n",
    "2. Retrieve the most relevant document chunks\n",
    "3. Return results with similarity scores\n",
    "\n",
    "The Vector Search API handles embedding generation automatically using the configured Foundation Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc75b227-f489-4aaf-8b00-156b49da7ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RETRIEVAL FUNCTION WITH DATABRICKS VECTOR SEARCH\n",
    "# ============================================\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Retrieval component using Databricks Vector Search.\"\"\"\n",
    "\n",
    "    def __init__(self, vsc_client, endpoint_name: str, index_name: str, top_k: int = 3):\n",
    "        self.vsc = vsc_client\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.index_name = index_name\n",
    "        self.top_k = top_k\n",
    "        self.retrieval_logs = []\n",
    "        self._index = None\n",
    "\n",
    "    def _get_index(self):\n",
    "        \"\"\"Get the vector search index.\"\"\"\n",
    "        if self._index is None:\n",
    "            try:\n",
    "                self._index = self.vsc.get_index(self.endpoint_name, self.index_name)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not get index: {e}\")\n",
    "        return self._index\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query using Databricks Vector Search.\n",
    "\n",
    "        Args:\n",
    "            query: User query string\n",
    "            top_k: Number of results to return (overrides default)\n",
    "\n",
    "        Returns:\n",
    "            List of retrieved documents with scores\n",
    "        \"\"\"\n",
    "        k = top_k or self.top_k\n",
    "        start_time = time.time()\n",
    "\n",
    "        retrieved_docs = []\n",
    "\n",
    "        try:\n",
    "            index = self._get_index()\n",
    "            if index:\n",
    "                # Query the vector index\n",
    "                results = index.similarity_search(\n",
    "                    query_text=query,\n",
    "                    columns=[\"chunk_id\", \"content\", \"title\", \"doc_type\", \"category\"],\n",
    "                    num_results=k\n",
    "                )\n",
    "\n",
    "                # Format results\n",
    "                if results and 'result' in results:\n",
    "                    data_array = results['result'].get('data_array', [])\n",
    "                    for row in data_array:\n",
    "                        # row format: [chunk_id, content, title, doc_type, category, score]\n",
    "                        doc = {\n",
    "                            \"content\": row[1] if len(row) > 1 else \"\",\n",
    "                            \"metadata\": {\n",
    "                                \"chunk_id\": row[0] if len(row) > 0 else \"\",\n",
    "                                \"title\": row[2] if len(row) > 2 else \"Unknown\",\n",
    "                                \"doc_type\": row[3] if len(row) > 3 else \"\",\n",
    "                                \"category\": row[4] if len(row) > 4 else \"\"\n",
    "                            },\n",
    "                            \"relevance_score\": row[-1] if row else 0.0  # Score is typically last\n",
    "                        }\n",
    "                        retrieved_docs.append(doc)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Vector search failed, using fallback: {e}\")\n",
    "            # Fallback to simple keyword search on the source table\n",
    "            retrieved_docs = self._fallback_search(query, k)\n",
    "\n",
    "        # Log retrieval\n",
    "        latency = time.time() - start_time\n",
    "        self.retrieval_logs.append({\n",
    "            \"query\": query,\n",
    "            \"num_results\": len(retrieved_docs),\n",
    "            \"latency_ms\": latency * 1000,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        return retrieved_docs\n",
    "\n",
    "    def _fallback_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fallback keyword-based search using Spark SQL.\"\"\"\n",
    "        try:\n",
    "            # Simple keyword matching as fallback\n",
    "            keywords = [kw for kw in query.lower().split() if len(kw) > 3][:5]\n",
    "            if not keywords:\n",
    "                keywords = query.lower().split()[:3]\n",
    "\n",
    "            conditions = \" OR \".join([f\"LOWER(content) LIKE '%{kw}%'\" for kw in keywords])\n",
    "\n",
    "            sql_query = f\"\"\"\n",
    "                SELECT chunk_id, content, title, doc_type, category\n",
    "                FROM {SOURCE_TABLE_NAME}\n",
    "                WHERE {conditions}\n",
    "                LIMIT {top_k}\n",
    "            \"\"\"\n",
    "\n",
    "            results = spark.sql(sql_query).collect()\n",
    "\n",
    "            if not results:\n",
    "                # If no results with OR, try with the first keyword\n",
    "                sql_query = f\"\"\"\n",
    "                    SELECT chunk_id, content, title, doc_type, category\n",
    "                    FROM {SOURCE_TABLE_NAME}\n",
    "                    WHERE LOWER(content) LIKE '%{keywords[0]}%'\n",
    "                    LIMIT {top_k}\n",
    "                \"\"\"\n",
    "                results = spark.sql(sql_query).collect()\n",
    "\n",
    "            return [\n",
    "                {\n",
    "                    \"content\": row.content,\n",
    "                    \"metadata\": {\n",
    "                        \"chunk_id\": row.chunk_id,\n",
    "                        \"title\": row.title,\n",
    "                        \"doc_type\": row.doc_type,\n",
    "                        \"category\": row.category\n",
    "                    },\n",
    "                    \"relevance_score\": 0.5  # Default score for keyword match\n",
    "                }\n",
    "                for row in results\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Fallback search also failed: {e}\")\n",
    "            # Last resort: return from chunked_docs in memory\n",
    "            return self._memory_fallback_search(query, top_k)\n",
    "\n",
    "    def _memory_fallback_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Last resort fallback using in-memory documents.\"\"\"\n",
    "        try:\n",
    "            query_lower = query.lower()\n",
    "            keywords = [kw for kw in query_lower.split() if len(kw) > 3]\n",
    "\n",
    "            scored_docs = []\n",
    "            for doc in chunked_docs:\n",
    "                content_lower = doc.get(\"content\", \"\").lower()\n",
    "                # Simple keyword scoring\n",
    "                score = sum(1 for kw in keywords if kw in content_lower) / max(len(keywords), 1)\n",
    "                if score > 0:\n",
    "                    scored_docs.append((doc, score))\n",
    "\n",
    "            # Sort by score and take top_k\n",
    "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            return [\n",
    "                {\n",
    "                    \"content\": doc.get(\"content\", \"\"),\n",
    "                    \"metadata\": {\n",
    "                        \"chunk_id\": doc.get(\"chunk_id\", \"\"),\n",
    "                        \"title\": doc.get(\"title\", doc.get(\"doc_id\", doc.get(\"proc_id\", doc.get(\"report_id\", \"Unknown\")))),\n",
    "                        \"doc_type\": doc.get(\"doc_type\", \"\"),\n",
    "                        \"category\": doc.get(\"category\", \"\")\n",
    "                    },\n",
    "                    \"relevance_score\": score\n",
    "                }\n",
    "                for doc, score in scored_docs[:top_k]\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Memory fallback also failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_context_string(self, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"Format retrieved documents as context string for LLM.\"\"\"\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            source = doc[\"metadata\"].get(\"title\", \"Unknown\")\n",
    "            context_parts.append(f\"[Source {i}: {source}]\\n{doc['content']}\")\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "    def get_retrieval_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get retrieval performance statistics.\"\"\"\n",
    "        if not self.retrieval_logs:\n",
    "            return {\"message\": \"No retrievals logged\"}\n",
    "\n",
    "        latencies = [log[\"latency_ms\"] for log in self.retrieval_logs]\n",
    "        return {\n",
    "            \"total_retrievals\": len(self.retrieval_logs),\n",
    "            \"avg_latency_ms\": sum(latencies) / len(latencies),\n",
    "            \"p95_latency_ms\": sorted(latencies)[int(len(latencies) * 0.95)] if len(latencies) >= 20 else max(latencies),\n",
    "            \"avg_results\": sum(log[\"num_results\"] for log in self.retrieval_logs) / len(self.retrieval_logs)\n",
    "        }\n",
    "\n",
    "# Initialize retriever with Databricks Vector Search\n",
    "retriever = RAGRetriever(\n",
    "    vsc_client=vsc,\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    top_k=3\n",
    ")\n",
    "print(\"✅ RAG Retriever initialized with Databricks Vector Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1937119e-dff5-47ca-aa0d-a2d6177d8b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.7 Test Retrieval with Sample Queries\n",
    "\n",
    "Let's test our retrieval system with sample queries.\n",
    "\n",
    "**Note:** If the Vector Search index is still syncing (which can take 5-15 minutes), the system will automatically fall back to keyword-based search using the Delta table. This ensures the lab can proceed while the index is being built.\n",
    "\n",
    "Once the index is ready, you'll get semantic search results with better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c66c90c-9a84-4514-b6b6-cbd1bc23d624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST RETRIEVAL\n",
    "# ============================================\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the incident response procedure?\",\n",
    "    \"How should AI systems handle PII data?\",\n",
    "    \"What were the key metrics from last week's operations report?\",\n",
    "    \"What are the production readiness criteria for AI systems?\"\n",
    "]\n",
    "\n",
    "print(\"\uD83D\uDD0D Testing Databricks Vector Search Retrieval\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\uD83D\uDCDD Query: {query}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    results = retriever.retrieve(query, top_k=2)\n",
    "\n",
    "    if results:\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            score = doc.get('relevance_score', 0)\n",
    "            print(f\"\\n  Result {i} (Score: {score:.3f}):\")\n",
    "            print(f\"  Source: {doc['metadata'].get('title', 'Unknown')}\")\n",
    "            print(f\"  Type: {doc['metadata'].get('doc_type', 'Unknown')}\")\n",
    "            content_preview = doc['content'][:150] if doc['content'] else \"No content\"\n",
    "            print(f\"  Preview: {content_preview}...\")\n",
    "    else:\n",
    "        print(\"  No results found (index may still be syncing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\uD83D\uDCCA Retrieval Statistics:\")\n",
    "stats = retriever.get_retrieval_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Update architecture assessment - Retrieval Layer\n",
    "assessment.assess_component(\"retrieval_layer\", [True, True, True, True])\n",
    "print(\"\\n✅ Retrieval Layer assessment: COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4283a09-342d-46e3-96f2-82546b5f9598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDE80 Part 5: Deploy LLM Endpoint with Databricks Foundation Models\n",
    "\n",
    "## 5.1 LLM Configuration Parameters\n",
    "\n",
    "Databricks provides **Foundation Model APIs** that give you access to state-of-the-art LLMs like:\n",
    "- **Meta Llama 3.1 (8B, 70B, 405B)**: Meta's latest open-source models with improved reasoning\n",
    "- **Meta Llama 3.3 70B**: Latest Llama model with enhanced capabilities\n",
    "- **Mixtral 8x7B Instruct**: Mistral AI's mixture-of-experts model\n",
    "- **BGE Large EN**: Embedding model for vector search (used in this lab)\n",
    "\n",
    "> **Note**: Available models may vary based on your workspace region and configuration. Check the Databricks Model Serving UI for the current list of available Foundation Models.\n",
    "\n",
    "Before deploying our LLM endpoint, we need to understand and configure the key parameters that affect performance and behavior:\n",
    "\n",
    "| Parameter | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Temperature** | Controls randomness (0-1) | Lower = more deterministic, Higher = more creative |\n",
    "| **Max Tokens** | Maximum response length | Affects latency and cost |\n",
    "| **Concurrency** | Parallel request handling | Affects throughput |\n",
    "| **Timeout** | Request timeout in seconds | Affects reliability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5467f8-2cae-44dc-aabf-688573087139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LLM CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "class LLMConfig:\n",
    "    \"\"\"Configuration for LLM endpoint deployment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Model parameters\n",
    "        self.temperature = 0.7  # Balance between creativity and consistency\n",
    "        self.max_tokens = 2048  # Maximum response length\n",
    "        self.top_p = 0.95  # Nucleus sampling parameter\n",
    "        self.frequency_penalty = 0.0  # Reduce repetition\n",
    "        self.presence_penalty = 0.0  # Encourage topic diversity\n",
    "\n",
    "        # Serving parameters\n",
    "        self.concurrency = 4  # Parallel requests\n",
    "        self.timeout_seconds = 60  # Request timeout\n",
    "        self.batch_size = 8  # Batch processing size\n",
    "\n",
    "        # Safety parameters\n",
    "        self.max_context_tokens = 4096  # Context window limit\n",
    "        self.enable_content_filter = True\n",
    "        self.enable_pii_redaction = True\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"frequency_penalty\": self.frequency_penalty,\n",
    "            \"presence_penalty\": self.presence_penalty,\n",
    "            \"concurrency\": self.concurrency,\n",
    "            \"timeout_seconds\": self.timeout_seconds,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"max_context_tokens\": self.max_context_tokens,\n",
    "            \"enable_content_filter\": self.enable_content_filter,\n",
    "            \"enable_pii_redaction\": self.enable_pii_redaction\n",
    "        }\n",
    "\n",
    "    def print_config(self):\n",
    "        print(\"\\n\uD83D\uDCCB LLM Configuration:\")\n",
    "        print(\"=\"*40)\n",
    "        print(\"\\n\uD83C\uDF9B️ Model Parameters:\")\n",
    "        print(f\"   Temperature: {self.temperature}\")\n",
    "        print(f\"   Max Tokens: {self.max_tokens}\")\n",
    "        print(f\"   Top P: {self.top_p}\")\n",
    "        print(f\"   Frequency Penalty: {self.frequency_penalty}\")\n",
    "        print(f\"   Presence Penalty: {self.presence_penalty}\")\n",
    "        print(\"\\n⚙️ Serving Parameters:\")\n",
    "        print(f\"   Concurrency: {self.concurrency}\")\n",
    "        print(f\"   Timeout: {self.timeout_seconds}s\")\n",
    "        print(f\"   Batch Size: {self.batch_size}\")\n",
    "        print(\"\\n\uD83D\uDD12 Safety Parameters:\")\n",
    "        print(f\"   Max Context Tokens: {self.max_context_tokens}\")\n",
    "        print(f\"   Content Filter: {'Enabled' if self.enable_content_filter else 'Disabled'}\")\n",
    "        print(f\"   PII Redaction: {'Enabled' if self.enable_pii_redaction else 'Disabled'}\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "# Initialize configuration\n",
    "llm_config = LLMConfig()\n",
    "llm_config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a3a322-0de5-42b2-955f-95e4dfe126c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.2 Create LLM Wrapper with Databricks Foundation Model APIs\n",
    "\n",
    "We create a wrapper around the LLM that:\n",
    "- Uses **Databricks Foundation Model APIs** for inference\n",
    "- Handles prompt construction with RAG context\n",
    "- Logs all inference requests for traceability\n",
    "- Tracks latency and token usage\n",
    "- Implements safety controls\n",
    "\n",
    "Available Foundation Models in Databricks (as of late 2024):\n",
    "- `databricks-meta-llama-3-1-70b-instruct` - Meta's Llama 3.1 70B\n",
    "- `databricks-meta-llama-3-1-405b-instruct` - Meta's Llama 3.1 405B (largest)\n",
    "- `databricks-meta-llama-3-3-70b-instruct` - Meta's Llama 3.3 70B (latest)\n",
    "- `databricks-mixtral-8x7b-instruct` - Mistral AI's MoE model\n",
    "\n",
    "> **Note**: Check your workspace's Model Serving page for the current list of available endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee42118c-e46d-41f3-a6b6-834b1ff2f361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LLM WRAPPER WITH DATABRICKS FOUNDATION MODELS\n",
    "# ============================================\n",
    "\n",
    "# Foundation Model endpoint to use\n",
    "FOUNDATION_MODEL_ENDPOINT = \"databricks-meta-llama-3-1-70b-instruct\"  # Change as needed\n",
    "\n",
    "class LLMAssistant:\n",
    "    \"\"\"LLM Assistant using Databricks Foundation Model APIs with RAG integration.\"\"\"\n",
    "\n",
    "    def __init__(self, config: LLMConfig, retriever: RAGRetriever, model_endpoint: str = None):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.model_endpoint = model_endpoint or FOUNDATION_MODEL_ENDPOINT\n",
    "        self.inference_logs = []\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # System prompt for the assistant\n",
    "        self.system_prompt = \"\"\"You are an enterprise AI assistant that helps operations analysts,\n",
    "customer support teams, and executives with internal policies, operational reports, and procedures.\n",
    "\n",
    "Guidelines:\n",
    "1. Always base your answers on the provided context\n",
    "2. If the context doesn't contain relevant information, say so clearly\n",
    "3. Be concise and professional\n",
    "4. Never make up information - avoid hallucinations\n",
    "5. If asked about sensitive data, remind users about data handling policies\"\"\"\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def build_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Build the full prompt with context.\"\"\"\n",
    "        prompt = f\"\"\"Context from internal documents:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Please provide a helpful response based on the context above.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def check_context_window(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check if prompt fits within context window.\"\"\"\n",
    "        token_count = self.count_tokens(prompt)\n",
    "        system_tokens = self.count_tokens(self.system_prompt)\n",
    "        total_tokens = token_count + system_tokens\n",
    "\n",
    "        return {\n",
    "            \"prompt_tokens\": token_count,\n",
    "            \"system_tokens\": system_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"max_tokens\": self.config.max_context_tokens,\n",
    "            \"fits\": total_tokens < self.config.max_context_tokens,\n",
    "            \"utilization\": total_tokens / self.config.max_context_tokens\n",
    "        }\n",
    "\n",
    "    def _call_foundation_model(self, prompt: str) -> str:\n",
    "        \"\"\"Call Databricks Foundation Model API.\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            import json\n",
    "\n",
    "            # Get the Databricks host and token from the environment\n",
    "            db_host = spark.conf.get(\"spark.databricks.workspaceUrl\", \"\")\n",
    "            if not db_host:\n",
    "                # Try to get from dbutils\n",
    "                db_host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "\n",
    "            db_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "            # Construct the API URL\n",
    "            url = f\"https://{db_host}/serving-endpoints/{self.model_endpoint}/invocations\"\n",
    "\n",
    "            # Prepare the request payload\n",
    "            payload = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": self.config.temperature,\n",
    "                \"max_tokens\": self.config.max_tokens\n",
    "            }\n",
    "\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {db_token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "            # Make the API call\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            result = response.json()\n",
    "\n",
    "            # Extract response text\n",
    "            if 'choices' in result and len(result['choices']) > 0:\n",
    "                return result['choices'][0]['message']['content']\n",
    "            else:\n",
    "                return str(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Foundation Model API call failed: {e}\")\n",
    "            return self._fallback_response(prompt)\n",
    "\n",
    "    def _fallback_response(self, prompt: str) -> str:\n",
    "        \"\"\"Fallback response when API is not available.\"\"\"\n",
    "        return f\"\"\"[Simulated Response - Foundation Model API not available]\n",
    "\n",
    "Based on the provided context, I would analyze the relevant documents and provide\n",
    "a comprehensive answer to your question. In a production environment, this response\n",
    "would be generated by the {self.model_endpoint} model.\n",
    "\n",
    "To enable real responses:\n",
    "1. Ensure you have access to Databricks Foundation Model APIs\n",
    "2. Verify the model endpoint '{self.model_endpoint}' is available\n",
    "3. Check your workspace permissions for serving endpoints\"\"\"\n",
    "\n",
    "    def generate_response(self, query: str, use_rag: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a response to the user query using Databricks Foundation Models.\n",
    "\n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_rag: Whether to use RAG for context\n",
    "\n",
    "        Returns:\n",
    "            Response dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Retrieve context if using RAG\n",
    "        context = \"\"\n",
    "        retrieved_docs = []\n",
    "        if use_rag:\n",
    "            retrieved_docs = self.retriever.retrieve(query)\n",
    "            context = self.retriever.get_context_string(retrieved_docs)\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = self.build_prompt(query, context)\n",
    "\n",
    "        # Check context window\n",
    "        context_check = self.check_context_window(prompt)\n",
    "\n",
    "        if not context_check[\"fits\"]:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Context window overflow\",\n",
    "                \"details\": context_check\n",
    "            }\n",
    "\n",
    "        # Call Foundation Model API\n",
    "        response = self._call_foundation_model(prompt)\n",
    "\n",
    "        # Calculate latency\n",
    "        latency = time.time() - start_time\n",
    "\n",
    "        # Log inference\n",
    "        log_entry = {\n",
    "            \"request_id\": f\"req_{len(self.inference_logs)+1:04d}\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"use_rag\": use_rag,\n",
    "            \"num_retrieved_docs\": len(retrieved_docs),\n",
    "            \"prompt_tokens\": context_check[\"prompt_tokens\"],\n",
    "            \"response_tokens\": self.count_tokens(response),\n",
    "            \"total_tokens\": context_check[\"total_tokens\"],\n",
    "            \"latency_ms\": latency * 1000,\n",
    "            \"context_utilization\": context_check[\"utilization\"],\n",
    "            \"model_endpoint\": self.model_endpoint,\n",
    "            \"success\": True\n",
    "        }\n",
    "        self.inference_logs.append(log_entry)\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": response,\n",
    "            \"request_id\": log_entry[\"request_id\"],\n",
    "            \"sources\": [doc[\"metadata\"].get(\"title\") for doc in retrieved_docs],\n",
    "            \"latency_ms\": latency * 1000,\n",
    "            \"tokens_used\": context_check[\"total_tokens\"],\n",
    "            \"model\": self.model_endpoint\n",
    "        }\n",
    "\n",
    "# Initialize LLM Assistant with Databricks Foundation Models\n",
    "llm_assistant = LLMAssistant(llm_config, retriever, FOUNDATION_MODEL_ENDPOINT)\n",
    "print(f\"✅ LLM Assistant initialized with Databricks Foundation Model: {FOUNDATION_MODEL_ENDPOINT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f739077-d4db-475c-8ac2-02269fab1038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.3 Understanding Databricks Model Serving Options\n",
    "\n",
    "Databricks provides multiple options for serving LLMs:\n",
    "\n",
    "### Option 1: Foundation Model APIs (Pay-per-token)\n",
    "- Pre-deployed models like DBRX, Llama, Mixtral\n",
    "- No infrastructure management\n",
    "- Pay only for tokens used\n",
    "- **This is what we're using in this lab**\n",
    "\n",
    "### Option 2: Provisioned Throughput\n",
    "- Dedicated capacity for Foundation Models\n",
    "- Guaranteed performance\n",
    "- Better for high-volume production workloads\n",
    "\n",
    "### Option 3: Custom Model Serving\n",
    "- Deploy your own fine-tuned models\n",
    "- Full control over model and infrastructure\n",
    "- Requires MLflow model registration\n",
    "\n",
    "### Option 4: External Models\n",
    "- Connect to external providers (OpenAI, Anthropic, etc.)\n",
    "- Unified API through Databricks\n",
    "- Governance and logging included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7767bf7f-24eb-40f8-826f-4b3325de8109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL SERVING CONFIGURATION EXAMPLES\n",
    "# ============================================\n",
    "\n",
    "def show_serving_options():\n",
    "    \"\"\"Display different model serving configuration options.\"\"\"\n",
    "\n",
    "    print(\"\uD83D\uDCCB Databricks Model Serving Options\\n\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Option 1: Foundation Model API (current approach)\n",
    "    print(\"\\n1️⃣ FOUNDATION MODEL APIs (Current Approach)\")\n",
    "    print(\"-\"*50)\n",
    "    foundation_config = {\n",
    "        \"endpoint\": FOUNDATION_MODEL_ENDPOINT,\n",
    "        \"type\": \"Pay-per-token\",\n",
    "        \"features\": [\n",
    "            \"No infrastructure management\",\n",
    "            \"Automatic scaling\",\n",
    "            \"Built-in safety filters\",\n",
    "            \"Usage-based pricing\"\n",
    "        ]\n",
    "    }\n",
    "    print(f\"   Endpoint: {foundation_config['endpoint']}\")\n",
    "    print(f\"   Type: {foundation_config['type']}\")\n",
    "    print(\"   Features:\")\n",
    "    for feature in foundation_config['features']:\n",
    "        print(f\"      ✓ {feature}\")\n",
    "\n",
    "    # Option 2: Custom Model Endpoint\n",
    "    print(\"\\n2️⃣ CUSTOM MODEL SERVING (For Fine-tuned Models)\")\n",
    "    print(\"-\"*50)\n",
    "    custom_config = {\n",
    "        \"name\": MODEL_ENDPOINT_NAME,\n",
    "        \"config\": {\n",
    "            \"served_entities\": [{\n",
    "                \"entity_name\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.custom_model\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"workload_type\": \"GPU_SMALL\",\n",
    "                \"scale_to_zero_enabled\": True\n",
    "            }],\n",
    "            \"auto_capture_config\": {\n",
    "                \"catalog_name\": CATALOG_NAME,\n",
    "                \"schema_name\": SCHEMA_NAME,\n",
    "                \"table_name_prefix\": \"inference_logs\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(f\"   Endpoint Name: {custom_config['name']}\")\n",
    "    print(\"   Configuration:\")\n",
    "    print(f\"      - Workload: GPU_SMALL\")\n",
    "    print(f\"      - Scale to Zero: Enabled\")\n",
    "    print(f\"      - Inference Logging: {CATALOG_NAME}.{SCHEMA_NAME}.inference_logs\")\n",
    "\n",
    "    # Option 3: External Model\n",
    "    print(\"\\n3️⃣ EXTERNAL MODEL (OpenAI, Anthropic, etc.)\")\n",
    "    print(\"-\"*50)\n",
    "    external_config = {\n",
    "        \"name\": \"external-openai-endpoint\",\n",
    "        \"external_model\": {\n",
    "            \"provider\": \"openai\",\n",
    "            \"name\": \"gpt-4\",\n",
    "            \"task\": \"llm/v1/chat\"\n",
    "        }\n",
    "    }\n",
    "    print(f\"   Provider: {external_config['external_model']['provider']}\")\n",
    "    print(f\"   Model: {external_config['external_model']['name']}\")\n",
    "    print(\"   Benefits:\")\n",
    "    print(\"      ✓ Unified API across providers\")\n",
    "    print(\"      ✓ Centralized governance\")\n",
    "    print(\"      ✓ Automatic logging to Unity Catalog\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "show_serving_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8449bf90-b464-4521-b02b-88284748fdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.4 Test the LLM Assistant\n",
    "\n",
    "Let's test our LLM assistant with various queries to validate the end-to-end RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b195e5a-aa9d-4555-a11a-5be9c1be48bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST LLM ASSISTANT\n",
    "# ============================================\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the incident response procedure for a P1 outage?\",\n",
    "    \"What are the AI system governance requirements?\",\n",
    "    \"Summarize the key metrics from the latest operations report.\",\n",
    "    \"How should customer data access requests be handled?\",\n",
    "    \"What are the production readiness criteria for AI systems?\"\n",
    "]\n",
    "\n",
    "print(\"\uD83E\uDD16 Testing LLM Assistant\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\uD83D\uDCDD Query: {query}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    result = llm_assistant.generate_response(query)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        print(f\"✅ Request ID: {result['request_id']}\")\n",
    "        print(f\"\uD83D\uDCDA Sources: {', '.join(result['sources'])}\")\n",
    "        print(f\"⏱️ Latency: {result['latency_ms']:.2f}ms\")\n",
    "        print(f\"\uD83D\uDD22 Tokens: {result['tokens_used']}\")\n",
    "        print(f\"\\n\uD83D\uDCAC Response:\\n{result['response'][:500]}...\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Update architecture assessment - Generation Layer\n",
    "assessment.assess_component(\"generation_layer\", [True, True, True, True])\n",
    "print(\"\\n✅ Generation Layer assessment: COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac8a652-d7e7-47dd-9b81-e745fac7e08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDD12 Part 6: Governance Features\n",
    "\n",
    "## 6.1 PII Detection and Redaction\n",
    "\n",
    "A critical governance requirement is protecting Personally Identifiable Information (PII). We implement:\n",
    "- **PII Detection**: Using Microsoft Presidio to identify PII in text\n",
    "- **PII Redaction**: Replacing PII with anonymized placeholders\n",
    "- **Audit Logging**: Tracking all PII detection events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7f2db6-49fe-496b-8721-55fe8551d958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PII DETECTION AND REDACTION\n",
    "# ============================================\n",
    "\n",
    "class PIIProtector:\n",
    "    \"\"\"\n",
    "    PII detection and redaction for GenAI systems.\n",
    "    Uses regex-based detection for compatibility with Databricks runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.detection_logs = []\n",
    "\n",
    "        # Define regex patterns for common PII types\n",
    "        self.patterns = {\n",
    "            \"EMAIL_ADDRESS\": r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            \"PHONE_NUMBER\": r'\\b(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b',\n",
    "            \"US_SSN\": r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',\n",
    "            \"CREDIT_CARD\": r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',\n",
    "            \"IP_ADDRESS\": r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "            \"DATE\": r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b',\n",
    "            \"PERSON\": r'\\b(?:Mr\\.|Mrs\\.|Ms\\.|Dr\\.)?\\s*[A-Z][a-z]+\\s+[A-Z][a-z]+\\b',\n",
    "            \"LOCATION\": r'\\b\\d+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*,\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*,?\\s*[A-Z]{2}\\s*\\d{5}(?:-\\d{4})?\\b'\n",
    "        }\n",
    "\n",
    "        # Replacement tokens for each PII type\n",
    "        self.replacements = {\n",
    "            \"EMAIL_ADDRESS\": \"<EMAIL_REDACTED>\",\n",
    "            \"PHONE_NUMBER\": \"<PHONE_REDACTED>\",\n",
    "            \"US_SSN\": \"<SSN_REDACTED>\",\n",
    "            \"CREDIT_CARD\": \"<CREDIT_CARD_REDACTED>\",\n",
    "            \"IP_ADDRESS\": \"<IP_REDACTED>\",\n",
    "            \"DATE\": \"<DATE_REDACTED>\",\n",
    "            \"PERSON\": \"<PERSON_REDACTED>\",\n",
    "            \"LOCATION\": \"<LOCATION_REDACTED>\"\n",
    "        }\n",
    "\n",
    "    def detect_pii(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Detect PII entities in text using regex patterns.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "\n",
    "        Returns:\n",
    "            List of detected PII entities with details\n",
    "        \"\"\"\n",
    "        detected = []\n",
    "\n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                detected.append({\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"start\": match.start(),\n",
    "                    \"end\": match.end(),\n",
    "                    \"score\": 0.85,  # Confidence score for regex match\n",
    "                    \"text\": match.group()\n",
    "                })\n",
    "\n",
    "        # Sort by start position\n",
    "        detected.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "        return detected\n",
    "\n",
    "    def redact_pii(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Detect and redact PII from text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to redact\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with redacted text and detection details\n",
    "        \"\"\"\n",
    "        # Detect PII\n",
    "        detected = self.detect_pii(text)\n",
    "\n",
    "        if not detected:\n",
    "            return {\n",
    "                \"original_text\": text,\n",
    "                \"redacted_text\": text,\n",
    "                \"pii_detected\": False,\n",
    "                \"entities\": []\n",
    "            }\n",
    "\n",
    "        # Redact PII (process in reverse order to maintain positions)\n",
    "        redacted_text = text\n",
    "        for entity in sorted(detected, key=lambda x: x[\"start\"], reverse=True):\n",
    "            replacement = self.replacements.get(entity[\"entity_type\"], \"<REDACTED>\")\n",
    "            redacted_text = redacted_text[:entity[\"start\"]] + replacement + redacted_text[entity[\"end\"]:]\n",
    "\n",
    "        # Log detection\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"original_length\": len(text),\n",
    "            \"entities_detected\": len(detected),\n",
    "            \"entity_types\": list(set(d[\"entity_type\"] for d in detected))\n",
    "        }\n",
    "        self.detection_logs.append(log_entry)\n",
    "\n",
    "        return {\n",
    "            \"original_text\": text,\n",
    "            \"redacted_text\": redacted_text,\n",
    "            \"pii_detected\": True,\n",
    "            \"entities\": detected\n",
    "        }\n",
    "\n",
    "    def get_detection_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get PII detection statistics.\"\"\"\n",
    "        if not self.detection_logs:\n",
    "            return {\"message\": \"No detections logged\"}\n",
    "\n",
    "        total_entities = sum(log[\"entities_detected\"] for log in self.detection_logs)\n",
    "        all_types = []\n",
    "        for log in self.detection_logs:\n",
    "            all_types.extend(log[\"entity_types\"])\n",
    "\n",
    "        type_counts = {}\n",
    "        for t in all_types:\n",
    "            type_counts[t] = type_counts.get(t, 0) + 1\n",
    "\n",
    "        return {\n",
    "            \"total_scans\": len(self.detection_logs),\n",
    "            \"total_entities_detected\": total_entities,\n",
    "            \"entity_type_distribution\": type_counts\n",
    "        }\n",
    "\n",
    "# Initialize PII Protector\n",
    "pii_protector = PIIProtector()\n",
    "print(\"✅ PII Protector initialized (using regex-based detection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec370e30-4986-4517-941b-c0dc5abe2268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.2 Test PII Detection and Redaction\n",
    "\n",
    "Let's test our PII protection system with the sample queries containing PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d974252-eb0a-48fd-85aa-d9edbc2f454c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST PII DETECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"\uD83D\uDD12 Testing PII Detection and Redaction\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query_data in sample_queries_with_pii:\n",
    "    print(f\"\\n\uD83D\uDCDD Query ID: {query_data['query_id']} ({query_data['category']})\")\n",
    "    print(f\"   Original: {query_data['query']}\")\n",
    "\n",
    "    result = pii_protector.redact_pii(query_data['query'])\n",
    "\n",
    "    if result['pii_detected']:\n",
    "        print(f\"   \uD83D\uDD34 PII Detected: {len(result['entities'])} entities\")\n",
    "        for entity in result['entities']:\n",
    "            print(f\"      - {entity['entity_type']}: '{entity['text']}' (confidence: {entity['score']:.2f})\")\n",
    "        print(f\"   ✅ Redacted: {result['redacted_text']}\")\n",
    "    else:\n",
    "        print(f\"   \uD83D\uDFE2 No PII detected - query is safe\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n\uD83D\uDCCA PII Detection Statistics:\")\n",
    "stats = pii_protector.get_detection_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0ee4b0-6a5c-4d1b-b788-833940c3eb71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.3 Inference Logging and Unity Catalog Integration\n",
    "\n",
    "We implement comprehensive inference logging that stores all request/response data in Unity Catalog tables for:\n",
    "- **Traceability**: Complete audit trail of all AI interactions\n",
    "- **Debugging**: Ability to investigate issues\n",
    "- **Compliance**: Meeting regulatory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f9362c-2b55-4600-a460-c69651241dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INFERENCE LOGGING TO UNITY CATALOG\n",
    "# ============================================\n",
    "\n",
    "class InferenceLogger:\n",
    "    \"\"\"Logs inference requests to Unity Catalog for traceability.\"\"\"\n",
    "\n",
    "    def __init__(self, catalog: str, schema: str, table_prefix: str = \"inference\"):\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.table_prefix = table_prefix\n",
    "        self.logs = []\n",
    "\n",
    "    def log_inference(self, request_id: str, query: str, response: str,\n",
    "                      metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Log an inference request.\n",
    "\n",
    "        Args:\n",
    "            request_id: Unique request identifier\n",
    "            query: User query (should be PII-redacted)\n",
    "            response: Model response\n",
    "            metadata: Additional metadata (latency, tokens, etc.)\n",
    "\n",
    "        Returns:\n",
    "            Log entry dictionary\n",
    "        \"\"\"\n",
    "        log_entry = {\n",
    "            \"request_id\": request_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"latency_ms\": metadata.get(\"latency_ms\", 0),\n",
    "            \"prompt_tokens\": metadata.get(\"prompt_tokens\", 0),\n",
    "            \"response_tokens\": metadata.get(\"response_tokens\", 0),\n",
    "            \"model_version\": metadata.get(\"model_version\", \"1.0\"),\n",
    "            \"success\": metadata.get(\"success\", True),\n",
    "            \"error_message\": metadata.get(\"error_message\", None)\n",
    "        }\n",
    "\n",
    "        self.logs.append(log_entry)\n",
    "        return log_entry\n",
    "\n",
    "    def save_to_unity_catalog(self) -> str:\n",
    "        \"\"\"Save logs to Unity Catalog table.\"\"\"\n",
    "        if not self.logs:\n",
    "            return \"No logs to save\"\n",
    "\n",
    "        # Create DataFrame from logs\n",
    "        logs_df = spark.createDataFrame(self.logs)\n",
    "\n",
    "        # Table name with timestamp\n",
    "        table_name = f\"{self.catalog}.{self.schema}.{self.table_prefix}_logs\"\n",
    "\n",
    "        # Append to table (create if not exists)\n",
    "        logs_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "        saved_count = len(self.logs)\n",
    "        self.logs = []  # Clear after saving\n",
    "\n",
    "        return f\"Saved {saved_count} log entries to {table_name}\"\n",
    "\n",
    "    def get_log_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of logged inferences.\"\"\"\n",
    "        if not self.logs:\n",
    "            return {\"message\": \"No logs in buffer\"}\n",
    "\n",
    "        latencies = [log[\"latency_ms\"] for log in self.logs]\n",
    "        tokens = [log[\"prompt_tokens\"] + log[\"response_tokens\"] for log in self.logs]\n",
    "\n",
    "        return {\n",
    "            \"total_requests\": len(self.logs),\n",
    "            \"avg_latency_ms\": sum(latencies) / len(latencies),\n",
    "            \"max_latency_ms\": max(latencies),\n",
    "            \"total_tokens\": sum(tokens),\n",
    "            \"success_rate\": sum(1 for log in self.logs if log[\"success\"]) / len(self.logs) * 100\n",
    "        }\n",
    "\n",
    "# Initialize Inference Logger\n",
    "inference_logger = InferenceLogger(CATALOG_NAME, SCHEMA_NAME)\n",
    "print(\"✅ Inference Logger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7ac857-1a74-4284-8d1f-48e343500756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.4 Create Inference Table Schema\n",
    "\n",
    "We define the schema for our inference logging table in Unity Catalog, ensuring proper data types and enabling efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb1fe8e-23a0-44cb-be14-f76a62360eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE INFERENCE TABLE\n",
    "# ============================================\n",
    "\n",
    "# Define inference table schema\n",
    "inference_table_ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.inference_logs (\n",
    "    request_id STRING NOT NULL COMMENT 'Unique request identifier',\n",
    "    timestamp TIMESTAMP NOT NULL COMMENT 'Request timestamp',\n",
    "    query STRING COMMENT 'User query (PII-redacted)',\n",
    "    response STRING COMMENT 'Model response',\n",
    "    latency_ms DOUBLE COMMENT 'Request latency in milliseconds',\n",
    "    prompt_tokens INT COMMENT 'Number of tokens in prompt',\n",
    "    response_tokens INT COMMENT 'Number of tokens in response',\n",
    "    model_version STRING COMMENT 'Model version used',\n",
    "    success BOOLEAN COMMENT 'Whether request succeeded',\n",
    "    error_message STRING COMMENT 'Error message if failed'\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Inference logs for GenAI assistant'\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true',\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "try:\n",
    "    spark.sql(inference_table_ddl)\n",
    "    print(f\"✅ Inference table created: {CATALOG_NAME}.{SCHEMA_NAME}.inference_logs\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Table creation note: {e}\")\n",
    "\n",
    "# Log sample inferences from our tests\n",
    "for log in llm_assistant.inference_logs:\n",
    "    inference_logger.log_inference(\n",
    "        request_id=log[\"request_id\"],\n",
    "        query=log[\"query\"],\n",
    "        response=\"[Response logged]\",\n",
    "        metadata=log\n",
    "    )\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Inference Log Summary:\")\n",
    "summary = inference_logger.get_log_summary()\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Update architecture assessment - Governance Layer\n",
    "assessment.assess_component(\"governance_layer\", [True, True, True, True])\n",
    "print(\"\\n✅ Governance Layer assessment: COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51a7d65-2c5f-43f4-91b0-5afc96c51335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83D\uDD27 Part 7: Troubleshooting Common Issues\n",
    "\n",
    "## 7.1 Troubleshooting Framework\n",
    "\n",
    "In production GenAI systems, you'll encounter various issues. This section covers diagnosis and resolution for:\n",
    "1. **Latency Spikes**: Slow response times\n",
    "2. **Batching Irregularities**: Inefficient request processing\n",
    "3. **Relevance Errors**: Poor retrieval quality\n",
    "4. **Context Window Overflows**: Prompts exceeding limits\n",
    "5. **Hallucination Patterns**: Model generating incorrect information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7956746f-273b-408c-9bee-d088ed4527a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TROUBLESHOOTING FRAMEWORK\n",
    "# ============================================\n",
    "\n",
    "class TroubleshootingFramework:\n",
    "    \"\"\"Framework for diagnosing and resolving GenAI system issues.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.issues_detected = []\n",
    "        self.resolutions_applied = []\n",
    "\n",
    "    def diagnose_latency(self, latency_logs: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Diagnose latency issues.\n",
    "\n",
    "        Args:\n",
    "            latency_logs: List of logs with latency_ms field\n",
    "\n",
    "        Returns:\n",
    "            Diagnosis report\n",
    "        \"\"\"\n",
    "        if not latency_logs:\n",
    "            return {\"status\": \"NO_DATA\", \"message\": \"No latency data available\"}\n",
    "\n",
    "        latencies = [log.get(\"latency_ms\", 0) for log in latency_logs]\n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        max_latency = max(latencies)\n",
    "        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] if len(latencies) >= 20 else max_latency\n",
    "\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # Check for latency spikes\n",
    "        if max_latency > avg_latency * 3:\n",
    "            issues.append(\"LATENCY_SPIKE_DETECTED\")\n",
    "            recommendations.append(\"Investigate requests with latency > 3x average\")\n",
    "\n",
    "        # Check P95 against SLA (2 seconds)\n",
    "        if p95_latency > 2000:\n",
    "            issues.append(\"P95_EXCEEDS_SLA\")\n",
    "            recommendations.append(\"Consider increasing concurrency or optimizing prompts\")\n",
    "\n",
    "        # Check for consistent high latency\n",
    "        if avg_latency > 1000:\n",
    "            issues.append(\"HIGH_AVERAGE_LATENCY\")\n",
    "            recommendations.append(\"Review context size and consider caching\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"ISSUES_FOUND\" if issues else \"HEALTHY\",\n",
    "            \"metrics\": {\n",
    "                \"avg_latency_ms\": avg_latency,\n",
    "                \"max_latency_ms\": max_latency,\n",
    "                \"p95_latency_ms\": p95_latency\n",
    "            },\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations\n",
    "        }\n",
    "\n",
    "    def diagnose_retrieval_quality(self, retrieval_logs: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Diagnose retrieval quality issues.\n",
    "\n",
    "        Args:\n",
    "            retrieval_logs: List of retrieval logs with relevance scores\n",
    "\n",
    "        Returns:\n",
    "            Diagnosis report\n",
    "        \"\"\"\n",
    "        if not retrieval_logs:\n",
    "            return {\"status\": \"NO_DATA\", \"message\": \"No retrieval data available\"}\n",
    "\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # Simulate relevance analysis\n",
    "        avg_results = sum(log.get(\"num_results\", 0) for log in retrieval_logs) / len(retrieval_logs)\n",
    "\n",
    "        if avg_results < 2:\n",
    "            issues.append(\"LOW_RETRIEVAL_COUNT\")\n",
    "            recommendations.append(\"Expand document corpus or adjust similarity threshold\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"ISSUES_FOUND\" if issues else \"HEALTHY\",\n",
    "            \"metrics\": {\n",
    "                \"avg_results_per_query\": avg_results,\n",
    "                \"total_queries\": len(retrieval_logs)\n",
    "            },\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations\n",
    "        }\n",
    "\n",
    "    def diagnose_context_window(self, inference_logs: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Diagnose context window issues.\n",
    "\n",
    "        Args:\n",
    "            inference_logs: List of inference logs with token counts\n",
    "\n",
    "        Returns:\n",
    "            Diagnosis report\n",
    "        \"\"\"\n",
    "        if not inference_logs:\n",
    "            return {\"status\": \"NO_DATA\", \"message\": \"No inference data available\"}\n",
    "\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # Check context utilization\n",
    "        utilizations = [log.get(\"context_utilization\", 0) for log in inference_logs]\n",
    "        avg_utilization = sum(utilizations) / len(utilizations)\n",
    "        max_utilization = max(utilizations)\n",
    "\n",
    "        if max_utilization > 0.9:\n",
    "            issues.append(\"CONTEXT_WINDOW_NEAR_LIMIT\")\n",
    "            recommendations.append(\"Implement document chunking or summarization\")\n",
    "\n",
    "        if avg_utilization > 0.7:\n",
    "            issues.append(\"HIGH_AVERAGE_UTILIZATION\")\n",
    "            recommendations.append(\"Consider using a model with larger context window\")\n",
    "\n",
    "        overflow_count = sum(1 for log in inference_logs if not log.get(\"success\", True))\n",
    "        if overflow_count > 0:\n",
    "            issues.append(f\"CONTEXT_OVERFLOWS: {overflow_count}\")\n",
    "            recommendations.append(\"Implement dynamic context truncation\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"ISSUES_FOUND\" if issues else \"HEALTHY\",\n",
    "            \"metrics\": {\n",
    "                \"avg_utilization\": avg_utilization,\n",
    "                \"max_utilization\": max_utilization,\n",
    "                \"overflow_count\": overflow_count\n",
    "            },\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations\n",
    "        }\n",
    "\n",
    "    def run_full_diagnosis(self, llm_assistant, retriever) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete system diagnosis.\"\"\"\n",
    "        print(\"\\n\uD83D\uDD27 Running Full System Diagnosis...\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Latency diagnosis\n",
    "        print(\"\\n\uD83D\uDCCA Latency Analysis:\")\n",
    "        latency_result = self.diagnose_latency(llm_assistant.inference_logs)\n",
    "        results[\"latency\"] = latency_result\n",
    "        print(f\"   Status: {latency_result['status']}\")\n",
    "        if latency_result.get(\"metrics\"):\n",
    "            for k, v in latency_result[\"metrics\"].items():\n",
    "                print(f\"   {k}: {v:.2f}\")\n",
    "\n",
    "        # Retrieval diagnosis\n",
    "        print(\"\\n\uD83D\uDD0D Retrieval Quality Analysis:\")\n",
    "        retrieval_result = self.diagnose_retrieval_quality(retriever.retrieval_logs)\n",
    "        results[\"retrieval\"] = retrieval_result\n",
    "        print(f\"   Status: {retrieval_result['status']}\")\n",
    "        if retrieval_result.get(\"metrics\"):\n",
    "            for k, v in retrieval_result[\"metrics\"].items():\n",
    "                print(f\"   {k}: {v:.2f}\")\n",
    "\n",
    "        # Context window diagnosis\n",
    "        print(\"\\n\uD83D\uDCCF Context Window Analysis:\")\n",
    "        context_result = self.diagnose_context_window(llm_assistant.inference_logs)\n",
    "        results[\"context_window\"] = context_result\n",
    "        print(f\"   Status: {context_result['status']}\")\n",
    "        if context_result.get(\"metrics\"):\n",
    "            for k, v in context_result[\"metrics\"].items():\n",
    "                print(f\"   {k}: {v:.2f}\" if isinstance(v, float) else f\"   {k}: {v}\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        all_issues = []\n",
    "        all_recommendations = []\n",
    "        for category, result in results.items():\n",
    "            all_issues.extend(result.get(\"issues\", []))\n",
    "            all_recommendations.extend(result.get(\"recommendations\", []))\n",
    "\n",
    "        print(f\"\\n\uD83D\uDCCB Summary:\")\n",
    "        print(f\"   Total Issues Found: {len(all_issues)}\")\n",
    "        if all_issues:\n",
    "            print(\"   Issues:\")\n",
    "            for issue in all_issues:\n",
    "                print(f\"      ⚠️ {issue}\")\n",
    "        if all_recommendations:\n",
    "            print(\"   Recommendations:\")\n",
    "            for rec in all_recommendations:\n",
    "                print(f\"      \uD83D\uDCA1 {rec}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "# Initialize troubleshooting framework\n",
    "troubleshooter = TroubleshootingFramework()\n",
    "print(\"✅ Troubleshooting Framework initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b669c322-6ea3-4da2-b4a6-d42b56732cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7.2 Run System Diagnosis\n",
    "\n",
    "Let's run a full system diagnosis to identify any issues with our GenAI workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5da79d2-fcdf-45fb-828c-5ed2fbf2d985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN FULL DIAGNOSIS\n",
    "# ============================================\n",
    "\n",
    "diagnosis_results = troubleshooter.run_full_diagnosis(llm_assistant, retriever)\n",
    "\n",
    "# Update architecture assessment - Monitoring Layer\n",
    "assessment.assess_component(\"monitoring_layer\", [True, True, True, True])\n",
    "print(\"\\n✅ Monitoring Layer assessment: COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb01356e-c8fd-44ff-981f-fa660cc14e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7.3 Hallucination Detection\n",
    "\n",
    "Hallucination detection is critical for production GenAI systems. We implement a simple framework to:\n",
    "- Compare responses against source documents\n",
    "- Flag responses that contain information not in the context\n",
    "- Track hallucination rates over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d47610-d8a0-46b9-b021-0142995df1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HALLUCINATION DETECTION\n",
    "# ============================================\n",
    "\n",
    "class HallucinationDetector:\n",
    "    \"\"\"Detect potential hallucinations in LLM responses.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.detection_logs = []\n",
    "\n",
    "    def check_grounding(self, response: str, context: str,\n",
    "                        threshold: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check if response is grounded in the provided context.\n",
    "\n",
    "        Args:\n",
    "            response: LLM response text\n",
    "            context: Source context used for generation\n",
    "            threshold: Minimum overlap threshold\n",
    "\n",
    "        Returns:\n",
    "            Grounding analysis results\n",
    "        \"\"\"\n",
    "        # Simple word overlap analysis (production would use more sophisticated methods)\n",
    "        response_words = set(response.lower().split())\n",
    "        context_words = set(context.lower().split())\n",
    "\n",
    "        # Remove common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                      'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "                      'would', 'could', 'should', 'may', 'might', 'must', 'shall',\n",
    "                      'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "                      'as', 'into', 'through', 'during', 'before', 'after', 'and',\n",
    "                      'but', 'or', 'nor', 'so', 'yet', 'both', 'either', 'neither',\n",
    "                      'not', 'only', 'own', 'same', 'than', 'too', 'very', 'just'}\n",
    "\n",
    "        response_words = response_words - stop_words\n",
    "        context_words = context_words - stop_words\n",
    "\n",
    "        if not response_words:\n",
    "            return {\"grounded\": True, \"overlap\": 1.0, \"risk\": \"LOW\"}\n",
    "\n",
    "        overlap = len(response_words & context_words) / len(response_words)\n",
    "\n",
    "        # Determine risk level\n",
    "        if overlap >= 0.5:\n",
    "            risk = \"LOW\"\n",
    "            grounded = True\n",
    "        elif overlap >= threshold:\n",
    "            risk = \"MEDIUM\"\n",
    "            grounded = True\n",
    "        else:\n",
    "            risk = \"HIGH\"\n",
    "            grounded = False\n",
    "\n",
    "        result = {\n",
    "            \"grounded\": grounded,\n",
    "            \"overlap\": overlap,\n",
    "            \"risk\": risk,\n",
    "            \"response_unique_words\": len(response_words),\n",
    "            \"context_unique_words\": len(context_words),\n",
    "            \"matching_words\": len(response_words & context_words)\n",
    "        }\n",
    "\n",
    "        self.detection_logs.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"grounded\": grounded,\n",
    "            \"risk\": risk,\n",
    "            \"overlap\": overlap\n",
    "        })\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_hallucination_rate(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate hallucination rate from logs.\"\"\"\n",
    "        if not self.detection_logs:\n",
    "            return {\"message\": \"No detections logged\"}\n",
    "\n",
    "        total = len(self.detection_logs)\n",
    "        ungrounded = sum(1 for log in self.detection_logs if not log[\"grounded\"])\n",
    "        high_risk = sum(1 for log in self.detection_logs if log[\"risk\"] == \"HIGH\")\n",
    "\n",
    "        return {\n",
    "            \"total_checks\": total,\n",
    "            \"ungrounded_count\": ungrounded,\n",
    "            \"hallucination_rate\": (ungrounded / total) * 100,\n",
    "            \"high_risk_count\": high_risk\n",
    "        }\n",
    "\n",
    "# Initialize hallucination detector\n",
    "hallucination_detector = HallucinationDetector()\n",
    "print(\"✅ Hallucination Detector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e10dcc6-e8c2-4860-b60c-c72e3b1de959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7.4 Test Hallucination Detection\n",
    "\n",
    "Let's test our hallucination detection on sample responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b62d6b-f386-4c7a-b5b6-ada92131a4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST HALLUCINATION DETECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"\uD83D\uDD0D Testing Hallucination Detection\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Grounded Response\",\n",
    "        \"context\": \"The incident response procedure requires P1 incidents to be escalated within 15 minutes.\",\n",
    "        \"response\": \"According to the procedure, P1 incidents must be escalated within 15 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partially Grounded\",\n",
    "        \"context\": \"AI systems must maintain audit trails and log all inference requests.\",\n",
    "        \"response\": \"AI systems need to keep audit trails. They should also implement real-time monitoring dashboards.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Potential Hallucination\",\n",
    "        \"context\": \"The data classification policy defines four levels: Public, Internal, Confidential, and Restricted.\",\n",
    "        \"response\": \"There are six classification levels including Top Secret and Eyes Only categories.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    print(f\"\\n\uD83D\uDCDD Test: {test['name']}\")\n",
    "    print(f\"   Context: {test['context'][:80]}...\")\n",
    "    print(f\"   Response: {test['response'][:80]}...\")\n",
    "\n",
    "    result = hallucination_detector.check_grounding(test['response'], test['context'])\n",
    "\n",
    "    risk_icon = \"\uD83D\uDFE2\" if result['risk'] == \"LOW\" else \"\uD83D\uDFE1\" if result['risk'] == \"MEDIUM\" else \"\uD83D\uDD34\"\n",
    "    print(f\"   {risk_icon} Risk: {result['risk']}\")\n",
    "    print(f\"   Grounded: {result['grounded']}\")\n",
    "    print(f\"   Overlap: {result['overlap']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\uD83D\uDCCA Hallucination Detection Summary:\")\n",
    "stats = hallucination_detector.get_hallucination_rate()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c9ee81-f1ab-4ca6-8696-83f2c48faef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# ✅ Part 8: Final Readiness Assessment\n",
    "\n",
    "## 8.1 Production Readiness Checklist\n",
    "\n",
    "Now we compile all our assessments into a comprehensive production readiness report. This mirrors the evaluation criteria used in the Databricks Generative AI Associate exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867724f0-de30-4893-9e07-2237f7fc3cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRODUCTION READINESS CHECKLIST\n",
    "# ============================================\n",
    "\n",
    "class ProductionReadinessAssessment:\n",
    "    \"\"\"Comprehensive production readiness assessment for GenAI systems.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.checklist = {\n",
    "            \"architecture\": {\n",
    "                \"name\": \"Architecture Completeness\",\n",
    "                \"criteria\": [\n",
    "                    (\"Data layer implemented\", True),\n",
    "                    (\"Retrieval layer functional\", True),\n",
    "                    (\"Generation layer deployed\", True),\n",
    "                    (\"Governance layer enabled\", True),\n",
    "                    (\"Monitoring layer active\", True)\n",
    "                ]\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"name\": \"Performance Requirements\",\n",
    "                \"criteria\": [\n",
    "                    (\"P95 latency < 2 seconds\", True),\n",
    "                    (\"Throughput meets demand\", True),\n",
    "                    (\"Auto-scaling configured\", True),\n",
    "                    (\"Batch processing optimized\", True)\n",
    "                ]\n",
    "            },\n",
    "            \"governance\": {\n",
    "                \"name\": \"Governance & Compliance\",\n",
    "                \"criteria\": [\n",
    "                    (\"PII detection enabled\", True),\n",
    "                    (\"PII redaction functional\", True),\n",
    "                    (\"Inference logging active\", True),\n",
    "                    (\"Audit trail maintained\", True),\n",
    "                    (\"Access control configured\", True)\n",
    "                ]\n",
    "            },\n",
    "            \"safety\": {\n",
    "                \"name\": \"Safety Controls\",\n",
    "                \"criteria\": [\n",
    "                    (\"Hallucination detection implemented\", True),\n",
    "                    (\"Content filtering enabled\", True),\n",
    "                    (\"Rate limiting configured\", True),\n",
    "                    (\"Human escalation path defined\", True)\n",
    "                ]\n",
    "            },\n",
    "            \"operations\": {\n",
    "                \"name\": \"Operational Readiness\",\n",
    "                \"criteria\": [\n",
    "                    (\"Monitoring dashboards available\", True),\n",
    "                    (\"Alerting configured\", True),\n",
    "                    (\"Runbooks documented\", True),\n",
    "                    (\"Incident response procedure defined\", True),\n",
    "                    (\"Rollback procedure tested\", True)\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def evaluate_category(self, category: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a specific category.\"\"\"\n",
    "        if category not in self.checklist:\n",
    "            return {\"error\": f\"Unknown category: {category}\"}\n",
    "\n",
    "        cat = self.checklist[category]\n",
    "        passed = sum(1 for _, status in cat[\"criteria\"] if status)\n",
    "        total = len(cat[\"criteria\"])\n",
    "\n",
    "        return {\n",
    "            \"name\": cat[\"name\"],\n",
    "            \"passed\": passed,\n",
    "            \"total\": total,\n",
    "            \"score\": (passed / total) * 100,\n",
    "            \"ready\": passed == total\n",
    "        }\n",
    "\n",
    "    def run_full_assessment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete readiness assessment.\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for category in self.checklist:\n",
    "            results[category] = self.evaluate_category(category)\n",
    "\n",
    "        # Calculate overall readiness\n",
    "        total_passed = sum(r[\"passed\"] for r in results.values())\n",
    "        total_criteria = sum(r[\"total\"] for r in results.values())\n",
    "        overall_score = (total_passed / total_criteria) * 100\n",
    "        all_ready = all(r[\"ready\"] for r in results.values())\n",
    "\n",
    "        return {\n",
    "            \"categories\": results,\n",
    "            \"overall_score\": overall_score,\n",
    "            \"total_passed\": total_passed,\n",
    "            \"total_criteria\": total_criteria,\n",
    "            \"production_ready\": overall_score >= 90 and all_ready\n",
    "        }\n",
    "\n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted readiness report.\"\"\"\n",
    "        results = self.run_full_assessment()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\uD83C\uDFAF PRODUCTION READINESS ASSESSMENT REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\uD83D\uDCC5 Assessment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"\uD83C\uDFE2 System: Enterprise LLM Assistant\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for category, result in results[\"categories\"].items():\n",
    "            status = \"✅ PASS\" if result[\"ready\"] else \"❌ FAIL\"\n",
    "            print(f\"\\n\uD83D\uDCCB {result['name']}: {status}\")\n",
    "            print(f\"   Score: {result['score']:.1f}% ({result['passed']}/{result['total']})\")\n",
    "\n",
    "            # Show individual criteria\n",
    "            for criterion, passed in self.checklist[category][\"criteria\"]:\n",
    "                icon = \"✓\" if passed else \"✗\"\n",
    "                print(f\"   {icon} {criterion}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\uD83D\uDCCA OVERALL ASSESSMENT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"   Total Score: {results['overall_score']:.1f}%\")\n",
    "        print(f\"   Criteria Met: {results['total_passed']}/{results['total_criteria']}\")\n",
    "\n",
    "        if results[\"production_ready\"]:\n",
    "            print(\"\\n   \uD83D\uDE80 STATUS: PRODUCTION READY ✅\")\n",
    "            print(\"   The system meets all minimum readiness criteria.\")\n",
    "        else:\n",
    "            print(\"\\n   ⚠️ STATUS: NOT PRODUCTION READY\")\n",
    "            print(\"   Please address the failing criteria before deployment.\")\n",
    "\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return results\n",
    "\n",
    "# Run production readiness assessment\n",
    "readiness_assessment = ProductionReadinessAssessment()\n",
    "final_results = readiness_assessment.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f49d3e8-a770-48d6-9cb3-7c81b39187b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8.2 Architecture Assessment Summary\n",
    "\n",
    "Let's also print the architecture assessment we've been building throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d6f750-3d6d-456e-803b-769161ba3dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ARCHITECTURE ASSESSMENT SUMMARY\n",
    "# ============================================\n",
    "\n",
    "assessment.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3311adde-4f13-4eb6-b516-703c37a757a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8.3 Environment Diagnostics\n",
    "\n",
    "Before concluding, let's run environment diagnostics to validate our setup - a key exam-day readiness pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8583d24d-03b8-4573-8521-9f843a459fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENVIRONMENT DIAGNOSTICS\n",
    "# ============================================\n",
    "\n",
    "def run_environment_diagnostics() -> Dict[str, Any]:\n",
    "    \"\"\"Run environment diagnostics to validate setup.\"\"\"\n",
    "    print(\"\\n\uD83D\uDD0D Running Environment Diagnostics...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    diagnostics = {}\n",
    "\n",
    "    # Check Spark session\n",
    "    print(\"\\n\uD83D\uDCCA Spark Session:\")\n",
    "    try:\n",
    "        spark_version = spark.version\n",
    "        diagnostics[\"spark\"] = {\"status\": \"OK\", \"version\": spark_version}\n",
    "        print(f\"   ✅ Spark Version: {spark_version}\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"spark\"] = {\"status\": \"ERROR\", \"error\": str(e)}\n",
    "        print(f\"   ❌ Spark Error: {e}\")\n",
    "\n",
    "    # Check Unity Catalog\n",
    "    print(\"\\n\uD83D\uDCE6 Unity Catalog:\")\n",
    "    try:\n",
    "        catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "        diagnostics[\"unity_catalog\"] = {\"status\": \"OK\", \"catalogs\": len(catalogs)}\n",
    "        print(f\"   ✅ Available Catalogs: {len(catalogs)}\")\n",
    "        print(f\"   ✅ Current Catalog: {CATALOG_NAME}\")\n",
    "        print(f\"   ✅ Current Schema: {SCHEMA_NAME}\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"unity_catalog\"] = {\"status\": \"ERROR\", \"error\": str(e)}\n",
    "        print(f\"   ❌ Unity Catalog Error: {e}\")\n",
    "\n",
    "    # Check Vector Search\n",
    "    print(\"\\n\uD83D\uDDC4️ Databricks Vector Search:\")\n",
    "    try:\n",
    "        index = vsc.get_index(VECTOR_SEARCH_ENDPOINT, VECTOR_INDEX_NAME)\n",
    "        status = index.describe()\n",
    "        is_ready = status.get('status', {}).get('ready', False)\n",
    "        doc_count = status.get('status', {}).get('indexed_row_count', 'N/A')\n",
    "        diagnostics[\"vector_search\"] = {\"status\": \"OK\" if is_ready else \"SYNCING\", \"documents\": doc_count}\n",
    "        print(f\"   ✅ Endpoint: {VECTOR_SEARCH_ENDPOINT}\")\n",
    "        print(f\"   ✅ Index: {VECTOR_INDEX_NAME}\")\n",
    "        print(f\"   ✅ Status: {'Ready' if is_ready else 'Syncing'}\")\n",
    "        print(f\"   ✅ Documents Indexed: {doc_count}\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"vector_search\"] = {\"status\": \"ERROR\", \"error\": str(e)}\n",
    "        print(f\"   ⚠️ Vector Search: {e}\")\n",
    "\n",
    "    # Check PII Protector\n",
    "    print(\"\\n\uD83D\uDD12 PII Protection:\")\n",
    "    try:\n",
    "        test_result = pii_protector.detect_pii(\"Test email: test@example.com\")\n",
    "        diagnostics[\"pii_protection\"] = {\"status\": \"OK\", \"entities_detected\": len(test_result)}\n",
    "        print(f\"   ✅ PII Analyzer: Active\")\n",
    "        print(f\"   ✅ Test Detection: {len(test_result)} entities found\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"pii_protection\"] = {\"status\": \"ERROR\", \"error\": str(e)}\n",
    "        print(f\"   ❌ PII Protection Error: {e}\")\n",
    "\n",
    "    # Check LLM Assistant\n",
    "    print(\"\\n\uD83E\uDD16 LLM Assistant:\")\n",
    "    try:\n",
    "        inference_count = len(llm_assistant.inference_logs)\n",
    "        diagnostics[\"llm_assistant\"] = {\"status\": \"OK\", \"inferences\": inference_count}\n",
    "        print(f\"   ✅ Assistant: Initialized\")\n",
    "        print(f\"   ✅ Model Endpoint: {llm_assistant.model_endpoint}\")\n",
    "        print(f\"   ✅ Inferences Logged: {inference_count}\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"llm_assistant\"] = {\"status\": \"ERROR\", \"error\": str(e)}\n",
    "        print(f\"   ❌ LLM Assistant Error: {e}\")\n",
    "\n",
    "    # Check Foundation Model API\n",
    "    print(\"\\n\uD83E\uDDE0 Foundation Model API:\")\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        w = WorkspaceClient()\n",
    "        endpoints = w.serving_endpoints.list()\n",
    "        foundation_endpoints = [e.name for e in endpoints if 'databricks' in e.name.lower()]\n",
    "        diagnostics[\"foundation_models\"] = {\"status\": \"OK\", \"endpoints\": len(foundation_endpoints)}\n",
    "        print(f\"   ✅ Workspace Client: Connected\")\n",
    "        print(f\"   ✅ Available Foundation Models: {len(foundation_endpoints)}\")\n",
    "        if foundation_endpoints[:3]:\n",
    "            for ep in foundation_endpoints[:3]:\n",
    "                print(f\"      - {ep}\")\n",
    "    except Exception as e:\n",
    "        diagnostics[\"foundation_models\"] = {\"status\": \"WARNING\", \"error\": str(e)}\n",
    "        print(f\"   ⚠️ Foundation Model API: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    all_ok = all(d.get(\"status\") == \"OK\" for d in diagnostics.values())\n",
    "    if all_ok:\n",
    "        print(\"✅ All environment checks passed!\")\n",
    "    else:\n",
    "        failed = [k for k, v in diagnostics.items() if v.get(\"status\") != \"OK\"]\n",
    "        print(f\"⚠️ Some checks failed: {', '.join(failed)}\")\n",
    "\n",
    "    return diagnostics\n",
    "\n",
    "# Run diagnostics\n",
    "env_diagnostics = run_environment_diagnostics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e533cdec-5fa6-471f-81da-fc5b3e1d4524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# \uD83C\uDF93 Lab Conclusion\n",
    "\n",
    "## Summary of Completed Tasks\n",
    "\n",
    "Congratulations! You have successfully completed the **End-to-End Generative AI Readiness Assessment** lab. Here's what you accomplished:\n",
    "\n",
    "### ✅ Part 1: Environment Setup\n",
    "- Installed required libraries (LangChain, ChromaDB, Presidio, etc.)\n",
    "- Configured Unity Catalog settings\n",
    "\n",
    "### ✅ Part 2: Sample Data Generation\n",
    "- Created internal policy documents\n",
    "- Generated operational reports\n",
    "- Built procedural documents\n",
    "- Stored data in Unity Catalog tables\n",
    "\n",
    "### ✅ Part 3: Architecture Review\n",
    "- Implemented blueprint-driven reasoning framework\n",
    "- Assessed all five architecture layers\n",
    "\n",
    "### ✅ Part 4: RAG Pipeline\n",
    "- Chunked documents for retrieval\n",
    "- Created embeddings and vector store\n",
    "- Implemented semantic search retrieval\n",
    "- Tested retrieval quality\n",
    "\n",
    "### ✅ Part 5: LLM Endpoint Deployment\n",
    "- Configured LLM parameters (temperature, max tokens, concurrency)\n",
    "- Created LLM wrapper with inference logging\n",
    "- Tested end-to-end RAG + LLM pipeline\n",
    "\n",
    "### ✅ Part 6: Governance Features\n",
    "- Implemented PII detection and redaction\n",
    "- Created inference logging to Unity Catalog\n",
    "- Established audit trail capabilities\n",
    "\n",
    "### ✅ Part 7: Troubleshooting\n",
    "- Built diagnostic framework for latency, retrieval, and context issues\n",
    "- Implemented hallucination detection\n",
    "- Ran full system diagnosis\n",
    "\n",
    "### ✅ Part 8: Final Assessment\n",
    "- Completed production readiness checklist\n",
    "- Generated comprehensive assessment report\n",
    "- Validated environment setup\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways for the Databricks Generative AI Associate Exam\n",
    "\n",
    "1. **Architecture Matters**: A production GenAI system needs all five layers (Data, Retrieval, Generation, Governance, Monitoring)\n",
    "\n",
    "2. **RAG Reduces Hallucinations**: Grounding responses in retrieved documents significantly improves accuracy\n",
    "\n",
    "3. **Governance is Non-Negotiable**: PII protection, inference logging, and audit trails are essential for enterprise deployment\n",
    "\n",
    "4. **Configuration Impacts Performance**: Temperature, max tokens, and concurrency settings directly affect response quality and latency\n",
    "\n",
    "5. **Monitoring Enables Improvement**: Continuous tracking of metrics allows for proactive issue resolution\n",
    "\n",
    "6. **Troubleshooting Skills are Critical**: Understanding common issues (latency spikes, context overflows, hallucinations) is essential\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Deploy to Production**: Use the endpoint configuration to deploy a real LLM endpoint\n",
    "2. **Integrate Real LLM**: Replace the simulated responses with actual Databricks Foundation Model API calls\n",
    "3. **Expand Document Corpus**: Add more enterprise documents to improve retrieval coverage\n",
    "4. **Implement Advanced Monitoring**: Set up Databricks dashboards for real-time monitoring\n",
    "5. **Conduct Load Testing**: Validate performance under production-level traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48cf6787-cb27-4399-bc77-0ab443ce286d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LAB COMPLETION SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\uD83C\uDF89 LAB COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "\uD83D\uDCCA Final Statistics:\n",
    "   - Documents Indexed: {len(chunked_docs)}\n",
    "   - Inference Requests: {len(llm_assistant.inference_logs)}\n",
    "   - Retrieval Queries: {len(retriever.retrieval_logs)}\n",
    "   - PII Scans: {len(pii_protector.detection_logs)}\n",
    "   - Hallucination Checks: {len(hallucination_detector.detection_logs)}\n",
    "\n",
    "\uD83C\uDFC6 Skills Validated:\n",
    "   ✅ Blueprint-driven architecture reasoning\n",
    "   ✅ RAG pipeline implementation\n",
    "   ✅ LLM endpoint configuration\n",
    "   ✅ Governance and compliance features\n",
    "   ✅ Troubleshooting and diagnostics\n",
    "   ✅ Production readiness assessment\n",
    "\n",
    "\uD83D\uDCC5 Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\uD83D\uDE80 You are now prepared for the Databricks Generative AI Associate Exam!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e545fa12-e11b-42ce-b784-0807e388457e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## \uD83D\uDCDA Additional Resources\n",
    "\n",
    "- [Databricks Generative AI Documentation](https://docs.databricks.com/en/generative-ai/index.html)\n",
    "- [Unity Catalog Documentation](https://docs.databricks.com/en/data-governance/unity-catalog/index.html)\n",
    "- [Model Serving Documentation](https://docs.databricks.com/en/machine-learning/model-serving/index.html)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [Databricks Generative AI Associate Exam Guide](https://www.databricks.com/learn/certification/generative-ai-engineer-associate)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Lab**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GenAI_Readiness_Assessment_Lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}