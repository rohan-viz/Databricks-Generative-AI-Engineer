{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3282519f-5457-499f-822b-1634939172a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Evaluating and Monitoring LLM Performance\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are an **LLM operations engineer** responsible for managing a production question-answering system used by internal employees across your organization. The system relies on a large language model deployed through **Databricks Model Serving**. In recent weeks, users have reported:\n",
    "\n",
    "- **Inconsistent answer quality**\n",
    "- **Occasional latency spikes**\n",
    "- **Rising usage costs**\n",
    "\n",
    "Leadership has asked you to diagnose these issues, evaluate system performance, and implement improvements using Databricks monitoring and evaluation tools.\n",
    "\n",
    "### Your Workflow in This Lab\n",
    "\n",
    "1. **Running structured evaluations** using MLflow to measure latency, token usage, and groundedness\n",
    "2. **Querying inference tables** to identify patterns in real-world traffic, including token growth and slow prompts\n",
    "3. **Using monitoring dashboards and agent traces** to investigate multi-step workflows\n",
    "4. **Detecting anomalies and configuring alert conditions** for early issue detection\n",
    "5. **Applying cost-optimization techniques** such as prompt refinement and context control\n",
    "6. **Validating improvements** with repeatable evaluation runs\n",
    "\n",
    "This lab mirrors real-world LLM observability and optimization challenges, where reliable performance and controlled operational cost are essential to business continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- ✅ Design an end-to-end evaluation and monitoring workflow on Databricks\n",
    "- ✅ Apply MLflow metrics to assess LLM quality and performance\n",
    "- ✅ Use inference tables to diagnose latency and cost issues\n",
    "- ✅ Monitor multi-step agent workflows to identify inefficiencies\n",
    "- ✅ Configure anomaly alerts to detect unusual production behavior\n",
    "- ✅ Apply optimization strategies to improve reliability and reduce cost\n",
    "- ✅ Incorporate Chapter 8 best practices including calibrated metric interpretation, baseline-aware monitoring, structured alerting strategies, and periodic review cycles\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Duration\n",
    "\n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ef0c96-3c4a-4610-b89e-2230ae3ef297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 1: Prerequisites and Environment Setup\n",
    "\n",
    "In this section, we will:\n",
    "1. Install required libraries\n",
    "2. Configure connection to Databricks workspace\n",
    "3. Set up MLflow tracking\n",
    "4. Generate synthetic Q&A data for our evaluation scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbf2ee7-ebdc-4286-81f5-d838fd82eb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.1: Install Required Libraries\n",
    "\n",
    "We need to install the necessary Python packages for working with Databricks, MLflow, and LLM evaluation. These packages provide the foundation for our monitoring and evaluation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ada4793-4745-43f1-bebf-d717d99752ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages for the lab\n",
    "# - databricks-sdk: Official Databricks SDK for Python\n",
    "# - mlflow: MLflow for experiment tracking and model evaluation\n",
    "# - databricks-agents: For agent evaluation capabilities\n",
    "# - openai: OpenAI client for model serving interactions\n",
    "\n",
    "%pip install databricks-sdk mlflow>=2.14.0 databricks-agents openai pandas numpy matplotlib seaborn --quiet\n",
    "\n",
    "# Restart Python to pick up new packages (required in Databricks notebooks)\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e73c520-5c14-4915-a6da-f094a186cb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.2: Configure Databricks Connection\n",
    "\n",
    "We'll set up the connection to our Databricks workspace using the SDK. This configuration enables us to:\n",
    "- Create and manage Model Serving endpoints\n",
    "- Access inference tables\n",
    "- Configure monitoring and alerts\n",
    "\n",
    "---\n",
    "\n",
    "#### \uD83D\uDCCB How to Find Your Workspace URL and ID\n",
    "\n",
    "1. **Workspace URL**: Look at your browser's address bar when logged into Databricks. It follows this format:\n",
    "   ```\n",
    "   https://adb-<workspace-id>.<region>.azuredatabricks.net\n",
    "   ```\n",
    "   For example: `https://adb-3141834805281316.15.azuredatabricks.net`\n",
    "\n",
    "2. **Workspace ID**: The numeric value after `adb-` in your URL (e.g., `3141834805281715`)\n",
    "\n",
    "3. **Alternative Method**:\n",
    "   - Click on your **username** in the top-right corner\n",
    "   - Select **User Settings**\n",
    "   - The Workspace ID is displayed under **Workspace Info**\n",
    "\n",
    "---\n",
    "\n",
    "#### \uD83D\uDD11 How to Generate a Personal Access Token (PAT)\n",
    "\n",
    "1. **Navigate to User Settings**:\n",
    "   - Click on your **username** in the top-right corner of the Databricks workspace\n",
    "   - Select **User Settings** from the dropdown menu\n",
    "\n",
    "2. **Access Developer Settings**:\n",
    "   - In the left sidebar, click on **Developer**\n",
    "   - You'll see the **Access tokens** section\n",
    "\n",
    "3. **Generate New Token**:\n",
    "   - Click the **Manage** button next to Access tokens\n",
    "   - Click **Generate new token**\n",
    "   - Enter a **Comment** (e.g., \"Chapter 8 Lab Token\")\n",
    "   - Set **Lifetime (days)** - leave blank for no expiration, or set a value like `90`\n",
    "   - Click **Generate**\n",
    "\n",
    "4. **Copy and Save the Token**:\n",
    "   - ⚠️ **IMPORTANT**: Copy the token immediately! It will only be shown once.\n",
    "   - The token starts with `dapi` (e.g., `dapixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-2`)\n",
    "   - Store it securely - you'll need it for the configuration below\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDD12 Security Note:** In a production environment, store credentials securely using Databricks Secrets instead of hardcoding tokens in notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224e23a0-c3a5-4324-8ddb-426e1303f42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries for Databricks connection\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "# =============================================================================\n",
    "# DATABRICKS WORKSPACE CONFIGURATION\n",
    "# =============================================================================\n",
    "# These credentials connect us to the Azure Databricks workspace\n",
    "# In production, use Databricks Secrets instead of hardcoding tokens\n",
    "# Please replace your own values in place of xxx based in the instructions provided above\n",
    "DATABRICKS_HOST = \"xxx\"\n",
    "DATABRICKS_TOKEN = \"xxx\"\n",
    "CLUSTER_ID = \"xxx\"\n",
    "\n",
    "# Set environment variables for SDK and MLflow\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "\n",
    "# Initialize the Workspace Client\n",
    "# This client provides access to all Databricks APIs\n",
    "w = WorkspaceClient(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN\n",
    ")\n",
    "\n",
    "# Verify connection by listing current user\n",
    "current_user = w.current_user.me()\n",
    "print(f\"✅ Connected to Databricks as: {current_user.user_name}\")\n",
    "print(f\"\uD83D\uDCCD Workspace: {DATABRICKS_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1a17e4-fa47-4f25-8bab-dd7a18ff8e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.3: Configure MLflow Tracking\n",
    "\n",
    "MLflow is our primary tool for tracking experiments, logging metrics, and evaluating LLM performance. We'll configure it to use the Databricks-hosted MLflow tracking server, which provides:\n",
    "\n",
    "- **Centralized experiment tracking** across team members\n",
    "- **Model versioning** through the Model Registry\n",
    "- **Built-in LLM evaluation metrics** for quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be7883e-bb92-4a29-8e07-34673a81d802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# =============================================================================\n",
    "# MLFLOW CONFIGURATION\n",
    "# =============================================================================\n",
    "# Configure MLflow to use Databricks as the tracking server\n",
    "# This enables centralized experiment management and model registry\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "# Create an experiment for our LLM evaluation work\n",
    "# Experiments group related runs together for easy comparison\n",
    "EXPERIMENT_NAME = \"/Users/\" + current_user.user_name + \"/llm_evaluation_monitoring_lab\"\n",
    "\n",
    "# Create or get the experiment\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"✅ Created new experiment: {EXPERIMENT_NAME}\")\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"✅ Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "# Set the active experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"\uD83D\uDCCA Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cc87df-00f8-4553-8c40-1d4ec7b6c859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.4: Generate Synthetic Q&A Dataset\n",
    "\n",
    "To simulate our production question-answering system, we'll create a synthetic dataset that includes:\n",
    "\n",
    "- **Questions** of varying complexity (simple, moderate, complex)\n",
    "- **Ground truth answers** for evaluation\n",
    "- **Context documents** to test groundedness\n",
    "- **Metadata** for categorization and analysis\n",
    "\n",
    "This dataset will be used throughout the lab to:\n",
    "1. Test our Model Serving endpoint\n",
    "2. Evaluate response quality\n",
    "3. Measure latency and token usage\n",
    "4. Simulate production traffic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bf88bb-2b79-4051-b498-aab556035762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "\n",
    "# =============================================================================\n",
    "# SYNTHETIC DATA GENERATION\n",
    "# =============================================================================\n",
    "# We create realistic Q&A data that mimics an internal knowledge base system\n",
    "# This includes various question types, complexities, and domains\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define question categories and their characteristics\n",
    "CATEGORIES = {\n",
    "    \"hr_policy\": {\n",
    "        \"questions\": [\n",
    "            \"What is the company's remote work policy?\",\n",
    "            \"How many vacation days do employees get per year?\",\n",
    "            \"What is the process for requesting parental leave?\",\n",
    "            \"How do I submit an expense report?\",\n",
    "            \"What are the health insurance options available?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"The company allows employees to work remotely up to 3 days per week. Remote work requests must be approved by the direct manager. Employees must maintain core hours of 10 AM to 3 PM in their local timezone.\",\n",
    "            \"Full-time employees receive 20 vacation days per year, accrued monthly. Unused vacation days can be carried over up to a maximum of 5 days. New employees are eligible for vacation after 90 days.\",\n",
    "            \"Parental leave policy provides 16 weeks of paid leave for primary caregivers and 8 weeks for secondary caregivers. Leave must be requested at least 30 days in advance through the HR portal.\",\n",
    "            \"Expense reports must be submitted within 30 days of the expense date. Use the Concur system to upload receipts and categorize expenses. Manager approval is required for expenses over $100.\",\n",
    "            \"The company offers three health insurance tiers: Basic, Standard, and Premium. All plans include dental and vision coverage. Open enrollment occurs annually in November.\"\n",
    "        ],\n",
    "        \"complexity\": \"simple\"\n",
    "    },\n",
    "    \"technical_docs\": {\n",
    "        \"questions\": [\n",
    "            \"How do I configure the API authentication for our microservices?\",\n",
    "            \"What is the recommended approach for database connection pooling?\",\n",
    "            \"How should I handle error logging in production applications?\",\n",
    "            \"What are the security requirements for storing customer data?\",\n",
    "            \"How do I set up CI/CD pipelines for new projects?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"API authentication uses OAuth 2.0 with JWT tokens. Services must register with the central auth server and obtain client credentials. Token expiration is set to 1 hour with refresh token support.\",\n",
    "            \"Database connection pooling should use HikariCP with a minimum pool size of 5 and maximum of 20 connections. Connection timeout is set to 30 seconds. Enable connection validation on borrow.\",\n",
    "            \"Production logging must use structured JSON format with correlation IDs. Log levels: ERROR for exceptions, WARN for recoverable issues, INFO for business events. Use ELK stack for aggregation.\",\n",
    "            \"Customer data must be encrypted at rest using AES-256 and in transit using TLS 1.3. PII requires additional masking in logs. Data retention policy is 7 years for financial data.\",\n",
    "            \"CI/CD pipelines use GitHub Actions with standardized templates. All projects must include unit tests (80% coverage), security scanning, and automated deployment to staging before production.\"\n",
    "        ],\n",
    "        \"complexity\": \"moderate\"\n",
    "    },\n",
    "    \"strategic_planning\": {\n",
    "        \"questions\": [\n",
    "            \"What is our company's five-year growth strategy and how does it align with market trends?\",\n",
    "            \"How should we approach the integration of AI capabilities across all product lines?\",\n",
    "            \"What are the key risk factors in our current market expansion plan?\",\n",
    "            \"How do we balance innovation investment with maintaining core business profitability?\",\n",
    "            \"What organizational changes are needed to support our digital transformation initiative?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"The five-year strategy focuses on three pillars: market expansion into APAC region, product diversification through AI integration, and operational efficiency through automation. Target growth is 25% CAGR.\",\n",
    "            \"AI integration roadmap includes: Phase 1 - customer service automation, Phase 2 - predictive analytics for sales, Phase 3 - AI-powered product features. Budget allocation is $50M over 3 years.\",\n",
    "            \"Key risks include: regulatory changes in target markets, currency fluctuation exposure, talent acquisition challenges, and competitive pressure from well-funded startups. Mitigation strategies are documented.\",\n",
    "            \"Innovation budget is set at 15% of revenue with quarterly review cycles. Core business must maintain 20% profit margin. New ventures have 18-month runway to demonstrate product-market fit.\",\n",
    "            \"Digital transformation requires: flattening organizational hierarchy, creating cross-functional pods, investing in employee upskilling, and establishing a dedicated transformation office with C-suite sponsorship.\"\n",
    "        ],\n",
    "        \"complexity\": \"complex\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Question categories defined\")\n",
    "print(f\"\uD83D\uDCCB Categories: {list(CATEGORIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdd6915-25df-4e8c-a587-33f9aef108d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating the Evaluation Dataset\n",
    "\n",
    "**What we're doing:** Building a structured evaluation dataset that pairs each question with its \"ground truth\" (correct) answer. This is essential for measuring how well the LLM performs.\n",
    "\n",
    "**How it works:**\n",
    "1. **`generate_ground_truth_answer()` function**: Extracts the expected answer from the context based on complexity:\n",
    "   - *Simple questions*: First sentence of context is the answer\n",
    "   - *Moderate questions*: First two sentences combined\n",
    "   - *Complex questions*: Full context needed for complete answer\n",
    "\n",
    "2. **Dataset structure**: Each record contains:\n",
    "   - `id`: Unique identifier (e.g., \"product_info_1\")\n",
    "   - `question`: The user's question\n",
    "   - `context`: Background information the LLM should use\n",
    "   - `ground_truth`: The correct answer we expect\n",
    "   - `category` and `complexity`: For stratified analysis\n",
    "   - `expected_tokens`: Estimated token count for cost prediction\n",
    "\n",
    "**Why this matters:** Without ground truth answers, we can't objectively measure if the LLM is giving correct responses. This dataset becomes our \"answer key\" for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0fbdaf-ab62-49e4-a219-f06ba0c37491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE EVALUATION DATASET\n",
    "# =============================================================================\n",
    "# Build a comprehensive dataset for LLM evaluation with ground truth answers\n",
    "\n",
    "def generate_ground_truth_answer(question, context, complexity):\n",
    "    \"\"\"\n",
    "    Generate a ground truth answer based on the context.\n",
    "    In a real scenario, these would be human-curated answers.\n",
    "    \"\"\"\n",
    "    # Extract key information from context as the ground truth\n",
    "    sentences = context.split(\". \")\n",
    "    if complexity == \"simple\":\n",
    "        return sentences[0] + \".\" if sentences else context\n",
    "    elif complexity == \"moderate\":\n",
    "        return \". \".join(sentences[:2]) + \".\" if len(sentences) >= 2 else context\n",
    "    else:  # complex\n",
    "        return context\n",
    "\n",
    "# Build the evaluation dataset\n",
    "eval_data = []\n",
    "\n",
    "for category, data in CATEGORIES.items():\n",
    "    for i, (question, context) in enumerate(zip(data[\"questions\"], data[\"contexts\"])):\n",
    "        ground_truth = generate_ground_truth_answer(question, context, data[\"complexity\"])\n",
    "\n",
    "        eval_data.append({\n",
    "            \"id\": f\"{category}_{i+1}\",\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"category\": category,\n",
    "            \"complexity\": data[\"complexity\"],\n",
    "            \"expected_tokens\": len(context.split()) * 2  # Rough estimate\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "print(f\"✅ Created evaluation dataset with {len(eval_df)} samples\")\n",
    "print(f\"\\n\uD83D\uDCCA Dataset Summary:\")\n",
    "print(eval_df.groupby([\"category\", \"complexity\"]).size().unstack(fill_value=0))\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9610c54a-1f3d-43a8-b4aa-95362f99e232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Generating Simulated Production Traffic\n",
    "\n",
    "**What we're doing:** Creating synthetic inference logs that mimic what a real production LLM endpoint would generate over 7 days of operation.\n",
    "\n",
    "**How the `generate_traffic_data()` function works:**\n",
    "\n",
    "1. **Time distribution**: Generates timestamps over 7 days with realistic patterns:\n",
    "   - More requests during business hours (9 AM - 6 PM)\n",
    "   - Fewer requests on weekends and nights\n",
    "\n",
    "2. **Latency simulation** using normal distribution:\n",
    "   ```python\n",
    "   base_latency = {\"simple\": 200, \"moderate\": 400, \"complex\": 800}\n",
    "   latency = np.random.normal(base_latency, std_dev)\n",
    "   ```\n",
    "   - Simple queries: ~200ms average\n",
    "   - Complex queries: ~800ms average\n",
    "   - 5% of requests get artificial \"spikes\" (3-5x normal latency)\n",
    "\n",
    "3. **Token usage**: Calculated based on complexity with random variation\n",
    "\n",
    "4. **Error injection**: 2% of requests marked as \"error\" or \"timeout\" to simulate real failures\n",
    "\n",
    "**Why this matters:** Real production data takes weeks to accumulate. Synthetic data lets us immediately practice monitoring, anomaly detection, and alerting without waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05920a9b-f262-4a29-ab08-20423a7629a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE SIMULATED PRODUCTION TRAFFIC DATA\n",
    "# =============================================================================\n",
    "# Create synthetic inference logs to simulate production traffic patterns\n",
    "# This data will be used for monitoring and anomaly detection exercises\n",
    "\n",
    "def generate_traffic_data(num_records=500):\n",
    "    \"\"\"\n",
    "    Generate synthetic production traffic data with realistic patterns:\n",
    "    - Normal latency with occasional spikes\n",
    "    - Varying token usage based on question complexity\n",
    "    - Time-based patterns (higher traffic during business hours)\n",
    "    \"\"\"\n",
    "    traffic_data = []\n",
    "    base_time = datetime.now() - timedelta(days=7)\n",
    "\n",
    "    for i in range(num_records):\n",
    "        # Select random question from our dataset\n",
    "        sample = random.choice(eval_data)\n",
    "\n",
    "        # Generate timestamp with business hour bias\n",
    "        hour_offset = random.gauss(14, 4)  # Peak around 2 PM\n",
    "        hour_offset = max(8, min(20, hour_offset))  # Clamp to business hours\n",
    "        timestamp = base_time + timedelta(\n",
    "            days=random.randint(0, 6),\n",
    "            hours=hour_offset,\n",
    "            minutes=random.randint(0, 59)\n",
    "        )\n",
    "\n",
    "        # Generate latency based on complexity with occasional spikes\n",
    "        base_latency = {\"simple\": 200, \"moderate\": 400, \"complex\": 800}\n",
    "        latency = base_latency[sample[\"complexity\"]]\n",
    "        latency += random.gauss(0, latency * 0.2)  # Add noise\n",
    "\n",
    "        # Inject latency spikes (5% of requests)\n",
    "        if random.random() < 0.05:\n",
    "            latency *= random.uniform(3, 8)  # Spike multiplier\n",
    "\n",
    "        # Generate token counts\n",
    "        input_tokens = len(sample[\"question\"].split()) + len(sample[\"context\"].split())\n",
    "        output_tokens = int(sample[\"expected_tokens\"] * random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Inject token anomalies (3% of requests)\n",
    "        if random.random() < 0.03:\n",
    "            output_tokens *= random.randint(3, 5)  # Unexpectedly long responses\n",
    "\n",
    "        # Generate quality score (simulated)\n",
    "        base_quality = {\"simple\": 0.9, \"moderate\": 0.8, \"complex\": 0.7}\n",
    "        quality_score = base_quality[sample[\"complexity\"]] + random.gauss(0, 0.1)\n",
    "        quality_score = max(0, min(1, quality_score))  # Clamp to [0, 1]\n",
    "\n",
    "        traffic_data.append({\n",
    "            \"request_id\": f\"req_{i:06d}\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": input_tokens + output_tokens,\n",
    "            \"quality_score\": round(quality_score, 3),\n",
    "            \"status\": \"success\" if random.random() > 0.02 else \"error\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(traffic_data)\n",
    "\n",
    "# Generate traffic data\n",
    "traffic_df = generate_traffic_data(500)\n",
    "\n",
    "print(f\"✅ Generated {len(traffic_df)} synthetic traffic records\")\n",
    "print(f\"\\n\uD83D\uDCCA Traffic Summary:\")\n",
    "print(f\"   Date Range: {traffic_df['timestamp'].min()} to {traffic_df['timestamp'].max()}\")\n",
    "print(f\"   Avg Latency: {traffic_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   Avg Tokens: {traffic_df['total_tokens'].mean():.1f}\")\n",
    "print(f\"   Error Rate: {(traffic_df['status'] == 'error').mean()*100:.1f}%\")\n",
    "display(traffic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a9f1cf-0d08-459d-97e9-2155cea20473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Saving Datasets to Delta Tables\n",
    "\n",
    "**What we're doing:** Persisting our synthetic datasets to Delta Lake tables in the Unity Catalog so they survive cluster restarts and can be queried by other notebooks.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Create schema**: `spark.sql(\"CREATE SCHEMA IF NOT EXISTS main.llm_monitoring_lab\")`\n",
    "   - Creates a namespace to organize our tables\n",
    "\n",
    "2. **Convert and save**:\n",
    "   ```python\n",
    "   spark.createDataFrame(pandas_df)  # Convert pandas → Spark DataFrame\n",
    "   .write.mode(\"overwrite\")          # Replace if exists\n",
    "   .saveAsTable(\"catalog.schema.table\")  # Save as managed Delta table\n",
    "   ```\n",
    "\n",
    "3. **Tables created**:\n",
    "   - `evaluation_dataset`: Our Q&A pairs with ground truth answers\n",
    "   - `traffic_data`: Synthetic production traffic logs\n",
    "\n",
    "**Why Delta Lake?**\n",
    "- **ACID transactions**: Safe concurrent reads/writes\n",
    "- **Time travel**: Query previous versions with `VERSION AS OF`\n",
    "- **Schema enforcement**: Prevents bad data from corrupting tables\n",
    "- **Optimized queries**: Auto-compaction and Z-ordering for fast reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11709039-4797-407e-a0f2-8d27ceba20b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE DATASETS TO DATABRICKS\n",
    "# =============================================================================\n",
    "# Store our synthetic data in Delta tables for use throughout the lab\n",
    "\n",
    "# Define catalog and schema (using default catalog)\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"llm_monitoring_lab\"\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Convert pandas DataFrames to Spark DataFrames and save as Delta tables\n",
    "eval_spark_df = spark.createDataFrame(eval_df)\n",
    "eval_spark_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.evaluation_dataset\")\n",
    "\n",
    "traffic_spark_df = spark.createDataFrame(traffic_df)\n",
    "traffic_spark_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.traffic_data\")\n",
    "\n",
    "print(f\"✅ Saved evaluation dataset to {CATALOG}.{SCHEMA}.evaluation_dataset\")\n",
    "print(f\"✅ Saved traffic data to {CATALOG}.{SCHEMA}.traffic_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdc9804-be7c-4a08-9f4d-b70a74651b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Creating and Deploying the Model Serving Endpoint\n",
    "\n",
    "In this section, we will:\n",
    "1. Register a foundation model for serving\n",
    "2. Create a Model Serving endpoint\n",
    "3. Configure inference tables for logging\n",
    "4. Test the endpoint with sample queries\n",
    "\n",
    "This endpoint will serve as our production Q&A system that we'll monitor and optimize throughout the lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb8f64dc-c1b8-4e38-af26-dcf7c449649a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2.1: Configure Foundation Model API Access\n",
    "\n",
    "Databricks provides **Foundation Model APIs** - pre-deployed, pay-per-token endpoints for popular LLMs. These are:\n",
    "\n",
    "- **Immediately available** - no deployment required\n",
    "- **Auto-scaling** - handles any traffic volume\n",
    "- **Pay-per-token** - cost-effective for variable workloads\n",
    "- **OpenAI-compatible** - easy integration with existing code\n",
    "\n",
    "We'll use the **Meta Llama 3.1 8B Instruct** model (`databricks-meta-llama-3-1-8b-instruct`), which is well-suited for Q&A tasks.\n",
    "\n",
    "For inference table logging, we'll create a **custom proxy endpoint** that wraps the foundation model and enables request/response logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990eefb4-057d-4c9a-b2c1-81f619a188ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SERVING ENDPOINT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Databricks Foundation Model APIs are pre-deployed and ready to use\n",
    "# No custom endpoint creation needed - we query them directly\n",
    "\n",
    "FOUNDATION_MODEL = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "# First, let's verify the Foundation Model is available\n",
    "print(\"\uD83D\uDD0D Checking available Foundation Model APIs...\")\n",
    "\n",
    "# List foundation model endpoints\n",
    "try:\n",
    "    all_endpoints = list(w.serving_endpoints.list())\n",
    "    foundation_endpoints = [e for e in all_endpoints if e.name.startswith(\"databricks-\")]\n",
    "    print(f\"   Found {len(foundation_endpoints)} Foundation Model endpoints\")\n",
    "\n",
    "    # Check if our model is available\n",
    "    available_models = [e.name for e in foundation_endpoints]\n",
    "    if FOUNDATION_MODEL in available_models:\n",
    "        print(f\"   ✅ {FOUNDATION_MODEL} is available\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ {FOUNDATION_MODEL} not found in list. Available models:\")\n",
    "        for model in available_models[:5]:\n",
    "            print(f\"      - {model}\")\n",
    "        print(f\"   Note: The model may still work - Foundation Models are always available\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: Could not list endpoints ({e})\")\n",
    "    print(f\"   Proceeding with Foundation Model - these are always available\")\n",
    "\n",
    "# For this lab, we'll use the Foundation Model directly\n",
    "# The endpoint name for queries will be the foundation model name\n",
    "ENDPOINT_NAME = FOUNDATION_MODEL\n",
    "\n",
    "print(f\"\\n✅ Using Foundation Model: {ENDPOINT_NAME}\")\n",
    "print(f\"   Endpoint URL: {DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cc1fcd-acfe-41e1-a52e-ee09db328937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verifying the Foundation Model Endpoint\n",
    "\n",
    "**What we're doing:** Confirming that the Databricks Foundation Model API is accessible before we start sending queries.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **SDK endpoint check**: `w.serving_endpoints.get(name=ENDPOINT_NAME)`\n",
    "   - Retrieves metadata about the endpoint\n",
    "   - Confirms the endpoint exists and is in \"READY\" state\n",
    "\n",
    "2. **What we verify**:\n",
    "   - Endpoint name matches our configuration\n",
    "   - State is \"READY\" (not \"PENDING\" or \"FAILED\")\n",
    "   - URL is correctly formed for API calls\n",
    "\n",
    "**Foundation Models vs Custom Endpoints:**\n",
    "\n",
    "| Aspect | Foundation Model | Custom Endpoint |\n",
    "|--------|-----------------|-----------------|\n",
    "| Provisioning | Always ready | Minutes to hours |\n",
    "| Scaling | Automatic | Manual configuration |\n",
    "| Billing | Pay-per-token | Pay for compute time |\n",
    "| Customization | None | Fine-tuning possible |\n",
    "\n",
    "**Why this matters:** Verifying the endpoint upfront prevents confusing errors later. If the endpoint isn't accessible, we want to know immediately rather than after running expensive evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227fd3a4-ce3d-44f5-be4e-495f2c0612d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY FOUNDATION MODEL ENDPOINT\n",
    "# =============================================================================\n",
    "# Foundation Model APIs are always ready - no provisioning needed\n",
    "# Let's verify the endpoint is accessible\n",
    "\n",
    "try:\n",
    "    endpoint_info = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "    print(f\"✅ Foundation Model Endpoint Verified\")\n",
    "    print(f\"\\n\uD83D\uDCCA Endpoint Details:\")\n",
    "    print(f\"   Name: {endpoint_info.name}\")\n",
    "    print(f\"   State: {endpoint_info.state.ready}\")\n",
    "    print(f\"   URL: {DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\")\n",
    "\n",
    "    # Check endpoint configuration\n",
    "    if endpoint_info.config and endpoint_info.config.served_entities:\n",
    "        for entity in endpoint_info.config.served_entities:\n",
    "            print(f\"\\n   Served Entity:\")\n",
    "            print(f\"      Name: {entity.entity_name if hasattr(entity, 'entity_name') else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not get endpoint details: {str(e)}\")\n",
    "    print(f\"   This is expected for Foundation Model APIs - they work without explicit configuration\")\n",
    "    print(f\"\\n✅ Proceeding with endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e060eda3-61a4-41b3-b3db-57355ebc2a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2.2: Test the Model Serving Endpoint\n",
    "\n",
    "Now we'll test our endpoint with sample queries to verify it's working correctly. We'll use the OpenAI-compatible API that Databricks provides for foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c660e3a0-8fec-4570-a3c6-ee117e7b5b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# =============================================================================\n",
    "# TEST MODEL SERVING ENDPOINT\n",
    "# =============================================================================\n",
    "# Use OpenAI-compatible client to query our endpoint\n",
    "\n",
    "# Disable MLflow's automatic tracing for OpenAI calls to avoid duplicate trace ID warnings\n",
    "# This happens because MLflow tries to auto-trace LLM calls, but re-running cells\n",
    "# can cause trace ID conflicts\n",
    "try:\n",
    "    mlflow.openai.autolog(disable=True)\n",
    "except AttributeError:\n",
    "    # Older MLflow versions may not have this - that's OK\n",
    "    pass\n",
    "\n",
    "# Initialize OpenAI client pointing to Databricks\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "def query_qa_endpoint(question, context, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Query the Q&A endpoint with a question and context.\n",
    "    Returns the response and timing information.\n",
    "    \"\"\"\n",
    "    # Construct the prompt for Q&A\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "    Be concise and accurate. If the answer is not in the context, say so.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=ENDPOINT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.1  # Low temperature for factual responses\n",
    "    )\n",
    "\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Test with a sample question\n",
    "test_sample = eval_data[0]\n",
    "print(f\"\uD83D\uDCDD Testing with question: {test_sample['question']}\")\n",
    "print(f\"\uD83D\uDCC4 Context: {test_sample['context'][:100]}...\")\n",
    "\n",
    "result = query_qa_endpoint(test_sample[\"question\"], test_sample[\"context\"])\n",
    "\n",
    "print(f\"\\n✅ Response received:\")\n",
    "print(f\"   Answer: {result['answer']}\")\n",
    "print(f\"   Latency: {result['latency_ms']:.2f} ms\")\n",
    "print(f\"   Tokens: {result['total_tokens']} (input: {result['input_tokens']}, output: {result['output_tokens']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d12c72f-6dd4-495d-959e-3ba0be6f5cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Establishing Baseline Performance Metrics\n",
    "\n",
    "**What we're doing:** Running multiple queries to collect baseline performance data that will inform our monitoring thresholds.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Batch execution loop**:\n",
    "   ```python\n",
    "   for sample in eval_data[:10]:\n",
    "       result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "       batch_results.append(result)\n",
    "       time.sleep(1)  # Rate limiting to avoid throttling\n",
    "   ```\n",
    "\n",
    "2. **Metrics collected per query**:\n",
    "   - `latency_ms`: End-to-end response time\n",
    "   - `input_tokens`: Tokens in prompt (question + context)\n",
    "   - `output_tokens`: Tokens in generated answer\n",
    "   - `category` and `complexity`: For stratified analysis\n",
    "\n",
    "3. **Baseline statistics calculated**:\n",
    "   - Mean, P50 (median), P95, P99 latency\n",
    "   - Average token usage by complexity\n",
    "   - Success rate\n",
    "\n",
    "**Why baselines matter (Chapter 8 best practice):**\n",
    "- **Threshold setting**: \"Alert when latency > P95 baseline × 1.5\"\n",
    "- **Drift detection**: Compare current metrics to baseline\n",
    "- **Capacity planning**: Predict costs based on token patterns\n",
    "- **SLA definition**: Set realistic performance guarantees\n",
    "\n",
    "Without baselines, you're guessing at what \"normal\" looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f8750e-bd46-4f36-ac7b-e887e872155a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BATCH TEST FOR BASELINE METRICS\n",
    "# =============================================================================\n",
    "# Run multiple queries to establish baseline performance metrics\n",
    "\n",
    "print(\"\uD83D\uDD04 Running batch test to establish baseline metrics...\")\n",
    "print(\"   This will take a few minutes...\\n\")\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "# Test with a subset of our evaluation data\n",
    "for i, sample in enumerate(eval_data[:10]):\n",
    "    try:\n",
    "        result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "        result[\"id\"] = sample[\"id\"]\n",
    "        result[\"category\"] = sample[\"category\"]\n",
    "        result[\"complexity\"] = sample[\"complexity\"]\n",
    "        result[\"ground_truth\"] = sample[\"ground_truth\"]\n",
    "        batch_results.append(result)\n",
    "        print(f\"   ✓ Completed {i+1}/10: {sample['id']} ({result['latency_ms']:.0f}ms)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Failed {i+1}/10: {sample['id']} - {str(e)}\")\n",
    "\n",
    "    # Small delay to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create results DataFrame\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Baseline Performance Summary:\")\n",
    "print(f\"   Average Latency: {batch_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   P95 Latency: {batch_df['latency_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"   Average Tokens: {batch_df['total_tokens'].mean():.1f}\")\n",
    "print(f\"\\n   By Complexity:\")\n",
    "print(batch_df.groupby('complexity')[['latency_ms', 'total_tokens']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7911f5-d062-4a39-972c-abf1f59031ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: Structured Evaluation with MLflow\n",
    "\n",
    "In this section, we will:\n",
    "1. Set up MLflow evaluation for LLM responses\n",
    "2. Measure quality metrics (groundedness, relevance, coherence)\n",
    "3. Track latency and token usage metrics\n",
    "4. Compare baseline vs. optimized performance\n",
    "\n",
    "MLflow provides built-in LLM evaluation capabilities that help us systematically assess model quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c4e816-5dff-47fb-be40-07632f85af31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3.1: Configure MLflow LLM Evaluation\n",
    "\n",
    "**What we're doing:** Setting up custom evaluation scorers using MLflow's new GenAI API (MLflow 3.4+).\n",
    "\n",
    "**How the new API works:**\n",
    "\n",
    "1. **`@scorer` decorator**: Creates a custom evaluation function\n",
    "   ```python\n",
    "   @scorer\n",
    "   def groundedness(inputs: dict, outputs: str) -> int:\n",
    "       # Use LLM-as-judge to score the output\n",
    "       return score  # 1-5\n",
    "   ```\n",
    "\n",
    "2. **Function signature**: Scorers receive standardized parameters:\n",
    "   - `inputs`: Dictionary with question, context, etc.\n",
    "   - `outputs`: The model's generated response\n",
    "   - `expectations`: Ground truth for comparison (optional)\n",
    "\n",
    "3. **LLM-as-Judge pattern**: We use another LLM (Llama 70B) to evaluate responses\n",
    "\n",
    "**Why the new API?**\n",
    "- The old `make_genai_metric` is deprecated since MLflow 3.4.0\n",
    "- New API is more flexible and Pythonic\n",
    "- Better integration with MLflow's tracing and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a3de05-8fc6-4feb-a9c0-be484e19f840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "from typing import Literal\n",
    "\n",
    "# =============================================================================\n",
    "# MLFLOW GENAI EVALUATION SETUP (New API - MLflow 3.4+)\n",
    "# =============================================================================\n",
    "# Configure evaluation scorers for our Q&A system using the new GenAI API\n",
    "# This replaces the deprecated make_genai_metric approach\n",
    "\n",
    "# Use the same foundation model for judging (8B model available in workspace)\n",
    "JUDGE_MODEL = FOUNDATION_MODEL  # databricks-meta-llama-3-1-8b-instruct\n",
    "\n",
    "# Define a custom groundedness scorer using the @scorer decorator\n",
    "# This measures how well the answer is grounded in the provided context\n",
    "@scorer\n",
    "def groundedness(inputs: dict, outputs: str) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer is grounded in the provided context.\n",
    "    Returns a score from 1-5.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    judge_client = OpenAI(\n",
    "        api_key=DATABRICKS_TOKEN,\n",
    "        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are evaluating whether an answer is grounded in the provided context.\n",
    "\n",
    "Context: {inputs.get('context', '')}\n",
    "Question: {inputs.get('question', '')}\n",
    "Answer: {outputs}\n",
    "\n",
    "Score the groundedness from 1-5:\n",
    "1 = Answer contains claims not supported by context\n",
    "2 = Answer partially supported but has unsupported claims\n",
    "3 = Answer mostly supported with minor gaps\n",
    "4 = Answer well supported by context\n",
    "5 = Answer fully grounded in context\n",
    "\n",
    "Respond with ONLY a single number (1-5).\"\"\"\n",
    "\n",
    "    response = judge_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score = int(response.choices[0].message.content.strip()[0])\n",
    "        return max(1, min(5, score))  # Ensure score is between 1-5\n",
    "    except:\n",
    "        return 3  # Default to middle score if parsing fails\n",
    "\n",
    "# Define answer correctness scorer\n",
    "@scorer\n",
    "def answer_correctness(inputs: dict, outputs: str, expectations: dict) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer correctly addresses the question based on ground truth.\n",
    "    Returns a score from 1-5.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    judge_client = OpenAI(\n",
    "        api_key=DATABRICKS_TOKEN,\n",
    "        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"Compare the generated answer with the ground truth answer.\n",
    "\n",
    "Question: {inputs.get('question', '')}\n",
    "Ground Truth: {expectations.get('ground_truth', '')}\n",
    "Generated Answer: {outputs}\n",
    "\n",
    "Score from 1-5:\n",
    "1 = Completely incorrect or contradicts ground truth\n",
    "2 = Partially correct but missing key information\n",
    "3 = Mostly correct with some inaccuracies\n",
    "4 = Correct with minor differences in wording\n",
    "5 = Fully correct and complete\n",
    "\n",
    "Respond with ONLY a single number (1-5).\"\"\"\n",
    "\n",
    "    response = judge_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score = int(response.choices[0].message.content.strip()[0])\n",
    "        return max(1, min(5, score))  # Ensure score is between 1-5\n",
    "    except:\n",
    "        return 3  # Default to middle score if parsing fails\n",
    "\n",
    "print(\"✅ Custom evaluation scorers configured (MLflow 3.4+ GenAI API):\")\n",
    "print(f\"   - Judge model: {JUDGE_MODEL}\")\n",
    "print(\"   - groundedness: Measures factual support from context\")\n",
    "print(\"   - answer_correctness: Compares with ground truth answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ae5c06-a59c-42a4-9ea2-e1a6cd832726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3.2: Run Comprehensive Evaluation\n",
    "\n",
    "Now we'll run a full evaluation using MLflow. This will:\n",
    "1. Query our endpoint for each evaluation sample\n",
    "2. Calculate quality metrics using LLM-as-judge\n",
    "3. Log all results to MLflow for tracking\n",
    "4. Generate a comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1db3e5f-6082-4617-9c1c-2923daf910ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN MLFLOW EVALUATION\n",
    "# =============================================================================\n",
    "# Execute comprehensive evaluation and log results\n",
    "\n",
    "# Prepare evaluation data with model outputs\n",
    "eval_results = []\n",
    "\n",
    "print(\"\uD83D\uDD04 Running comprehensive evaluation...\")\n",
    "print(\"   Querying endpoint and collecting responses...\\n\")\n",
    "\n",
    "for i, sample in enumerate(eval_data):\n",
    "    try:\n",
    "        # Query the endpoint\n",
    "        result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        eval_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"context\": sample[\"context\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"output\": result[\"answer\"],  # MLflow expects 'output' column\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": result[\"latency_ms\"],\n",
    "            \"input_tokens\": result[\"input_tokens\"],\n",
    "            \"output_tokens\": result[\"output_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   ✓ {i+1}/{len(eval_data)}: {sample['id']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ {i+1}/{len(eval_data)}: {sample['id']} - {str(e)}\")\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "eval_results_df = pd.DataFrame(eval_results)\n",
    "print(f\"\\n✅ Collected {len(eval_results_df)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb3b606-9111-43b2-8280-5d66826056d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Logging Evaluation Results to MLflow\n",
    "\n",
    "**What we're doing:** Running our custom scorers on each sample and logging results to MLflow.\n",
    "\n",
    "**How the new GenAI API works:**\n",
    "\n",
    "1. **Prepare data in new format**:\n",
    "   ```python\n",
    "   eval_data_for_genai.append({\n",
    "       \"inputs\": {\"question\": ..., \"context\": ...},\n",
    "       \"outputs\": row[\"output\"],\n",
    "       \"expectations\": {\"ground_truth\": ...}\n",
    "   })\n",
    "   ```\n",
    "\n",
    "2. **Call scorers directly**:\n",
    "   ```python\n",
    "   g_score = groundedness(inputs=item[\"inputs\"], outputs=item[\"outputs\"])\n",
    "   c_score = answer_correctness(inputs=..., outputs=..., expectations=...)\n",
    "   ```\n",
    "   - Each scorer calls the judge LLM (Llama 70B)\n",
    "   - Returns a score from 1-5\n",
    "\n",
    "3. **Log aggregate metrics**:\n",
    "   ```python\n",
    "   mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "   mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "   ```\n",
    "\n",
    "**Why this approach?** The new MLflow 3.4+ GenAI API is more flexible and Pythonic. We call scorers directly, giving us full control over error handling and rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b11c31-bfc6-4f70-a1b4-acc37af165aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG EVALUATION TO MLFLOW\n",
    "# =============================================================================\n",
    "# Run MLflow evaluation with our custom scorers (New GenAI API)\n",
    "\n",
    "with mlflow.start_run(run_name=\"baseline_evaluation\") as run:\n",
    "\n",
    "    # Log evaluation parameters\n",
    "    mlflow.log_param(\"endpoint_name\", ENDPOINT_NAME)\n",
    "    mlflow.log_param(\"model_name\", FOUNDATION_MODEL)\n",
    "    mlflow.log_param(\"num_samples\", len(eval_results_df))\n",
    "    mlflow.log_param(\"evaluation_date\", datetime.now().isoformat())\n",
    "\n",
    "    # Prepare data in the new format for mlflow.genai.evaluate()\n",
    "    # The new API expects: inputs (dict), outputs (str), expectations (dict)\n",
    "    eval_data_for_genai = []\n",
    "    for _, row in eval_results_df.iterrows():\n",
    "        eval_data_for_genai.append({\n",
    "            \"inputs\": {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"context\": row[\"context\"]\n",
    "            },\n",
    "            \"outputs\": row[\"output\"],\n",
    "            \"expectations\": {\n",
    "                \"ground_truth\": row[\"ground_truth\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Run evaluation with custom scorers\n",
    "    print(\"\uD83D\uDD04 Running MLflow evaluation with LLM-as-judge scorers...\")\n",
    "    print(\"   (Using new MLflow 3.4+ GenAI API)\")\n",
    "\n",
    "    # Calculate scores manually and log them\n",
    "    groundedness_scores = []\n",
    "    correctness_scores = []\n",
    "\n",
    "    for i, item in enumerate(eval_data_for_genai):\n",
    "        try:\n",
    "            g_score = groundedness(inputs=item[\"inputs\"], outputs=item[\"outputs\"])\n",
    "            c_score = answer_correctness(\n",
    "                inputs=item[\"inputs\"],\n",
    "                outputs=item[\"outputs\"],\n",
    "                expectations=item[\"expectations\"]\n",
    "            )\n",
    "            groundedness_scores.append(g_score)\n",
    "            correctness_scores.append(c_score)\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"   Evaluated {i + 1}/{len(eval_data_for_genai)} samples...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Error evaluating sample {i}: {e}\")\n",
    "            groundedness_scores.append(3)\n",
    "            correctness_scores.append(3)\n",
    "        time.sleep(0.5)  # Rate limiting for judge model\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    avg_groundedness = sum(groundedness_scores) / len(groundedness_scores)\n",
    "    avg_correctness = sum(correctness_scores) / len(correctness_scores)\n",
    "\n",
    "    # Log LLM-as-judge metrics\n",
    "    mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "    mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "\n",
    "    # Log additional performance metrics\n",
    "    mlflow.log_metric(\"avg_latency_ms\", eval_results_df[\"latency_ms\"].mean())\n",
    "    mlflow.log_metric(\"p95_latency_ms\", eval_results_df[\"latency_ms\"].quantile(0.95))\n",
    "    mlflow.log_metric(\"avg_total_tokens\", eval_results_df[\"total_tokens\"].mean())\n",
    "    mlflow.log_metric(\"total_input_tokens\", eval_results_df[\"input_tokens\"].sum())\n",
    "    mlflow.log_metric(\"total_output_tokens\", eval_results_df[\"output_tokens\"].sum())\n",
    "\n",
    "    # Add scores to dataframe and save\n",
    "    eval_results_df[\"groundedness_score\"] = groundedness_scores\n",
    "    eval_results_df[\"correctness_score\"] = correctness_scores\n",
    "\n",
    "    # Log the evaluation dataset as artifact\n",
    "    eval_results_df.to_csv(\"/tmp/evaluation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/evaluation_results.csv\")\n",
    "\n",
    "    print(f\"\\n✅ Evaluation logged to MLflow\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "    # Display metrics summary\n",
    "    print(f\"\\n\uD83D\uDCCA Evaluation Metrics Summary:\")\n",
    "    print(f\"   groundedness_mean: {avg_groundedness:.2f}\")\n",
    "    print(f\"   answer_correctness_mean: {avg_correctness:.2f}\")\n",
    "    print(f\"   avg_latency_ms: {eval_results_df['latency_ms'].mean():.2f}\")\n",
    "    print(f\"   p95_latency_ms: {eval_results_df['latency_ms'].quantile(0.95):.2f}\")\n",
    "    print(f\"   avg_total_tokens: {eval_results_df['total_tokens'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2e4fa0-42a8-4ebd-be51-ef3621dbc9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 4: Analyzing Inference Tables\n",
    "\n",
    "In this section, we will:\n",
    "1. Query inference tables to analyze production traffic\n",
    "2. Identify latency patterns and anomalies\n",
    "3. Analyze token usage trends\n",
    "4. Detect problematic query patterns\n",
    "\n",
    "Inference tables automatically capture all requests to Model Serving endpoints, providing rich data for monitoring and debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272d8961-8540-4a60-84a4-e6a963be73fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.1: Query Inference Tables\n",
    "\n",
    "Databricks automatically logs all inference requests to Delta tables when auto-capture is enabled. These tables contain:\n",
    "\n",
    "- **Request payloads**: Input prompts and parameters\n",
    "- **Response data**: Model outputs and metadata\n",
    "- **Timing information**: Latency measurements\n",
    "- **Token counts**: Usage metrics for cost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5ff071-a74f-4188-9f3b-ceeac309c4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# QUERY INFERENCE TABLES\n",
    "# =============================================================================\n",
    "# Analyze production traffic patterns from inference logs\n",
    "\n",
    "# For this lab, we'll use our synthetic traffic data\n",
    "# In production, you would query the actual inference tables:\n",
    "# inference_table = f\"{CATALOG}.{SCHEMA}.qa_endpoint_payload\"\n",
    "\n",
    "# Load our synthetic traffic data\n",
    "traffic_analysis_df = spark.table(f\"{CATALOG}.{SCHEMA}.traffic_data\").toPandas()\n",
    "\n",
    "print(f\"\uD83D\uDCCA Analyzing {len(traffic_analysis_df)} inference records\")\n",
    "print(f\"   Time Range: {traffic_analysis_df['timestamp'].min()} to {traffic_analysis_df['timestamp'].max()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n\uD83D\uDCC8 Traffic Statistics:\")\n",
    "print(f\"   Total Requests: {len(traffic_analysis_df)}\")\n",
    "print(f\"   Success Rate: {(traffic_analysis_df['status'] == 'success').mean()*100:.1f}%\")\n",
    "print(f\"   Avg Latency: {traffic_analysis_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   P50 Latency: {traffic_analysis_df['latency_ms'].median():.2f} ms\")\n",
    "print(f\"   P95 Latency: {traffic_analysis_df['latency_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"   P99 Latency: {traffic_analysis_df['latency_ms'].quantile(0.99):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09dd23f-6955-43aa-a3ed-a5c3c88dd5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.2: Identify Latency Patterns\n",
    "\n",
    "We'll analyze latency patterns to identify:\n",
    "- Time-based variations (peak hours vs. off-peak)\n",
    "- Complexity-based differences\n",
    "- Anomalous slow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc2cb94-f102-4019-b07f-c359b36f03ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================================================================\n",
    "# LATENCY PATTERN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Latency distribution by complexity\n",
    "ax1 = axes[0, 0]\n",
    "for complexity in ['simple', 'moderate', 'complex']:\n",
    "    data = traffic_analysis_df[traffic_analysis_df['complexity'] == complexity]['latency_ms']\n",
    "    ax1.hist(data, bins=30, alpha=0.5, label=complexity)\n",
    "ax1.set_xlabel('Latency (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Latency Distribution by Complexity')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Latency over time\n",
    "ax2 = axes[0, 1]\n",
    "traffic_analysis_df['hour'] = pd.to_datetime(traffic_analysis_df['timestamp']).dt.hour\n",
    "hourly_latency = traffic_analysis_df.groupby('hour')['latency_ms'].mean()\n",
    "ax2.plot(hourly_latency.index, hourly_latency.values, marker='o')\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Average Latency (ms)')\n",
    "ax2.set_title('Average Latency by Hour')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Token usage vs latency\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(traffic_analysis_df['total_tokens'], traffic_analysis_df['latency_ms'],\n",
    "            alpha=0.5, c=traffic_analysis_df['complexity'].map({'simple': 0, 'moderate': 1, 'complex': 2}))\n",
    "ax3.set_xlabel('Total Tokens')\n",
    "ax3.set_ylabel('Latency (ms)')\n",
    "ax3.set_title('Token Usage vs Latency')\n",
    "\n",
    "# 4. Latency percentiles by category\n",
    "ax4 = axes[1, 1]\n",
    "category_stats = traffic_analysis_df.groupby('category')['latency_ms'].agg(['mean', 'median', lambda x: x.quantile(0.95)])\n",
    "category_stats.columns = ['Mean', 'Median', 'P95']\n",
    "category_stats.plot(kind='bar', ax=ax4)\n",
    "ax4.set_xlabel('Category')\n",
    "ax4.set_ylabel('Latency (ms)')\n",
    "ax4.set_title('Latency Statistics by Category')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/latency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Latency analysis charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728ec779-bb7d-41f8-bda1-52378d8e6f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.3: Identify Slow and Anomalous Queries\n",
    "\n",
    "We'll identify queries that exceed our latency thresholds and investigate their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf8f48d-9bdd-4ef8-9140-809c210d1a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANOMALY DETECTION IN LATENCY\n",
    "# =============================================================================\n",
    "\n",
    "# Define latency thresholds based on complexity\n",
    "LATENCY_THRESHOLDS = {\n",
    "    \"simple\": 500,      # 500ms for simple queries\n",
    "    \"moderate\": 1000,   # 1s for moderate queries\n",
    "    \"complex\": 2000     # 2s for complex queries\n",
    "}\n",
    "\n",
    "# Identify slow queries\n",
    "def is_slow_query(row):\n",
    "    threshold = LATENCY_THRESHOLDS.get(row['complexity'], 1000)\n",
    "    return row['latency_ms'] > threshold\n",
    "\n",
    "traffic_analysis_df['is_slow'] = traffic_analysis_df.apply(is_slow_query, axis=1)\n",
    "\n",
    "# Analyze slow queries\n",
    "slow_queries = traffic_analysis_df[traffic_analysis_df['is_slow']]\n",
    "\n",
    "print(f\"\uD83D\uDC22 Slow Query Analysis:\")\n",
    "print(f\"   Total Slow Queries: {len(slow_queries)} ({len(slow_queries)/len(traffic_analysis_df)*100:.1f}%)\")\n",
    "print(f\"\\n   By Complexity:\")\n",
    "print(slow_queries.groupby('complexity').size())\n",
    "print(f\"\\n   By Category:\")\n",
    "print(slow_queries.groupby('category').size())\n",
    "\n",
    "# Identify extreme outliers (> 3 standard deviations)\n",
    "mean_latency = traffic_analysis_df['latency_ms'].mean()\n",
    "std_latency = traffic_analysis_df['latency_ms'].std()\n",
    "outlier_threshold = mean_latency + 3 * std_latency\n",
    "\n",
    "outliers = traffic_analysis_df[traffic_analysis_df['latency_ms'] > outlier_threshold]\n",
    "print(f\"\\n⚠️ Extreme Outliers (>{outlier_threshold:.0f}ms): {len(outliers)}\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\n   Sample outlier queries:\")\n",
    "    for _, row in outliers.head(3).iterrows():\n",
    "        print(f\"   - {row['request_id']}: {row['latency_ms']:.0f}ms ({row['complexity']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53a1b59-aa32-4f13-94a4-c436c6ba946a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 5: Monitoring Dashboards and Agent Traces\n",
    "\n",
    "In this section, we will:\n",
    "1. Create monitoring dashboards for key metrics\n",
    "2. Implement agent tracing for multi-step workflows\n",
    "3. Analyze trace data to identify bottlenecks\n",
    "4. Set up baseline-aware monitoring\n",
    "\n",
    "Effective monitoring requires both real-time dashboards and detailed traces for debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f6c8a1-b313-4bee-9112-2ee0b9e7a000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5.1: Create Monitoring Dashboard Metrics\n",
    "\n",
    "We'll define key metrics for our monitoring dashboard following Chapter 8 best practices:\n",
    "\n",
    "- **Latency metrics**: P50, P95, P99 response times\n",
    "- **Quality metrics**: Groundedness, correctness scores\n",
    "- **Cost metrics**: Token usage, request volume\n",
    "- **Error metrics**: Error rates, timeout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6ddf9b-bea1-4935-a7d5-910aff31877b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MONITORING DASHBOARD METRICS\n",
    "# =============================================================================\n",
    "# Define and calculate key monitoring metrics\n",
    "\n",
    "def calculate_monitoring_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive monitoring metrics from traffic data.\n",
    "    These metrics form the basis of our monitoring dashboard.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        # Latency Metrics\n",
    "        \"latency_p50\": df['latency_ms'].median(),\n",
    "        \"latency_p95\": df['latency_ms'].quantile(0.95),\n",
    "        \"latency_p99\": df['latency_ms'].quantile(0.99),\n",
    "        \"latency_mean\": df['latency_ms'].mean(),\n",
    "        \"latency_std\": df['latency_ms'].std(),\n",
    "\n",
    "        # Volume Metrics\n",
    "        \"total_requests\": len(df),\n",
    "        \"requests_per_hour\": len(df) / 168,  # 7 days * 24 hours\n",
    "        \"success_rate\": (df['status'] == 'success').mean(),\n",
    "        \"error_rate\": (df['status'] == 'error').mean(),\n",
    "\n",
    "        # Token Metrics\n",
    "        \"avg_input_tokens\": df['input_tokens'].mean(),\n",
    "        \"avg_output_tokens\": df['output_tokens'].mean(),\n",
    "        \"avg_total_tokens\": df['total_tokens'].mean(),\n",
    "        \"total_tokens_consumed\": df['total_tokens'].sum(),\n",
    "\n",
    "        # Quality Metrics (from our synthetic data)\n",
    "        \"avg_quality_score\": df['quality_score'].mean(),\n",
    "        \"quality_below_threshold\": (df['quality_score'] < 0.7).mean(),\n",
    "\n",
    "        # Complexity Distribution\n",
    "        \"pct_simple\": (df['complexity'] == 'simple').mean(),\n",
    "        \"pct_moderate\": (df['complexity'] == 'moderate').mean(),\n",
    "        \"pct_complex\": (df['complexity'] == 'complex').mean(),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Calculate current metrics\n",
    "current_metrics = calculate_monitoring_metrics(traffic_analysis_df)\n",
    "\n",
    "print(\"\uD83D\uDCCA Current Monitoring Metrics:\")\n",
    "print(\"\\n   Latency Metrics:\")\n",
    "print(f\"      P50: {current_metrics['latency_p50']:.2f} ms\")\n",
    "print(f\"      P95: {current_metrics['latency_p95']:.2f} ms\")\n",
    "print(f\"      P99: {current_metrics['latency_p99']:.2f} ms\")\n",
    "\n",
    "print(\"\\n   Volume Metrics:\")\n",
    "print(f\"      Total Requests: {current_metrics['total_requests']}\")\n",
    "print(f\"      Success Rate: {current_metrics['success_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n   Token Metrics:\")\n",
    "print(f\"      Avg Total Tokens: {current_metrics['avg_total_tokens']:.1f}\")\n",
    "print(f\"      Total Consumed: {current_metrics['total_tokens_consumed']:,}\")\n",
    "\n",
    "print(\"\\n   Quality Metrics:\")\n",
    "print(f\"      Avg Quality Score: {current_metrics['avg_quality_score']:.3f}\")\n",
    "print(f\"      Below Threshold: {current_metrics['quality_below_threshold']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460c82e3-10d6-49da-a562-7a8f2db7fa82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5.2: Implement Agent Tracing\n",
    "\n",
    "For multi-step agent workflows, we need detailed tracing to understand:\n",
    "- Which steps take the longest\n",
    "- Where errors occur\n",
    "- How context flows between steps\n",
    "\n",
    "We'll use MLflow's tracing capabilities to instrument our Q&A workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0025e716-202f-405a-bbeb-96037736f117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT TRACING IMPLEMENTATION\n",
    "# =============================================================================\n",
    "# Implement manual tracing for multi-step Q&A workflow\n",
    "# We'll track timing and metrics for each step without relying on decorators\n",
    "\n",
    "class TracedQAAgent:\n",
    "    \"\"\"\n",
    "    A traced Q&A agent that logs detailed execution information.\n",
    "    This simulates a multi-step agent workflow with:\n",
    "    1. Query preprocessing\n",
    "    2. Context retrieval\n",
    "    3. LLM inference\n",
    "    4. Response post-processing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_name, client):\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.client = client\n",
    "        self.trace_log = []\n",
    "\n",
    "    def preprocess_query(self, question):\n",
    "        \"\"\"Step 1: Preprocess the user query\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # Simulate preprocessing (normalization, spell check, etc.)\n",
    "        processed = question.strip().lower()\n",
    "        processed = ' '.join(processed.split())  # Normalize whitespace\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"original\": question,\n",
    "            \"processed\": processed,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def retrieve_context(self, query, context):\n",
    "        \"\"\"Step 2: Retrieve relevant context\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # In production, this would query a vector database\n",
    "        # For this lab, we use the provided context\n",
    "        relevant_context = context\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"context\": relevant_context,\n",
    "            \"context_tokens\": len(relevant_context.split()),\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def generate_response(self, question, context):\n",
    "        \"\"\"Step 3: Generate response using LLM\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer based on the context provided.\"\"\"\n",
    "        user_prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.endpoint_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def postprocess_response(self, response):\n",
    "        \"\"\"Step 4: Post-process the response\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # Simulate post-processing (formatting, safety checks, etc.)\n",
    "        processed = response.strip()\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"final_response\": processed,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def run(self, question, context):\n",
    "        \"\"\"Execute the full Q&A pipeline with manual tracing\"\"\"\n",
    "        trace_id = str(uuid.uuid4())[:8]\n",
    "        trace_record = {\"trace_id\": trace_id, \"steps\": []}\n",
    "\n",
    "        # Step 1: Preprocess\n",
    "        preprocessed = self.preprocess_query(question)\n",
    "        trace_record[\"steps\"].append({\"step\": \"preprocess\", \"latency_ms\": preprocessed[\"latency_ms\"]})\n",
    "\n",
    "        # Step 2: Retrieve context\n",
    "        retrieved = self.retrieve_context(preprocessed[\"processed\"], context)\n",
    "        trace_record[\"steps\"].append({\"step\": \"retrieve\", \"latency_ms\": retrieved[\"latency_ms\"]})\n",
    "\n",
    "        # Step 3: Generate response\n",
    "        generated = self.generate_response(question, retrieved[\"context\"])\n",
    "        trace_record[\"steps\"].append({\"step\": \"generate\", \"latency_ms\": generated[\"latency_ms\"]})\n",
    "\n",
    "        # Step 4: Post-process\n",
    "        final = self.postprocess_response(generated[\"response\"])\n",
    "        trace_record[\"steps\"].append({\"step\": \"postprocess\", \"latency_ms\": final[\"latency_ms\"]})\n",
    "\n",
    "        # Calculate total latency\n",
    "        total_latency = sum(step[\"latency_ms\"] for step in trace_record[\"steps\"])\n",
    "        trace_record[\"total_latency_ms\"] = total_latency\n",
    "\n",
    "        # Store trace\n",
    "        self.trace_log.append(trace_record)\n",
    "\n",
    "        return {\n",
    "            \"trace_id\": trace_id,\n",
    "            \"answer\": final[\"final_response\"],\n",
    "            \"total_latency_ms\": total_latency,\n",
    "            \"step_latencies\": {\n",
    "                \"preprocess\": preprocessed[\"latency_ms\"],\n",
    "                \"retrieve\": retrieved[\"latency_ms\"],\n",
    "                \"generate\": generated[\"latency_ms\"],\n",
    "                \"postprocess\": final[\"latency_ms\"]\n",
    "            },\n",
    "            \"tokens\": generated[\"tokens\"]\n",
    "        }\n",
    "\n",
    "# Initialize traced agent\n",
    "traced_agent = TracedQAAgent(ENDPOINT_NAME, client)\n",
    "print(\"✅ Traced Q&A Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210637a1-2507-4f3e-9d1b-0c6d7171616f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Running Traced Queries\n",
    "\n",
    "**What we're doing:** Executing queries through our `TracedQAAgent` to collect detailed timing data for each step of the pipeline.\n",
    "\n",
    "**How the tracing works:**\n",
    "\n",
    "1. **Trace ID generation**: Each query gets a unique ID for correlation:\n",
    "   ```python\n",
    "   trace_id = str(uuid.uuid4())[:8]  # e.g., \"a1b2c3d4\"\n",
    "   ```\n",
    "\n",
    "2. **Step-by-step timing**: Each pipeline step is timed:\n",
    "   ```python\n",
    "   start = time.time()\n",
    "   result = self.preprocess_query(question)\n",
    "   latency_ms = (time.time() - start) * 1000\n",
    "   trace_record[\"steps\"].append({\"step\": \"preprocess\", \"latency_ms\": latency_ms})\n",
    "   ```\n",
    "\n",
    "3. **Pipeline steps traced**:\n",
    "   | Step | What it does | Typical time |\n",
    "   |------|-------------|--------------|\n",
    "   | preprocess | Clean/normalize question | 1-5ms |\n",
    "   | retrieve | Find relevant context | 5-20ms |\n",
    "   | generate | LLM API call | 200-2000ms |\n",
    "   | postprocess | Format response | 1-5ms |\n",
    "\n",
    "4. **Trace log storage**: All traces stored in `traced_agent.trace_log` for analysis\n",
    "\n",
    "**Why trace?** The LLM call typically dominates latency (>95%), but tracing helps identify when other steps become bottlenecks (e.g., slow retrieval from a vector database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a35ff8d-35a2-4114-98b4-d901f3fa10ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN TRACED QUERIES\n",
    "# =============================================================================\n",
    "# Execute queries with manual tracing\n",
    "\n",
    "print(\"\uD83D\uDD04 Running traced queries...\")\n",
    "\n",
    "traced_results = []\n",
    "\n",
    "# Run a few traced queries\n",
    "for sample in eval_data[:5]:\n",
    "    try:\n",
    "        result = traced_agent.run(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        traced_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"trace_id\": result[\"trace_id\"],\n",
    "            **result[\"step_latencies\"],\n",
    "            \"total_latency\": result[\"total_latency_ms\"],\n",
    "            \"tokens\": result[\"tokens\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   ✓ {sample['id']} [trace:{result['trace_id']}]: {result['total_latency_ms']:.0f}ms total\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Analyze step latencies\n",
    "traced_df = pd.DataFrame(traced_results)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Step Latency Analysis:\")\n",
    "print(f\"   Preprocess:  {traced_df['preprocess'].mean():.2f}ms avg\")\n",
    "print(f\"   Retrieve:    {traced_df['retrieve'].mean():.2f}ms avg\")\n",
    "print(f\"   Generate:    {traced_df['generate'].mean():.2f}ms avg\")\n",
    "print(f\"   Postprocess: {traced_df['postprocess'].mean():.2f}ms avg\")\n",
    "print(f\"\\n   LLM generation accounts for {traced_df['generate'].mean()/traced_df['total_latency'].mean()*100:.1f}% of total latency\")\n",
    "\n",
    "# Display trace log from agent\n",
    "print(f\"\\n\uD83D\uDCCB Trace Log ({len(traced_agent.trace_log)} traces recorded):\")\n",
    "for trace in traced_agent.trace_log[:3]:\n",
    "    print(f\"   Trace {trace['trace_id']}: {trace['total_latency_ms']:.0f}ms\")\n",
    "    for step in trace['steps']:\n",
    "        print(f\"      └─ {step['step']}: {step['latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21e42be-0184-4346-91f7-11e07f48bd55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 6: Anomaly Detection and Alerting\n",
    "\n",
    "In this section, we will:\n",
    "1. Implement statistical anomaly detection\n",
    "2. Configure alert thresholds based on baselines\n",
    "3. Create alerting rules for production monitoring\n",
    "4. Apply calibrated alerting strategies from Chapter 8\n",
    "\n",
    "Effective alerting requires careful calibration to avoid alert fatigue while catching real issues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb06f0b-ab89-4288-87aa-b2df8cbb97ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6.1: Statistical Anomaly Detection\n",
    "\n",
    "We'll implement anomaly detection using statistical methods:\n",
    "- **Z-score detection**: Identify values beyond N standard deviations\n",
    "- **IQR method**: Detect outliers using interquartile range\n",
    "- **Rolling window analysis**: Detect trend changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c050dbac-41be-42f8-8fe2-e4bb950e2795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANOMALY DETECTION IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Statistical anomaly detection for LLM monitoring metrics.\n",
    "    Implements multiple detection methods for robust alerting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, baseline_df):\n",
    "        \"\"\"Initialize with baseline data for threshold calculation.\"\"\"\n",
    "        self.baseline = baseline_df\n",
    "        self.thresholds = self._calculate_thresholds()\n",
    "\n",
    "    def _calculate_thresholds(self):\n",
    "        \"\"\"Calculate detection thresholds from baseline data.\"\"\"\n",
    "        latency = self.baseline['latency_ms']\n",
    "        tokens = self.baseline['total_tokens']\n",
    "\n",
    "        return {\n",
    "            'latency': {\n",
    "                'mean': latency.mean(),\n",
    "                'std': latency.std(),\n",
    "                'p95': latency.quantile(0.95),\n",
    "                'p99': latency.quantile(0.99),\n",
    "                'iqr_low': latency.quantile(0.25) - 1.5 * (latency.quantile(0.75) - latency.quantile(0.25)),\n",
    "                'iqr_high': latency.quantile(0.75) + 1.5 * (latency.quantile(0.75) - latency.quantile(0.25))\n",
    "            },\n",
    "            'tokens': {\n",
    "                'mean': tokens.mean(),\n",
    "                'std': tokens.std(),\n",
    "                'p95': tokens.quantile(0.95),\n",
    "                'p99': tokens.quantile(0.99)\n",
    "            },\n",
    "            'error_rate': {\n",
    "                'baseline': (self.baseline['status'] == 'error').mean(),\n",
    "                'threshold': max(0.05, (self.baseline['status'] == 'error').mean() * 2)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def detect_latency_anomaly(self, value, method='zscore', threshold=3):\n",
    "        \"\"\"Detect if a latency value is anomalous.\"\"\"\n",
    "        if method == 'zscore':\n",
    "            z = (value - self.thresholds['latency']['mean']) / self.thresholds['latency']['std']\n",
    "            return abs(z) > threshold, z\n",
    "        elif method == 'percentile':\n",
    "            return value > self.thresholds['latency']['p99'], value / self.thresholds['latency']['p99']\n",
    "        elif method == 'iqr':\n",
    "            return value > self.thresholds['latency']['iqr_high'], value / self.thresholds['latency']['iqr_high']\n",
    "\n",
    "    def detect_token_anomaly(self, value, threshold=2):\n",
    "        \"\"\"Detect if token usage is anomalous.\"\"\"\n",
    "        z = (value - self.thresholds['tokens']['mean']) / self.thresholds['tokens']['std']\n",
    "        return abs(z) > threshold, z\n",
    "\n",
    "    def detect_error_rate_anomaly(self, current_error_rate):\n",
    "        \"\"\"Detect if error rate exceeds threshold.\"\"\"\n",
    "        return current_error_rate > self.thresholds['error_rate']['threshold'], current_error_rate\n",
    "\n",
    "    def analyze_batch(self, df):\n",
    "        \"\"\"Analyze a batch of data for anomalies.\"\"\"\n",
    "        anomalies = {\n",
    "            'latency_anomalies': [],\n",
    "            'token_anomalies': [],\n",
    "            'error_rate_anomaly': False\n",
    "        }\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            is_latency_anomaly, score = self.detect_latency_anomaly(row['latency_ms'])\n",
    "            if is_latency_anomaly:\n",
    "                anomalies['latency_anomalies'].append({\n",
    "                    'request_id': row['request_id'],\n",
    "                    'latency_ms': row['latency_ms'],\n",
    "                    'z_score': score\n",
    "                })\n",
    "\n",
    "            is_token_anomaly, score = self.detect_token_anomaly(row['total_tokens'])\n",
    "            if is_token_anomaly:\n",
    "                anomalies['token_anomalies'].append({\n",
    "                    'request_id': row['request_id'],\n",
    "                    'total_tokens': row['total_tokens'],\n",
    "                    'z_score': score\n",
    "                })\n",
    "\n",
    "        # Check overall error rate\n",
    "        current_error_rate = (df['status'] == 'error').mean()\n",
    "        anomalies['error_rate_anomaly'], _ = self.detect_error_rate_anomaly(current_error_rate)\n",
    "        anomalies['current_error_rate'] = current_error_rate\n",
    "\n",
    "        return anomalies\n",
    "\n",
    "# Initialize detector with baseline data\n",
    "detector = AnomalyDetector(traffic_analysis_df)\n",
    "\n",
    "print(\"✅ Anomaly Detector initialized with baseline thresholds:\")\n",
    "print(f\"\\n   Latency Thresholds:\")\n",
    "print(f\"      Mean: {detector.thresholds['latency']['mean']:.2f} ms\")\n",
    "print(f\"      Std: {detector.thresholds['latency']['std']:.2f} ms\")\n",
    "print(f\"      P95: {detector.thresholds['latency']['p95']:.2f} ms\")\n",
    "print(f\"      P99: {detector.thresholds['latency']['p99']:.2f} ms\")\n",
    "print(f\"\\n   Token Thresholds:\")\n",
    "print(f\"      Mean: {detector.thresholds['tokens']['mean']:.1f}\")\n",
    "print(f\"      P95: {detector.thresholds['tokens']['p95']:.1f}\")\n",
    "print(f\"\\n   Error Rate Threshold: {detector.thresholds['error_rate']['threshold']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8117ada3-be7e-44ec-82fe-b2b3226bbeda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Running Anomaly Detection\n",
    "\n",
    "**What we're doing:** Scanning our traffic data to find requests that deviate significantly from normal patterns.\n",
    "\n",
    "**How the `analyze_batch()` method works:**\n",
    "\n",
    "1. **Iterate through each request**:\n",
    "   ```python\n",
    "   for idx, row in df.iterrows():\n",
    "       is_anomaly, z_score = self.detect_latency_anomaly(row['latency_ms'])\n",
    "   ```\n",
    "\n",
    "2. **Z-score calculation** for each metric:\n",
    "   ```python\n",
    "   z_score = (value - mean) / std_dev\n",
    "   # If z_score > 3, the value is 3+ standard deviations from mean\n",
    "   ```\n",
    "\n",
    "3. **Anomaly types detected**:\n",
    "\n",
    "   | Type | Detection Method | Threshold |\n",
    "   |------|-----------------|-----------|\n",
    "   | Latency | Z-score > 3 | ~99.7% of normal data is below this |\n",
    "   | Tokens | Z-score > 2 | ~95% of normal data is below this |\n",
    "   | Error Rate | Rate > 5% | Fixed threshold based on SLA |\n",
    "\n",
    "4. **Output structure**:\n",
    "   ```python\n",
    "   {\n",
    "       'latency_anomalies': [{'request_id': 'x', 'latency_ms': 5000, 'z_score': 4.2}],\n",
    "       'token_anomalies': [...],\n",
    "       'error_rate_anomaly': True/False,\n",
    "       'current_error_rate': 0.03\n",
    "   }\n",
    "   ```\n",
    "\n",
    "**Why Z-scores?** They're distribution-agnostic and automatically adapt to your baseline. A Z-score of 3 means \"this is extremely unusual\" regardless of whether your mean latency is 100ms or 1000ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eaee015-55df-42f2-b893-8db0928fafdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Analyze our traffic data for anomalies\n",
    "anomalies = detector.analyze_batch(traffic_analysis_df)\n",
    "\n",
    "print(\"\uD83D\uDD0D Anomaly Detection Results:\")\n",
    "print(f\"\\n   Latency Anomalies: {len(anomalies['latency_anomalies'])}\")\n",
    "if anomalies['latency_anomalies']:\n",
    "    print(\"   Top 5 latency anomalies:\")\n",
    "    for a in sorted(anomalies['latency_anomalies'], key=lambda x: x['z_score'], reverse=True)[:5]:\n",
    "        print(f\"      {a['request_id']}: {a['latency_ms']:.0f}ms (z={a['z_score']:.2f})\")\n",
    "\n",
    "print(f\"\\n   Token Anomalies: {len(anomalies['token_anomalies'])}\")\n",
    "if anomalies['token_anomalies']:\n",
    "    print(\"   Top 5 token anomalies:\")\n",
    "    for a in sorted(anomalies['token_anomalies'], key=lambda x: x['z_score'], reverse=True)[:5]:\n",
    "        print(f\"      {a['request_id']}: {a['total_tokens']} tokens (z={a['z_score']:.2f})\")\n",
    "\n",
    "print(f\"\\n   Error Rate Anomaly: {'⚠️ YES' if anomalies['error_rate_anomaly'] else '✅ NO'}\")\n",
    "print(f\"   Current Error Rate: {anomalies['current_error_rate']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de8d60a-fbb2-4dd5-a851-af6045abcf22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6.2: Configure Alert Rules\n",
    "\n",
    "Following Chapter 8 best practices for calibrated alerting:\n",
    "- **Severity levels**: Critical, Warning, Info\n",
    "- **Cooldown periods**: Prevent alert storms\n",
    "- **Aggregation windows**: Reduce noise from transient spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c40015-9b98-4f80-880e-40ac73f37352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALERT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"\n",
    "    Alert management system with calibrated thresholds and cooldowns.\n",
    "    Implements Chapter 8 best practices for production alerting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, detector):\n",
    "        self.detector = detector\n",
    "        self.alert_history = []\n",
    "        self.cooldown_periods = {\n",
    "            'critical': 300,   # 5 minutes\n",
    "            'warning': 900,    # 15 minutes\n",
    "            'info': 3600       # 1 hour\n",
    "        }\n",
    "\n",
    "        # Define alert rules\n",
    "        self.rules = {\n",
    "            'latency_critical': {\n",
    "                'condition': lambda m: m['latency_p99'] > detector.thresholds['latency']['p99'] * 2,\n",
    "                'severity': 'critical',\n",
    "                'message': 'P99 latency exceeds 2x baseline'\n",
    "            },\n",
    "            'latency_warning': {\n",
    "                'condition': lambda m: m['latency_p95'] > detector.thresholds['latency']['p95'] * 1.5,\n",
    "                'severity': 'warning',\n",
    "                'message': 'P95 latency exceeds 1.5x baseline'\n",
    "            },\n",
    "            'error_rate_critical': {\n",
    "                'condition': lambda m: m['error_rate'] > 0.10,\n",
    "                'severity': 'critical',\n",
    "                'message': 'Error rate exceeds 10%'\n",
    "            },\n",
    "            'error_rate_warning': {\n",
    "                'condition': lambda m: m['error_rate'] > 0.05,\n",
    "                'severity': 'warning',\n",
    "                'message': 'Error rate exceeds 5%'\n",
    "            },\n",
    "            'token_spike': {\n",
    "                'condition': lambda m: m['avg_total_tokens'] > detector.thresholds['tokens']['p95'],\n",
    "                'severity': 'warning',\n",
    "                'message': 'Average token usage exceeds P95 baseline'\n",
    "            },\n",
    "            'quality_degradation': {\n",
    "                'condition': lambda m: m.get('avg_quality_score', 1) < 0.7,\n",
    "                'severity': 'warning',\n",
    "                'message': 'Average quality score below threshold'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def evaluate_rules(self, metrics):\n",
    "        \"\"\"Evaluate all alert rules against current metrics.\"\"\"\n",
    "        triggered_alerts = []\n",
    "\n",
    "        for rule_name, rule in self.rules.items():\n",
    "            try:\n",
    "                if rule['condition'](metrics):\n",
    "                    triggered_alerts.append({\n",
    "                        'rule': rule_name,\n",
    "                        'severity': rule['severity'],\n",
    "                        'message': rule['message'],\n",
    "                        'timestamp': datetime.now()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating rule {rule_name}: {e}\")\n",
    "\n",
    "        return triggered_alerts\n",
    "\n",
    "    def should_alert(self, alert):\n",
    "        \"\"\"Check if alert should fire based on cooldown.\"\"\"\n",
    "        cooldown = self.cooldown_periods[alert['severity']]\n",
    "\n",
    "        # Check recent alerts of same type\n",
    "        for hist in self.alert_history:\n",
    "            if hist['rule'] == alert['rule']:\n",
    "                time_diff = (alert['timestamp'] - hist['timestamp']).total_seconds()\n",
    "                if time_diff < cooldown:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def process_alerts(self, metrics):\n",
    "        \"\"\"Process metrics and generate alerts.\"\"\"\n",
    "        triggered = self.evaluate_rules(metrics)\n",
    "        fired_alerts = []\n",
    "\n",
    "        for alert in triggered:\n",
    "            if self.should_alert(alert):\n",
    "                fired_alerts.append(alert)\n",
    "                self.alert_history.append(alert)\n",
    "\n",
    "        return fired_alerts\n",
    "\n",
    "# Initialize alert manager\n",
    "alert_manager = AlertManager(detector)\n",
    "\n",
    "# Evaluate current metrics\n",
    "alerts = alert_manager.process_alerts(current_metrics)\n",
    "\n",
    "print(\"\uD83D\uDEA8 Alert Evaluation Results:\")\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        severity_icon = {'critical': '\uD83D\uDD34', 'warning': '\uD83D\uDFE1', 'info': '\uD83D\uDD35'}\n",
    "        print(f\"   {severity_icon[alert['severity']]} [{alert['severity'].upper()}] {alert['message']}\")\n",
    "else:\n",
    "    print(\"   ✅ No alerts triggered - all metrics within normal range\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCB Configured Alert Rules:\")\n",
    "for rule_name, rule in alert_manager.rules.items():\n",
    "    print(f\"   - {rule_name} ({rule['severity']}): {rule['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3207ae0-8e9e-45db-b798-ce58d32aece0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 7: Cost Optimization Strategies\n",
    "\n",
    "In this section, we will:\n",
    "1. Analyze token usage patterns for cost drivers\n",
    "2. Implement prompt optimization techniques\n",
    "3. Apply context control strategies\n",
    "4. Validate improvements with evaluation runs\n",
    "\n",
    "Cost optimization is critical for sustainable LLM operations at scale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2ece53-deff-4f76-96b3-dd4370f4f6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 7.1: Analyze Cost Drivers\n",
    "\n",
    "We'll identify the main contributors to token usage and cost:\n",
    "- Long context documents\n",
    "- Verbose prompts\n",
    "- Unnecessarily detailed responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ab8dda-4f1f-48cb-a335-e4caff2ae673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COST ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Analyze token usage patterns\n",
    "print(\"\uD83D\uDCB0 Cost Analysis:\")\n",
    "\n",
    "# Token usage by complexity\n",
    "token_by_complexity = traffic_analysis_df.groupby('complexity').agg({\n",
    "    'input_tokens': 'mean',\n",
    "    'output_tokens': 'mean',\n",
    "    'total_tokens': ['mean', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n   Token Usage by Complexity:\")\n",
    "print(token_by_complexity)\n",
    "\n",
    "# Token usage by category\n",
    "token_by_category = traffic_analysis_df.groupby('category').agg({\n",
    "    'total_tokens': ['mean', 'sum', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n   Token Usage by Category:\")\n",
    "print(token_by_category)\n",
    "\n",
    "# Identify high-cost queries\n",
    "high_cost_threshold = traffic_analysis_df['total_tokens'].quantile(0.9)\n",
    "high_cost_queries = traffic_analysis_df[traffic_analysis_df['total_tokens'] > high_cost_threshold]\n",
    "\n",
    "print(f\"\\n   High-Cost Queries (>{high_cost_threshold:.0f} tokens): {len(high_cost_queries)}\")\n",
    "print(f\"   These represent {len(high_cost_queries)/len(traffic_analysis_df)*100:.1f}% of requests\")\n",
    "print(f\"   But consume {high_cost_queries['total_tokens'].sum()/traffic_analysis_df['total_tokens'].sum()*100:.1f}% of tokens\")\n",
    "\n",
    "# Estimate costs (using approximate pricing)\n",
    "COST_PER_1K_INPUT_TOKENS = 0.0015  # Example pricing\n",
    "COST_PER_1K_OUTPUT_TOKENS = 0.002\n",
    "\n",
    "total_input_tokens = traffic_analysis_df['input_tokens'].sum()\n",
    "total_output_tokens = traffic_analysis_df['output_tokens'].sum()\n",
    "\n",
    "estimated_cost = (total_input_tokens / 1000 * COST_PER_1K_INPUT_TOKENS +\n",
    "                  total_output_tokens / 1000 * COST_PER_1K_OUTPUT_TOKENS)\n",
    "\n",
    "print(f\"\\n   Estimated Cost for {len(traffic_analysis_df)} requests: ${estimated_cost:.2f}\")\n",
    "print(f\"   Average Cost per Request: ${estimated_cost/len(traffic_analysis_df)*1000:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e50aab5-0e00-46ef-aab5-947044a26d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 7.2: Implement Prompt Optimization\n",
    "\n",
    "We'll implement several prompt optimization techniques:\n",
    "1. **Context truncation**: Limit context to relevant portions\n",
    "2. **Prompt compression**: Remove redundant instructions\n",
    "3. **Response length control**: Set appropriate max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece0e1c9-2f8a-4185-be13-1d029ea75216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROMPT OPTIMIZATION TECHNIQUES\n",
    "# =============================================================================\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"\n",
    "    Implements prompt optimization strategies for cost reduction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_context_tokens=200, max_response_tokens=150):\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.max_response_tokens = max_response_tokens\n",
    "\n",
    "    def truncate_context(self, context, max_tokens=None):\n",
    "        \"\"\"Truncate context to maximum token limit.\"\"\"\n",
    "        max_tokens = max_tokens or self.max_context_tokens\n",
    "        words = context.split()\n",
    "\n",
    "        # Approximate: 1 token ≈ 0.75 words\n",
    "        max_words = int(max_tokens * 0.75)\n",
    "\n",
    "        if len(words) <= max_words:\n",
    "            return context, False\n",
    "\n",
    "        truncated = ' '.join(words[:max_words]) + '...'\n",
    "        return truncated, True\n",
    "\n",
    "    def compress_prompt(self, system_prompt, user_prompt):\n",
    "        \"\"\"Compress prompts by removing redundancy.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        system_prompt = ' '.join(system_prompt.split())\n",
    "        user_prompt = ' '.join(user_prompt.split())\n",
    "\n",
    "        # Use shorter system prompt\n",
    "        compressed_system = \"Answer questions based on the context. Be concise.\"\n",
    "\n",
    "        return compressed_system, user_prompt\n",
    "\n",
    "    def optimize_query(self, question, context):\n",
    "        \"\"\"Apply all optimization techniques.\"\"\"\n",
    "        # Truncate context\n",
    "        truncated_context, was_truncated = self.truncate_context(context)\n",
    "\n",
    "        # Create optimized prompts\n",
    "        system_prompt = \"Answer based on context. Be concise and accurate.\"\n",
    "        user_prompt = f\"Context: {truncated_context}\\n\\nQ: {question}\\n\\nA:\"\n",
    "\n",
    "        return {\n",
    "            'system_prompt': system_prompt,\n",
    "            'user_prompt': user_prompt,\n",
    "            'context_truncated': was_truncated,\n",
    "            'max_tokens': self.max_response_tokens\n",
    "        }\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = PromptOptimizer(max_context_tokens=150, max_response_tokens=100)\n",
    "\n",
    "# Test optimization on a sample\n",
    "sample = eval_data[0]\n",
    "original_context_tokens = len(sample['context'].split())\n",
    "optimized = optimizer.optimize_query(sample['question'], sample['context'])\n",
    "\n",
    "print(\"\uD83D\uDD27 Prompt Optimization Example:\")\n",
    "print(f\"\\n   Original context tokens: ~{original_context_tokens}\")\n",
    "print(f\"   Optimized context tokens: ~{len(optimized['user_prompt'].split())}\")\n",
    "print(f\"   Context truncated: {optimized['context_truncated']}\")\n",
    "print(f\"   Max response tokens: {optimized['max_tokens']}\")\n",
    "print(f\"\\n   Optimized system prompt: '{optimized['system_prompt']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88345efb-d0a9-48fe-a80d-618b8b19e79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Comparing Optimized vs Baseline Performance\n",
    "\n",
    "**What we're doing:** Running the same queries with optimized prompts and comparing token usage, latency, and answer quality against our baseline.\n",
    "\n",
    "**How the comparison works:**\n",
    "\n",
    "1. **Run both versions** on the same questions:\n",
    "   ```python\n",
    "   baseline_result = query_qa_endpoint(question, context)  # Full prompts\n",
    "   optimized_result = query_optimized(question, context, optimizer)  # Compressed\n",
    "   ```\n",
    "\n",
    "2. **Metrics compared**:\n",
    "   | Metric | Baseline | Optimized | Goal |\n",
    "   |--------|----------|-----------|------|\n",
    "   | Input tokens | ~300 | ~150 | 50% reduction |\n",
    "   | Output tokens | ~200 | ~100 | 50% reduction |\n",
    "   | Latency | ~500ms | ~350ms | Lower is better |\n",
    "   | Quality | 0.85 | 0.80+ | Minimal degradation |\n",
    "\n",
    "3. **Cost calculation**:\n",
    "   ```python\n",
    "   # Typical pricing: $0.002 per 1K tokens\n",
    "   baseline_cost = baseline_tokens * 0.002 / 1000\n",
    "   optimized_cost = optimized_tokens * 0.002 / 1000\n",
    "   savings = (baseline_cost - optimized_cost) / baseline_cost * 100\n",
    "   ```\n",
    "\n",
    "**The trade-off:** Aggressive optimization saves money but may reduce answer quality. The goal is finding the sweet spot where you save 30-50% on tokens while maintaining >95% of baseline quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424f692a-dba8-49d7-b2e1-3573a0574f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARE OPTIMIZED VS BASELINE PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "def query_optimized(question, context, optimizer):\n",
    "    \"\"\"Query endpoint with optimized prompts.\"\"\"\n",
    "    opt = optimizer.optimize_query(question, context)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=ENDPOINT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": opt['system_prompt']},\n",
    "            {\"role\": \"user\", \"content\": opt['user_prompt']}\n",
    "        ],\n",
    "        max_tokens=opt['max_tokens'],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "print(\"\uD83D\uDD04 Comparing baseline vs optimized performance...\")\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for sample in eval_data[:5]:\n",
    "    try:\n",
    "        # Baseline query\n",
    "        baseline = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        # Optimized query\n",
    "        optimized_result = query_optimized(sample[\"question\"], sample[\"context\"], optimizer)\n",
    "\n",
    "        comparison_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"baseline_tokens\": baseline[\"total_tokens\"],\n",
    "            \"optimized_tokens\": optimized_result[\"total_tokens\"],\n",
    "            \"token_reduction\": baseline[\"total_tokens\"] - optimized_result[\"total_tokens\"],\n",
    "            \"baseline_latency\": baseline[\"latency_ms\"],\n",
    "            \"optimized_latency\": optimized_result[\"latency_ms\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   ✓ {sample['id']}: {baseline['total_tokens']} → {optimized_result['total_tokens']} tokens\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Analyze results\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Optimization Results:\")\n",
    "print(f\"   Average Token Reduction: {comparison_df['token_reduction'].mean():.1f} tokens ({comparison_df['token_reduction'].mean()/comparison_df['baseline_tokens'].mean()*100:.1f}%)\")\n",
    "print(f\"   Average Latency Change: {(comparison_df['optimized_latency'].mean() - comparison_df['baseline_latency'].mean()):.1f}ms\")\n",
    "\n",
    "# Estimate cost savings\n",
    "baseline_cost = comparison_df['baseline_tokens'].sum() / 1000 * 0.002\n",
    "optimized_cost = comparison_df['optimized_tokens'].sum() / 1000 * 0.002\n",
    "savings = baseline_cost - optimized_cost\n",
    "\n",
    "print(f\"\\n   Estimated Cost Savings: ${savings:.4f} for {len(comparison_df)} queries\")\n",
    "print(f\"   Projected Monthly Savings (10K queries/day): ${savings/len(comparison_df)*10000*30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992a7d23-f6f4-483d-85d3-3ec451369367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 8: Validation and Periodic Review Cycles\n",
    "\n",
    "In this section, we will:\n",
    "1. Validate optimizations with repeatable evaluation runs\n",
    "2. Establish baseline metrics for ongoing comparison\n",
    "3. Implement periodic review workflows\n",
    "4. Create evaluation versioning for reproducibility\n",
    "\n",
    "Following Chapter 8 best practices, we'll ensure our improvements are measurable and sustainable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad585fd7-7a58-4b32-956f-07f8c60a260c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.1: Validate Optimizations\n",
    "\n",
    "We'll run a comprehensive evaluation to validate that our optimizations maintain quality while reducing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b01f0e-d688-4af0-aae4-dff6a9a0037f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION EVALUATION RUN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\uD83D\uDD04 Running validation evaluation...\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for sample in eval_data:\n",
    "    try:\n",
    "        # Run optimized query\n",
    "        result = query_optimized(sample[\"question\"], sample[\"context\"], optimizer)\n",
    "\n",
    "        validation_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"context\": sample[\"context\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"output\": result[\"answer\"],\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": result[\"latency_ms\"],\n",
    "            \"input_tokens\": result[\"input_tokens\"],\n",
    "            \"output_tokens\": result[\"output_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "\n",
    "print(f\"\\n✅ Validation complete: {len(validation_df)} samples evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440e5c7d-8850-45d0-9749-19bf35f3d0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Logging Validation Results to MLflow\n",
    "\n",
    "**What we're doing:** Creating a new MLflow run for our optimized model to enable side-by-side comparison with the baseline.\n",
    "\n",
    "**How the comparison works in MLflow:**\n",
    "\n",
    "1. **Separate runs for each configuration**:\n",
    "   - `baseline_evaluation`: Original prompts, no optimization\n",
    "   - `optimized_validation`: Compressed prompts, token limits\n",
    "\n",
    "2. **Parameters logged** (what we changed):\n",
    "   ```python\n",
    "   mlflow.log_param(\"optimization_type\", \"prompt_compression\")\n",
    "   mlflow.log_param(\"max_context_tokens\", 150)  # Was unlimited\n",
    "   mlflow.log_param(\"max_response_tokens\", 100)  # Was unlimited\n",
    "   ```\n",
    "\n",
    "3. **Metrics logged using new GenAI API**:\n",
    "   ```python\n",
    "   # Run scorers on each sample\n",
    "   g_score = groundedness(inputs=inputs, outputs=outputs)\n",
    "   c_score = answer_correctness(inputs=inputs, outputs=outputs, expectations=expectations)\n",
    "\n",
    "   # Log aggregate metrics\n",
    "   mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "   mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "   ```\n",
    "\n",
    "4. **MLflow UI comparison**:\n",
    "   - Navigate to Experiments → Select both runs → Compare\n",
    "   - See metrics side-by-side in charts\n",
    "   - Identify if quality dropped with optimization\n",
    "\n",
    "**Decision framework:**\n",
    "- If quality drops <5% and tokens drop >30% → Accept optimization\n",
    "- If quality drops >10% → Reject, try less aggressive settings\n",
    "- If quality unchanged → Great! Deploy optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21668cb1-050c-41b7-8a50-9c923fea4c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG VALIDATION RUN TO MLFLOW\n",
    "# =============================================================================\n",
    "\n",
    "with mlflow.start_run(run_name=\"optimized_validation\") as run:\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"endpoint_name\", ENDPOINT_NAME)\n",
    "    mlflow.log_param(\"optimization_type\", \"prompt_compression\")\n",
    "    mlflow.log_param(\"max_context_tokens\", optimizer.max_context_tokens)\n",
    "    mlflow.log_param(\"max_response_tokens\", optimizer.max_response_tokens)\n",
    "    mlflow.log_param(\"num_samples\", len(validation_df))\n",
    "\n",
    "    # Run evaluation using new GenAI API scorers\n",
    "    print(\"\uD83D\uDD04 Running MLflow evaluation on optimized results...\")\n",
    "    print(\"   (Using new MLflow 3.4+ GenAI API)\")\n",
    "\n",
    "    # Prepare data and run scorers\n",
    "    val_groundedness_scores = []\n",
    "    val_correctness_scores = []\n",
    "\n",
    "    for i, row in validation_df.iterrows():\n",
    "        try:\n",
    "            inputs = {\"question\": row[\"question\"], \"context\": row[\"context\"]}\n",
    "            outputs = row[\"output\"]\n",
    "            expectations = {\"ground_truth\": row[\"ground_truth\"]}\n",
    "\n",
    "            g_score = groundedness(inputs=inputs, outputs=outputs)\n",
    "            c_score = answer_correctness(inputs=inputs, outputs=outputs, expectations=expectations)\n",
    "\n",
    "            val_groundedness_scores.append(g_score)\n",
    "            val_correctness_scores.append(c_score)\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Error evaluating sample {i}: {e}\")\n",
    "            val_groundedness_scores.append(3)\n",
    "            val_correctness_scores.append(3)\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "    # Calculate and log aggregate metrics\n",
    "    avg_groundedness = sum(val_groundedness_scores) / len(val_groundedness_scores)\n",
    "    avg_correctness = sum(val_correctness_scores) / len(val_correctness_scores)\n",
    "\n",
    "    mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "    mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "\n",
    "    # Log performance metrics\n",
    "    mlflow.log_metric(\"avg_latency_ms\", validation_df[\"latency_ms\"].mean())\n",
    "    mlflow.log_metric(\"p95_latency_ms\", validation_df[\"latency_ms\"].quantile(0.95))\n",
    "    mlflow.log_metric(\"avg_total_tokens\", validation_df[\"total_tokens\"].mean())\n",
    "    mlflow.log_metric(\"total_tokens_consumed\", validation_df[\"total_tokens\"].sum())\n",
    "\n",
    "    # Add scores to dataframe\n",
    "    validation_df[\"groundedness_score\"] = val_groundedness_scores\n",
    "    validation_df[\"correctness_score\"] = val_correctness_scores\n",
    "\n",
    "    # Log artifacts\n",
    "    validation_df.to_csv(\"/tmp/validation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/validation_results.csv\")\n",
    "\n",
    "    print(f\"\\n✅ Validation logged to MLflow\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "\n",
    "    # Compare with baseline\n",
    "    print(f\"\\n\uD83D\uDCCA Validation Metrics:\")\n",
    "    print(f\"   groundedness_mean: {avg_groundedness:.2f}\")\n",
    "    print(f\"   answer_correctness_mean: {avg_correctness:.2f}\")\n",
    "    print(f\"   avg_latency_ms: {validation_df['latency_ms'].mean():.2f}\")\n",
    "    print(f\"   avg_total_tokens: {validation_df['total_tokens'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc6cafc-cc09-4c02-98e9-4c83c81754d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.2: Establish Review Cycle Workflow\n",
    "\n",
    "We'll create a structured review workflow that can be run periodically to:\n",
    "- Compare current performance against baselines\n",
    "- Detect drift in quality or performance\n",
    "- Generate actionable reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ee5f6c-0cdf-4406-a97b-57499a9c380c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PERIODIC REVIEW WORKFLOW\n",
    "# =============================================================================\n",
    "\n",
    "class PeriodicReviewWorkflow:\n",
    "    \"\"\"\n",
    "    Implements a structured review workflow for LLM monitoring.\n",
    "    Following Chapter 8 best practices for periodic evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, baseline_metrics, alert_manager):\n",
    "        self.baseline = baseline_metrics\n",
    "        self.alert_manager = alert_manager\n",
    "        self.review_history = []\n",
    "\n",
    "    def run_review(self, current_df, review_name=\"periodic_review\"):\n",
    "        \"\"\"Execute a complete review cycle.\"\"\"\n",
    "        review_timestamp = datetime.now()\n",
    "\n",
    "        # Calculate current metrics\n",
    "        current_metrics = calculate_monitoring_metrics(current_df)\n",
    "\n",
    "        # Compare with baseline\n",
    "        comparison = self._compare_metrics(current_metrics)\n",
    "\n",
    "        # Check for alerts\n",
    "        alerts = self.alert_manager.evaluate_rules(current_metrics)\n",
    "\n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(comparison, alerts)\n",
    "\n",
    "        # Create review report\n",
    "        report = {\n",
    "            \"review_name\": review_name,\n",
    "            \"timestamp\": review_timestamp,\n",
    "            \"current_metrics\": current_metrics,\n",
    "            \"baseline_comparison\": comparison,\n",
    "            \"alerts\": alerts,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"status\": \"HEALTHY\" if not alerts else \"NEEDS_ATTENTION\"\n",
    "        }\n",
    "\n",
    "        self.review_history.append(report)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _compare_metrics(self, current):\n",
    "        \"\"\"Compare current metrics against baseline.\"\"\"\n",
    "        comparison = {}\n",
    "\n",
    "        for key in ['latency_p50', 'latency_p95', 'avg_total_tokens', 'error_rate']:\n",
    "            if key in self.baseline and key in current:\n",
    "                baseline_val = self.baseline[key]\n",
    "                current_val = current[key]\n",
    "\n",
    "                if baseline_val > 0:\n",
    "                    pct_change = (current_val - baseline_val) / baseline_val * 100\n",
    "                else:\n",
    "                    pct_change = 0\n",
    "\n",
    "                comparison[key] = {\n",
    "                    \"baseline\": baseline_val,\n",
    "                    \"current\": current_val,\n",
    "                    \"change_pct\": pct_change,\n",
    "                    \"status\": \"OK\" if abs(pct_change) < 20 else \"DEGRADED\" if pct_change > 0 else \"IMPROVED\"\n",
    "                }\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def _generate_recommendations(self, comparison, alerts):\n",
    "        \"\"\"Generate actionable recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Check latency\n",
    "        if comparison.get('latency_p95', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"HIGH\",\n",
    "                \"area\": \"Latency\",\n",
    "                \"action\": \"Investigate latency increase. Consider scaling up or optimizing prompts.\"\n",
    "            })\n",
    "\n",
    "        # Check token usage\n",
    "        if comparison.get('avg_total_tokens', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"MEDIUM\",\n",
    "                \"area\": \"Cost\",\n",
    "                \"action\": \"Token usage increased. Review prompt templates and context lengths.\"\n",
    "            })\n",
    "\n",
    "        # Check error rate\n",
    "        if comparison.get('error_rate', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"HIGH\",\n",
    "                \"area\": \"Reliability\",\n",
    "                \"action\": \"Error rate increased. Check endpoint health and input validation.\"\n",
    "            })\n",
    "\n",
    "        # Add alert-based recommendations\n",
    "        for alert in alerts:\n",
    "            if alert['severity'] == 'critical':\n",
    "                recommendations.append({\n",
    "                    \"priority\": \"CRITICAL\",\n",
    "                    \"area\": alert['rule'],\n",
    "                    \"action\": f\"Address immediately: {alert['message']}\"\n",
    "                })\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "# Initialize review workflow\n",
    "review_workflow = PeriodicReviewWorkflow(current_metrics, alert_manager)\n",
    "\n",
    "# Run a review\n",
    "review_report = review_workflow.run_review(traffic_analysis_df, \"lab_final_review\")\n",
    "\n",
    "print(\"\uD83D\uDCCB Periodic Review Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Review: {review_report['review_name']}\")\n",
    "print(f\"   Timestamp: {review_report['timestamp']}\")\n",
    "print(f\"   Status: {review_report['status']}\")\n",
    "\n",
    "print(f\"\\n   Baseline Comparison:\")\n",
    "for metric, data in review_report['baseline_comparison'].items():\n",
    "    print(f\"      {metric}: {data['current']:.2f} ({data['change_pct']:+.1f}%) - {data['status']}\")\n",
    "\n",
    "if review_report['recommendations']:\n",
    "    print(f\"\\n   Recommendations:\")\n",
    "    for rec in review_report['recommendations']:\n",
    "        print(f\"      [{rec['priority']}] {rec['area']}: {rec['action']}\")\n",
    "else:\n",
    "    print(f\"\\n   ✅ No recommendations - system performing within expected parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ee52bf-a14e-460d-bc4d-199bdc325187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Generating the Review Report\n",
    "\n",
    "**What we're doing:** Creating a human-readable report that summarizes the review findings for stakeholders who may not be technical.\n",
    "\n",
    "**Report structure:**\n",
    "\n",
    "1. **Executive Summary**: High-level health status\n",
    "   - Overall health: HEALTHY / NEEDS ATTENTION / CRITICAL\n",
    "   - Review period and sample size\n",
    "   - One-line summary of key findings\n",
    "\n",
    "2. **Key Metrics Table**: Current vs baseline comparison\n",
    "   ```\n",
    "   ✅ latency_p50: 245ms (baseline: 240ms, change: +2.1%)\n",
    "   ⚠️ latency_p95: 890ms (baseline: 720ms, change: +23.6%)\n",
    "   ✅ error_rate: 1.2% (baseline: 1.5%, change: -20.0%)\n",
    "   ```\n",
    "\n",
    "3. **Active Alerts**: Any triggered alert rules\n",
    "   - Severity level (CRITICAL/WARNING/INFO)\n",
    "   - Rule that triggered\n",
    "   - Specific message\n",
    "\n",
    "4. **Recommendations**: Prioritized action items\n",
    "   - CRITICAL: Address immediately\n",
    "   - HIGH: Address within 24 hours\n",
    "   - MEDIUM: Address within 1 week\n",
    "\n",
    "**Why structured reports matter (Chapter 8 best practice):**\n",
    "- Executives need summaries, not raw data\n",
    "- Consistent format enables trend tracking over time\n",
    "- Recommendations drive action, not just awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e2528a-b66d-47ab-9db0-34bbd2fc913d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE REVIEW REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\uD83D\uDCCB Generating Review Report...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PERIODIC REVIEW REPORT\")\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA EXECUTIVE SUMMARY\")\n",
    "print(f\"   Review Period: Last 7 days\")\n",
    "print(f\"   Total Requests Analyzed: {len(traffic_analysis_df)}\")\n",
    "print(f\"   Overall Health: {'✅ HEALTHY' if len(review_report['alerts']) == 0 else '⚠️ NEEDS ATTENTION'}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCC8 KEY METRICS\")\n",
    "for metric, data in review_report['baseline_comparison'].items():\n",
    "    status_icon = \"✅\" if data['status'] == 'ok' else \"⚠️\" if data['status'] == 'warning' else \"\uD83D\uDD34\"\n",
    "    print(f\"   {status_icon} {metric}: {data['current']:.2f} (baseline: {data['baseline']:.2f}, change: {data['change_pct']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDEA8 ALERTS ({len(review_report['alerts'])})\")\n",
    "if review_report['alerts']:\n",
    "    for alert in review_report['alerts']:\n",
    "        print(f\"   [{alert['severity'].upper()}] {alert['rule_name']}: {alert['message']}\")\n",
    "else:\n",
    "    print(\"   No active alerts\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCA1 RECOMMENDATIONS ({len(review_report['recommendations'])})\")\n",
    "if review_report['recommendations']:\n",
    "    for i, rec in enumerate(review_report['recommendations'], 1):\n",
    "        print(f\"   {i}. [{rec['priority'].upper()}] {rec['area']}: {rec['action']}\")\n",
    "else:\n",
    "    print(\"   No recommendations - system performing optimally\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856fdba8-cb34-4e94-9910-4c40da8e8569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.3: Create Evaluation Dataset Versioning\n",
    "\n",
    "**What we're doing:** Saving a timestamped version of our evaluation dataset so we can reproduce results and track changes over time.\n",
    "\n",
    "**How versioning works:**\n",
    "\n",
    "1. **Generate version string** from timestamp:\n",
    "   ```python\n",
    "   version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "   # Result: \"20241130_143022\"\n",
    "   ```\n",
    "\n",
    "2. **Save versioned Delta table**:\n",
    "   ```python\n",
    "   versioned_table_name = f\"{CATALOG}.{SCHEMA}.evaluation_dataset_v{version}\"\n",
    "   spark_df.write.saveAsTable(versioned_table_name)\n",
    "   # Creates: main.llm_monitoring_lab.evaluation_dataset_v20241130_143022\n",
    "   ```\n",
    "\n",
    "3. **Log to MLflow** for discoverability:\n",
    "   ```python\n",
    "   mlflow.log_param(\"dataset_version\", version)\n",
    "   mlflow.log_param(\"dataset_table\", versioned_table_name)\n",
    "   mlflow.log_artifact(\"eval_dataset.csv\")  # Downloadable copy\n",
    "   ```\n",
    "\n",
    "**Why version datasets?**\n",
    "- **Reproducibility**: \"What data did we use for the March evaluation?\"\n",
    "- **Debugging**: \"Did the dataset change, or did the model regress?\"\n",
    "- **Compliance**: Audit trails for regulated industries\n",
    "- **A/B testing**: Compare model performance on identical datasets\n",
    "\n",
    "**Best practice:** Always version both the dataset AND the evaluation results together. Link them via MLflow run IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6dbb21-2d7d-48ba-b664-7e825e32cf46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION DATASET VERSIONING\n",
    "# =============================================================================\n",
    "\n",
    "# Save versioned evaluation dataset\n",
    "version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_table_name = f\"{CATALOG}.{SCHEMA}.evaluation_dataset_v{version}\"\n",
    "\n",
    "# Save to Delta table with version\n",
    "eval_spark_df = spark.createDataFrame(eval_df)\n",
    "eval_spark_df.write.mode(\"overwrite\").saveAsTable(versioned_table_name)\n",
    "\n",
    "print(f\"✅ Evaluation dataset versioned: {versioned_table_name}\")\n",
    "\n",
    "# Log version to MLflow\n",
    "with mlflow.start_run(run_name=f\"dataset_version_{version}\"):\n",
    "    mlflow.log_param(\"dataset_version\", version)\n",
    "    mlflow.log_param(\"dataset_table\", versioned_table_name)\n",
    "    mlflow.log_param(\"num_samples\", len(eval_df))\n",
    "    mlflow.log_param(\"categories\", list(eval_df['category'].unique()))\n",
    "\n",
    "    # Log dataset as artifact\n",
    "    eval_df.to_csv(f\"/tmp/eval_dataset_{version}.csv\", index=False)\n",
    "    mlflow.log_artifact(f\"/tmp/eval_dataset_{version}.csv\")\n",
    "\n",
    "    print(f\"   Logged to MLflow experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dafd686-0bee-40a3-86a2-6e696ded384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Lab Summary and Key Takeaways\n",
    "\n",
    "## What You Accomplished\n",
    "\n",
    "In this hands-on lab, you successfully:\n",
    "\n",
    "### 1. ✅ Designed an End-to-End Evaluation and Monitoring Workflow\n",
    "- Set up MLflow experiment tracking\n",
    "- Created synthetic evaluation datasets\n",
    "- Established baseline metrics\n",
    "\n",
    "### 2. ✅ Applied MLflow Metrics to Assess LLM Quality and Performance\n",
    "- Implemented custom evaluation metrics (groundedness, answer correctness)\n",
    "- Ran comprehensive evaluations with LLM-as-judge\n",
    "- Logged all results for comparison and tracking\n",
    "\n",
    "### 3. ✅ Used Inference Tables to Diagnose Latency and Cost Issues\n",
    "- Analyzed traffic patterns and latency distributions\n",
    "- Identified slow queries and anomalies\n",
    "- Correlated token usage with performance\n",
    "\n",
    "### 4. ✅ Monitored Multi-Step Agent Workflows\n",
    "- Implemented tracing for Q&A pipeline steps\n",
    "- Identified bottlenecks in the workflow\n",
    "- Measured step-by-step latency contributions\n",
    "\n",
    "### 5. ✅ Configured Anomaly Alerts for Production Monitoring\n",
    "- Implemented statistical anomaly detection\n",
    "- Created calibrated alert rules with severity levels\n",
    "- Applied cooldown periods to prevent alert fatigue\n",
    "\n",
    "### 6. ✅ Applied Optimization Strategies\n",
    "- Analyzed cost drivers in token usage\n",
    "- Implemented prompt compression techniques\n",
    "- Validated improvements with evaluation runs\n",
    "\n",
    "### 7. ✅ Incorporated Chapter 8 Best Practices\n",
    "- Calibrated metric interpretation\n",
    "- Baseline-aware monitoring\n",
    "- Structured alerting strategies\n",
    "- Periodic review cycles\n",
    "- Evaluation dataset versioning\n",
    "\n",
    "---\n",
    "\n",
    "## Key Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418084e6-7365-4740-9c51-6ccb53c71dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"           LAB COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Evaluation Metrics:\")\n",
    "print(f\"   Samples Evaluated: {len(eval_df)}\")\n",
    "print(f\"   Categories: {list(eval_df['category'].unique())}\")\n",
    "\n",
    "print(f\"\\n⚡ Performance Baseline:\")\n",
    "print(f\"   P50 Latency: {current_metrics['latency_p50']:.2f} ms\")\n",
    "print(f\"   P95 Latency: {current_metrics['latency_p95']:.2f} ms\")\n",
    "print(f\"   Avg Tokens: {current_metrics['avg_total_tokens']:.1f}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCB0 Cost Optimization:\")\n",
    "if len(comparison_df) > 0:\n",
    "    print(f\"   Token Reduction: {comparison_df['token_reduction'].mean():.1f} tokens/query\")\n",
    "    print(f\"   Projected Monthly Savings: ${savings/len(comparison_df)*10000*30:.2f}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDD0D Anomaly Detection:\")\n",
    "print(f\"   Latency Anomalies Detected: {len(anomalies['latency_anomalies'])}\")\n",
    "print(f\"   Token Anomalies Detected: {len(anomalies['token_anomalies'])}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCB Review Status: {review_report['status']}\")\n",
    "\n",
    "print(f\"\\n✅ Lab completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f3bc11-f62a-4c71-b4f5-5bce65eb2875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After completing this lab, consider:\n",
    "\n",
    "1. **Deploy to Production**: Apply these monitoring patterns to your production LLM systems\n",
    "2. **Customize Metrics**: Add domain-specific evaluation metrics for your use case\n",
    "3. **Automate Reviews**: Schedule periodic review workflows using Databricks Jobs\n",
    "4. **Expand Alerting**: Integrate alerts with your team's notification systems (Slack, PagerDuty)\n",
    "5. **Continuous Improvement**: Use evaluation results to guide model fine-tuning or prompt engineering\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Databricks MLflow Documentation](https://docs.databricks.com/mlflow/index.html)\n",
    "- [Model Serving Best Practices](https://docs.databricks.com/machine-learning/model-serving/index.html)\n",
    "- [LLM Evaluation with MLflow](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [Databricks Monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing the Hands-On Lab!** \uD83C\uDF89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80dcdca-515c-4b3e-83dc-606051f9f773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Run the following cell to clean up resources created during this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f22452-81bc-498e-9539-86f72fed0859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP RESOURCES\n",
    "# =============================================================================\n",
    "# Uncomment and run to delete resources created during this lab\n",
    "\n",
    "# Delete Model Serving endpoint\n",
    "# w.serving_endpoints.delete(name=ENDPOINT_NAME)\n",
    "# print(f\"✅ Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "# Delete Delta tables\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")\n",
    "# print(f\"✅ Deleted schema: {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Delete MLflow experiment\n",
    "# Note: MLflow experiments cannot be permanently deleted, only archived\n",
    "# mlflow.delete_experiment(experiment_id)\n",
    "# print(f\"✅ Archived experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "print(\"⚠️ Cleanup code is commented out. Uncomment to delete resources.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Chapter8_LLM_Evaluation_Monitoring_Lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
